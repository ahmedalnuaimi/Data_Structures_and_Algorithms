{
    "__type__": "Deck",
    "children": [],
    "crowdanki_uuid": "2ac8a044-6e46-11ec-9eba-18c04df1fa07",
    "deck_config_uuid": "2ac8a045-6e46-11ec-adf0-18c04df1fa07",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoplay": true,
            "buryInterdayLearning": false,
            "crowdanki_uuid": "2ac8a045-6e46-11ec-adf0-18c04df1fa07",
            "dyn": false,
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 0,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "5 per day",
            "new": {
                "bury": true,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 5,
                "separate": true
            },
            "newGatherPriority": 0,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "replayq": true,
            "rev": {
                "bury": true,
                "ease4": 1.3,
                "fuzz": 0.05,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "minSpace": 1,
                "perDay": 100
            },
            "reviewOrder": 0,
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [
        "paste-00f99c08238fdb001bddb1f822755c538c24605c.jpg",
        "paste-02088ec5250f2c0c58a5a71ea2db2de66c4a419f.jpg",
        "paste-0361f4901f8e7233ddc5f94beefada26779e08e4.jpg",
        "paste-03f6be8cf7c68d74588d43e738f0ca0ac16041a6.jpg",
        "paste-0586bd7d02607541187b8ac9e534790c3cee089e.jpg",
        "paste-0689df9309716c497c8d2c47888d56626a9f111e.jpg",
        "paste-075234ac7ecc6df052195e6f6145051da686eb0c.jpg",
        "paste-07e070a38dc5b94908d78e90970a2c85f148f1a1.jpg",
        "paste-07f33a00b349b29d8793850932732e3b650a8f31.jpg",
        "paste-08514b4472eef13f9db712b0a3985ec93d70ca85.jpg",
        "paste-085e451dba547532239c93eb14e90dd6e17b7bcb.jpg",
        "paste-0893a8cae5fb900700addfbb654b3149881c3e74.jpg",
        "paste-091b2dc77c1ea227303a50ab376e0c6c88af5112.jpg",
        "paste-0a489379cc9aaf77f5dc25d77880269283b36e12.jpg",
        "paste-0a589a7abadbdaebcd06f45344dbbf3e8de900d5.jpg",
        "paste-0ad793a5745951ffe77c7d36161b030e8c5636df.jpg",
        "paste-0b92770d84c2c2ca44a7d58cdd0ab83695b346d1.jpg",
        "paste-0c7f90d442aa367deca741aeca767f438140f62b.jpg",
        "paste-0d03228e2309a9229e1a9ca6ca7e4d66fcce916f.jpg",
        "paste-0d39d9998006353a94e90dfb2ea010e2d6383aca.jpg",
        "paste-0f8d9ec3dbd035273ebc6c4d7d998110aacaff01.jpg",
        "paste-0f98456e3b4da2e5d865c036782de30cfc1d2211.jpg",
        "paste-1019cffdc21d039085adc66be5b24a928060db22.jpg",
        "paste-12f3a7e6cc65fa617ab9239a27b31c0a0bb167e8.jpg",
        "paste-138eeba44a90e7cb5959060753c99daa3f8df68a.jpg",
        "paste-14f64e311620cf3515281c2b84a6b9fca61eb254.jpg",
        "paste-152e70520fb2a034923e9d517afcdb5983babe79.jpg",
        "paste-15c1861be7d9e204ebda7c33baf4c216925e9126.jpg",
        "paste-16cfbd27da8e3bbd928c57dbd60be34a11affb09.jpg",
        "paste-16fe74b80b60d6ad24d282cdccfcc6f34223e0cf.jpg",
        "paste-1773f101a1f0ea9c1655db9709db9f5b68d0fc4e.jpg",
        "paste-17866d9c05d567f5b8aab12cef9e74a9604c2d4e.jpg",
        "paste-181f50b65b3affa7adc52bb6aff32958c3dd24a2.jpg",
        "paste-189b424d973e013c1eff29d109341c2d3adeb537.jpg",
        "paste-19c122db7cc3c29766d032b37c3fc98616632903.jpg",
        "paste-1a3a3e924c2529e0d3ae6665efac1ed9e27c9040.jpg",
        "paste-1a59eee151ca60f3537025c1eec3f001ce7b1b16.jpg",
        "paste-1aa377763dce9a19c0e5bcba54aec33338a3a307.jpg",
        "paste-1b2677abb3718cee7d30cd7f918b9f0dc2058e7a.jpg",
        "paste-1d1112407221c3dcc2a0fac54938d5d1e40963ea.jpg",
        "paste-1d8b032be31297c586ad5c1164bd0dfddbade2ec.jpg",
        "paste-1e890c3d036844c1e460d3b6fb45f011e0840da4.jpg",
        "paste-1f32a7e5542a3981d552ec0927b197e69f96ff1e.jpg",
        "paste-1f66f0baa48d9f7241b4420f7119fb7fd85c159d.jpg",
        "paste-20dd187c1e762f5399fbf9c20782c56555431ef4.jpg",
        "paste-217ebe08300a935500b838ef8a1437350902853b.jpg",
        "paste-21b7167266cc571cb88d2ceefa50702f2aaa773b.jpg",
        "paste-21e80bb03883801b91d88d4ba8e02598f2949c76.jpg",
        "paste-21ea0184d8b75a195a2743d8e6aada2dc0271859.jpg",
        "paste-2342e8d8fdb61f7085309799af12c8d088fd9c00.jpg",
        "paste-237767969ba2af0d632b596cf4807653c7968c4a.jpg",
        "paste-239d9916b6aba59b35f1f5bb4dcdd8afdd6ca6df.jpg",
        "paste-252f1f33951d320e152e8fc4f9722865ee6e09e0.jpg",
        "paste-25950947b1193801a715541288690e4ea087d281.jpg",
        "paste-25b4262438225c1a4904f9c40f8e610451902b88.jpg",
        "paste-25e5f9cf99df03aa3e03e2000f0d7d0d6ba31b74.jpg",
        "paste-26a80d1874f6083de2a6bbb343b4180b2cf8c63d.jpg",
        "paste-283fc2bedc9a8515cd604bd1dfc8f8e11a53b3c7.jpg",
        "paste-294e9e4c42d34e7e10d1e612f669fd6a65f7ddfe.jpg",
        "paste-297dd7624892ca12ddf629143366e2f8d1c4d8ac.jpg",
        "paste-2af56cafd950e0cbb567702bd08afa9401b7937a.jpg",
        "paste-2b624bc74bcabf69354245109a1ee8118c3d4cdb.jpg",
        "paste-2b6d43694ad888ce4151e2888746d9f2f9952e72.jpg",
        "paste-2bf528e8bb77d803fe196947ba0326ee11a0c769.jpg",
        "paste-2c721c419c4aedb03939cf508937104821c78f35.jpg",
        "paste-2d0f19178b203a065eee932cf47cf1d25492b039.jpg",
        "paste-2f4f0fa840ba16db307f5e7698a7b0fab753580d.jpg",
        "paste-2f7652a98b89ce1f1e8b4f09806ef411108d5590.jpg",
        "paste-2fc56680fd4b4c0aaac3e0bdf145b2f723e2b889.jpg",
        "paste-304edf77b40cd69e88519ef4305ec8031f846847.jpg",
        "paste-30963476b82950f7b2013dd41ecb653605a76af4.jpg",
        "paste-30c17064fb9ec75c9fbb193ea85e93fc811b0aff.jpg",
        "paste-317c1546c1203e39c4968ecd3daf859ec3d46ea1.jpg",
        "paste-317cc996cd21a24e3569047e5b49a1b0e711b2fa.jpg",
        "paste-32379a0e5a47060e1c31caf229a0020844e3c8d9.jpg",
        "paste-3247dea264178f5d22e247946aa1bfca23fc5bfa.jpg",
        "paste-3328f29f8fbae67b793800c5aaf275c9f6b419f8.jpg",
        "paste-332b5e5cfd47084ef40b008299f0fedcf38ad03b.jpg",
        "paste-33c4a58f14918968a81a7d1a077a91c3746a3c23.jpg",
        "paste-3425bbeea1747147eafd63ef514039faf2ba9f19.jpg",
        "paste-3555a7924e0d98470a5742f4d5329ee08cea8d3a.jpg",
        "paste-3769af4173894b3f6570ee5d6055100c042181d1.jpg",
        "paste-39302a9e4c4b19eff0ea931cf45e5d285f0e52e6.jpg",
        "paste-3ab4c907cd64f3c98d7e8486a01b40b24490ae40.jpg",
        "paste-3acaddd2a21ffbf8d1e25b6029c2a82d765b1590.jpg",
        "paste-3b068a67028d2fd37628996e5aee0d591eec4b9a.jpg",
        "paste-3b34a29d8f42307be988c1ae4f9d00b888ac8aec.jpg",
        "paste-3c7b264556f08af3bdb44584095b2fd85c2ca0db.jpg",
        "paste-3d31de6c9e08eba3e99436ef3a4670c1695aa9d2.jpg",
        "paste-3d570dea05c02022deb071443e38b9efd805201a.jpg",
        "paste-4024b838fe787867c329d557229fef38d3ca19de.jpg",
        "paste-403714400f63227de204855151ee2448cb59f6b7.jpg",
        "paste-418e305a854e291d8a0408735c60ab3b9e6df56f.jpg",
        "paste-42ca6ae3f94b1aab2f918c407b03d1eb87a22d67.jpg",
        "paste-43d9956fcd57c2e2459153d6306d0a554ba06031.jpg",
        "paste-4417a05617c84f5ec6d53b092b3ee788add37f83.jpg",
        "paste-44e816d412d434c74406f217b38f8056e78f4f1f.jpg",
        "paste-451428472900b303aadce2d023f2e69744d73d1a.jpg",
        "paste-457b62ee8e12e5c8aed91c0f1ee159930cc655c3.jpg",
        "paste-45c3a19d6442fba912749666c8206a8acb5447f3.jpg",
        "paste-45ccbfdf0ff9a07d832050b595e3cabd4a9e3103.jpg",
        "paste-469b9dfc71b07040d09b8d9bef357e46781915a0.jpg",
        "paste-47afc056d248665879e4b7ebd51d16ba73463b97.jpg",
        "paste-482a887dd52a089d02bbac5cee63a8ef89aaa8b5.jpg",
        "paste-48c72e2bd7fcaa1e3ca1123c2f664a2f98e163be.jpg",
        "paste-4a5cd15f5efd5d6e9747aed1bfdfa65c1ba9fc97.jpg",
        "paste-4b526a7803daa2d53b8d65b68011d4e7a4e1868d.jpg",
        "paste-4d7d8a7f32e406a237944f3d2441c8209362db22.jpg",
        "paste-4d93a61a959ebebddcacddf9c400e114160c7004.jpg",
        "paste-4df8a6fac3ee8ff605b2587946a6a1fc2eacc76d.jpg",
        "paste-4ed2168d3b2b001c9c8a69a8499773d2e8206d77.jpg",
        "paste-4faa76c082caf71ba5f6aa5b1fee6e5eb8c34dd1.jpg",
        "paste-4ff51f5cbde13fe4ada98cb70b204f67f7312e7d.jpg",
        "paste-506cb1143e1ca5a2f34884d4d79cb9ae27c29f77.jpg",
        "paste-50be55afb0ae18a879c893973878ce4a93b8b659.jpg",
        "paste-52605f4aa09c0442e5a62a1732730ad3cda84c58.jpg",
        "paste-52e3e688dede4ff5d49a42630f38adf21f1a8721.jpg",
        "paste-54045d30f572c478b1e0ca22fe7174d914d20dad.jpg",
        "paste-54777724a6625f065aac510ced0683e35c345d6b.jpg",
        "paste-548e2866cb60fdd14bb1911f6be06810faea5840.jpg",
        "paste-54b9f2aa2a98dcd6f5a6c931853fd147a9e34b8b.jpg",
        "paste-55e11d4b799768028208abc2214294eead9019dc.jpg",
        "paste-56109689030757a42b52c88f56fa3c94efbaf258.jpg",
        "paste-56ef98dd0d05d922142f2b8c483028feb8358e6f.jpg",
        "paste-58b21ec371ffde421dc7aaad4f8c3f13e309c60a.jpg",
        "paste-58c77b2aea46543e4080a8b4b0a83959d25a4ad7.jpg",
        "paste-58c7f2867f1e1ad7f827df5bc8422c9ace7ab89b.jpg",
        "paste-5900a33426e7d0b291955e6a8583e98b2b156398.jpg",
        "paste-599e081c9312f6717ea86e336f828e1e0a8b0658.jpg",
        "paste-5ad888ff62f21cf2ab6790635bc67e8109c230c3.jpg",
        "paste-5baf2e4075678e9bcee1aac407046b85f241d7cb.jpg",
        "paste-5bd49da3b0b94622f058657d8e94b27c1a7bef61.jpg",
        "paste-5c3bf6b2ff2e50479fcc10d04eb64343d37fd027.jpg",
        "paste-5d0cf353be53c6dfa54a59481b3b79ef22d843c6.jpg",
        "paste-5d9dc6a3f3d6ae9d326f73f69bb61b2dbac00634.jpg",
        "paste-5de2d2d0a25f144cdb0e496f3bc3bbf9952a7171.jpg",
        "paste-5e624e0405e2d43b055ff6b7555d216fd1473d81.jpg",
        "paste-6066f0488c507364d11b1b100ba8d635c1d087e1.jpg",
        "paste-60934bee6a350ebe264dc0bbe7cad6bab5723dd0.jpg",
        "paste-61808f2da8a62723216eb4bb649e3c6a7eec98ca.jpg",
        "paste-63996316413844fd2e89e376fb4f25f9ce1cf015.jpg",
        "paste-64681e1ce28bb6eab65bd029f571e53bb641ff57.jpg",
        "paste-655e45ce746fe051a9f9ef914863fec0febf43ec.jpg",
        "paste-657956ac05a39f3da757204cdae689ab6aa220be.jpg",
        "paste-6618833ff5f21027e6a65c66038288137dad1e7e.jpg",
        "paste-664f58cf5b203e5c3c013f1b81974a550a554747.jpg",
        "paste-66c015b7f610336ea1b4aa4bf7d00c4eb89e29a3.jpg",
        "paste-67067edfbf3988d40e6df9067564e6b098cb2ef6.jpg",
        "paste-6869b08b3c8739dd047ae93cdb095a962e991382.jpg",
        "paste-6981b1bfdd56b0530ac3423294299fe101bbf63e.jpg",
        "paste-6bd4b2dd58a6ddbb0e3340d175173adaf0a4ebb5.jpg",
        "paste-6d705b5698d78a41552119766a0937f33aa1d4c8.jpg",
        "paste-6d9e0f24ef3ff7ae362fb583a12d6efda733a7b7.jpg",
        "paste-6dd28699613d365694c01a25077499eae9831bc4.jpg",
        "paste-6e3a316db5bdb5059f44ae76b2c676141c16e621.jpg",
        "paste-6ed4cb4db01f7ab284fe8745140b45a3ca9d89db.jpg",
        "paste-70079154169e6cf8fa504990ec76229c99f57de5.jpg",
        "paste-72fcfd247d80544e3dfa7a5f5eaecd86313786cd.jpg",
        "paste-737ea761d23452b9f711d8f6cfb4b5590f0c863f.jpg",
        "paste-738c435a7a53bcb9399576d5c709440d3c2a26bc.jpg",
        "paste-738d4a0f2e0f403309fb844cba67dce125eee98d.jpg",
        "paste-73c77fb390f3082d2b4a28bd834ef1cc47b924db.jpg",
        "paste-755c925a62bfc10b6ed08fa50443777f99fa551d.jpg",
        "paste-76f6c98beee0811b370ab4201ee6b1b6088ad884.jpg",
        "paste-7780261789fd30bc60d1aa75bd7935bbe972ac36.jpg",
        "paste-782a3499ad72473942795f70ea31f726f7a7f6f0.jpg",
        "paste-78370bbc47ca102d68a6de1ebbbaff5ebf6031d2.jpg",
        "paste-79673edf754f59b2659b9d3787eeefd8035c2185.jpg",
        "paste-7a6482ee5d1a30d1a8621ebe298a236a77ad2c3a.jpg",
        "paste-7b42642087ca77b8ce7a1a0796b5e802ba1fd873.jpg",
        "paste-7d2051b8ac2b10122dc64d304e395c3a17ed7dd9.jpg",
        "paste-7e9a2f6bffa7b0d82bc79d08d27fbad73e9d08fa.jpg",
        "paste-80c96d8b7480dd1908cffac23478d54ce0bad605.jpg",
        "paste-8185a09237df01bfa309c5670ae016a5c89da1e7.jpg",
        "paste-81a466385593a92e4271349a11793a84270ba0cd.jpg",
        "paste-81d78cc1e008a3af42970e15b2982dd734bab181.jpg",
        "paste-82a2d8672aa3c143217af2b898eeacbcfe7905ee.jpg",
        "paste-82aada586bf6d3142f9365acc8f03f13b852596b.jpg",
        "paste-830fcd81093a7670a3140a1de9bea91b34b571d5.jpg",
        "paste-838febabac9024fa3e6436eb3936e233a7540fa4.jpg",
        "paste-83e52836349ced9e73bcc3a2886148228c1ee7cd.jpg",
        "paste-85a40e97bc8ca51553bb0a72d4749f9df36c6114.jpg",
        "paste-85df9bb180cfc08df6e8d8ab3bcedb63a460dc6b.jpg",
        "paste-87776b5f92b11e71cc42e802fe70751b1fa7886d.jpg",
        "paste-87fcc822dc9ddd16c1dd90cabf81c7b2cd9bbf90.jpg",
        "paste-887ce7f8c9c42ef7bf5a05cf1afb1444ab360d48.jpg",
        "paste-8944a416c2aff3101e8de7ba91df5eb68e35932a.jpg",
        "paste-8bc818db865ebfee26473515a6fea4d404e5f818.jpg",
        "paste-8c3366c4c273a57f0546432ae3c0ce7b47fe1b32.jpg",
        "paste-8c68e82d8a545c4d40144f3b7d6e305075f947b6.jpg",
        "paste-8ce8ae8a5c56ba3a06f0ef5f70b9f99f9774dbc4.jpg",
        "paste-8e804226c87e81ef0ae579d897618d88159d2fd8.jpg",
        "paste-8ef2fbc44c81120e0c6247cc1ea09721ab957da7.jpg",
        "paste-8ffb06ee34372d426d7cf273de7fdd1e58f5b9d5.jpg",
        "paste-9005c9e40282f7670c601bcedc5031c76fb422ad.jpg",
        "paste-903a902b305b6352c36549a4f9e3c471e8bc54d8.jpg",
        "paste-91103479777465e4fdeb255e7b0eebaeec1a30de.jpg",
        "paste-91a2711d28eed5b2e0808c5a38ad33fa21df5ff8.jpg",
        "paste-91fb6ac845ac09ba3d94afaa85ef64d529b04889.jpg",
        "paste-931e71a5385ce9c9266c2573c9037f48e51822f7.jpg",
        "paste-93664f8e30cb0c31d519b90192ed2274a22eedf8.jpg",
        "paste-973c3547647fe0bcde1206b14fcafab34de4e4b0.jpg",
        "paste-97c8eebfbc98e3d967e85e6d5ac4d546f3fb4e4d.jpg",
        "paste-98e69cc8b78d60a0045ba4e224825afcc05fe8ff.jpg",
        "paste-997553b5ac4186b4d6e8ed82a61c9d597ed83893.jpg",
        "paste-9a8ad76effc5c2481303a0a57fc7d17f6c641910.jpg",
        "paste-9b37081e9fd399acab9728744b6c7f4689c5efb4.jpg",
        "paste-9bb862de47a6f79008059b54abc38c38cbfcb6ba.jpg",
        "paste-9cc0449def2ad66080fe40811c474509f7a16049.jpg",
        "paste-9d47ffbabccb221b576555ace0c86e6899173b4f.jpg",
        "paste-9e186cf04192205626e56fee41b87802d390944e.jpg",
        "paste-9edcb5a74ff9c8c568285eda8dd0ca3a6710657b.jpg",
        "paste-9f3340fbfb2a6eacb14f9572c9b4cbfcdfaebef0.jpg",
        "paste-9f97deafddb34a4d4a7b617ff14735b8dc724dfb.jpg",
        "paste-9fed57e45185ee7f381431f70ceb6c287e410718.jpg",
        "paste-a0640992b26f65f3804ae02db77b1a5def55d21b.jpg",
        "paste-a0ebfee0516dac2d560a467f68385ba4c6820273.jpg",
        "paste-a113a9af94fa6327970b06d8d4e7e359d0eecc5a.jpg",
        "paste-a11a6986fac3d7e1674504516a6c8f7ae1a03d9f.jpg",
        "paste-a17b89881dd1d1256a09f35ea7c50c077ad1774c.jpg",
        "paste-a2a51799c5613b8b570d6ecbad2c8cb1eab3fe29.jpg",
        "paste-a2ca1b44fd69a97b85a5bf550c588099fb7b471e.jpg",
        "paste-a3ee0b560b26281eb3cd623b5cb7addc6eb7f1d0.jpg",
        "paste-a4c59bc90b61f37366056f4ebee728aabe35175a.jpg",
        "paste-a510c39f639951e7326af12ca62c31167ec826d0.jpg",
        "paste-a5d102b03a54da8e83d66ea65e5d696f73c78bb9.jpg",
        "paste-a6916e3524fa8b221406150c1bfe4ba991b05208.jpg",
        "paste-a7e62a120818080a897d70243488c01b3c60159b.jpg",
        "paste-a8472babb1400070a6e3fc88c4e23e93b0f6f256.jpg",
        "paste-ab4f56d3d662b0d8adaf1725fc24d46e908fbda4.jpg",
        "paste-ac8112e16455063cbcb82c1ed13860844cb264ff.jpg",
        "paste-ad2b64415d5ed9a3da90cacf77ed1e7d8baf2c42.jpg",
        "paste-ad8e6bbca827fb2bd4570fe30a5a727b1fd0d4f3.jpg",
        "paste-ad9fa24795dde3f49ea22eeb5d21963b5062e40f.jpg",
        "paste-aeba2d19d5544744a5cfb431f26f8324684e9d05.jpg",
        "paste-b02ddc574bcd3b6c9c3729e1bc5d8dddd418af91.jpg",
        "paste-b32df738d4ff168eec37c25ed0a5d7a4605e15fd.jpg",
        "paste-b52bfdd9f0120194a607f158e4f12b9b7456ac10.jpg",
        "paste-b58b4caffc3fe3cd40d6dac572be2c815cc55d80.jpg",
        "paste-b787a56dae16ba09df0eb0a52952d2fbc1df94d3.jpg",
        "paste-b8cc3746e416849bba85b54c74221dfa8a28038d.jpg",
        "paste-b9f6668a2b1d50faac7e4d76ad781d4065689060.jpg",
        "paste-ba5aa4733fa77deb9fc04a339f78c672d5ada772.jpg",
        "paste-bb7204bb9d0e4afb17de88c08de0e0d33468ee01.jpg",
        "paste-bc2ade651cf9e2e92140f54a47d1b805cdd8bfdf.jpg",
        "paste-bd40421692b51039e97b050744017ef77be4b7c4.jpg",
        "paste-be4e723174991ade35931612a711a714d14ca119.jpg",
        "paste-be8d0286c07aa572989a628333fd23dc6dfad4f5.jpg",
        "paste-bec8bb8f3d76d5e94512ff2af9266de35b9bf0f1.jpg",
        "paste-bed2fa4a0d92b759bdcd18fce24c1bdee72aca3a.jpg",
        "paste-bf409e64da466adc427ef0c6bbc48625f0fea0bd.jpg",
        "paste-bf907aa44342a1c56f589a89d4fe8db13e3d06a2.jpg",
        "paste-bfe1d8aaad5f12dc43d008bdeaf7fa5a812266f4.jpg",
        "paste-c24a64ed279d59ca0371c0b707e98ea0fe010904.jpg",
        "paste-c28635903e4cacf58e2cd4c7bcd93818cfe5f9fd.jpg",
        "paste-c2f4da9e651d147ec2be2e05d76aa28e849ec3e0.jpg",
        "paste-c31c53f1eb1eed3670242a6db8307e7c4f3369ee.jpg",
        "paste-c324d8bccbf3e161653077e8b916ed0ce4af91ca.jpg",
        "paste-c3ece164e96901bb96c7095fb58bfad0ad9721d9.jpg",
        "paste-c67e6489b051b882d1321ed022ed27f8fb02099a.jpg",
        "paste-c6b67a50c3807984c8d2e3c1b9c15413c50007ce.jpg",
        "paste-c8562ce7a3bf6240c42b9812136af939327316fa.jpg",
        "paste-c97b30b7dc85a753e63f2b99bb00d90e2d72eb96.jpg",
        "paste-c983775819086b1543ac0c03c90f9d6cf16cfb63.jpg",
        "paste-cab7beb9772de88b0a0bd5fe4396eb156cdfbd7a.jpg",
        "paste-caf8b352f892a58b68a9e7b4ad46bd79b4601262.jpg",
        "paste-cb4c91bbb05e86bff147aad9a00eba584049bbeb.jpg",
        "paste-cd90fe01f6286f6090f9c822ae1bffc81b3217f5.jpg",
        "paste-cef2cc7a7f38f1000eb7ad0e9dc539c79d4d4f5b.jpg",
        "paste-d020223182cf852dfaf0421f11d742d1a6ace915.jpg",
        "paste-d04fa2fdfb325f5ee3b7f3be5d5a1e8ef068ab72.jpg",
        "paste-d187e1f6869401a55297a83f9931340a0c38c65c.jpg",
        "paste-d1b329cddf0808ea15f70e106019645f46300275.jpg",
        "paste-d1fed0acf4019f8220b18c977e226cb8bc0da754.jpg",
        "paste-d3ceb8c583fd9c24410b603929373107f03f7eee.jpg",
        "paste-d68194029ed87bccd06fbc90cd1dfb827da3f591.jpg",
        "paste-d8032a3f82abcab07b4450da7cc0aa5bb34bb74a.jpg",
        "paste-d806069343b9645943b424d12f6de09a5780e059.jpg",
        "paste-d87ed0531c040f53daee8e324a03eae8e8291edd.jpg",
        "paste-dac78ea69bca731bc67ef8a789266d91ff4438f2.jpg",
        "paste-dad1f31867490f93a1aed8be5274a5a74e92b0ee.jpg",
        "paste-db810e660d1e98de030e8cc725d1cc6f7ac482ec.jpg",
        "paste-dd5b8762ffb3881b64c53fb766ecc4464a913f70.jpg",
        "paste-df8aa052447d7ab7c4aa0372948a520c696dff63.jpg",
        "paste-e0466af0340b5990d79f8ac048e9cae07b7b6909.jpg",
        "paste-e0a4be7b9804895b5ef5f1c3f6b7a615b3c2fe09.jpg",
        "paste-e0b4c82202836e8480bacae0f55544b14709feac.jpg",
        "paste-e0bd31f0877619d1b0186625ab0457e00b3025c4.jpg",
        "paste-e1488de42eba716e09dfefd66fd4e554c7882d3a.jpg",
        "paste-e14ff42f2792c75fac801a7fd0804c92159c0e71.jpg",
        "paste-e1d7f5dd1983461c2c5b91de785fe0d78dc7bba9.jpg",
        "paste-e46c1e291b6f3f2b6e43be4384aa614df863c6df.jpg",
        "paste-e47f2bd70eed027f29a4e1a7b9380fabd4c0008f.jpg",
        "paste-e548184e3400ee1010f08e3be4e6b9daa0585f9a.jpg",
        "paste-e57d18e37aba7192630e5696bed02dddaf6bd61e.jpg",
        "paste-e5b9d5be1e4aa03baaaab156b48896cb6d36aaa3.jpg",
        "paste-e5e3b9da0df9c8bf4a2cc600b2530c17aa65f2ae.jpg",
        "paste-e5e4b96f5b0eede1f0f37e5f8990c46e7555e879.jpg",
        "paste-e6422bb0d09f13b99ec89f7b5ca54c1b21f8f85d.jpg",
        "paste-e7d80da2e19b615613012e61b9e58ef76e6b2746.jpg",
        "paste-e967ef1bd67619a982e167da114d0b29479a80be.jpg",
        "paste-ea8f40d52f5be8c12c22f202563ce9d1f9845b21.jpg",
        "paste-ebca703d814df5026017db3210024c2a7bc267d4.jpg",
        "paste-ecb98386498b5511cdf857a7bab1e676112fbd5c.jpg",
        "paste-ed3d09bf876884254d4494da3ec4186111f43cc2.jpg",
        "paste-eddb3c7a16cb90abc3bca2d3c86eb42a17a95330.jpg",
        "paste-eeca0f36d8f4f60ea0a701e1ae371587d4a47451.jpg",
        "paste-ef45a4211df55ae5025cffce4e32391813ac5cee.jpg",
        "paste-f137e8911897863666a51272a6299679e571e0bd.jpg",
        "paste-f14bc5e16577f912a4939e6d87d4d1751328634b.jpg",
        "paste-f34ea77f074496495b2233397c0adcb3941acbcd.jpg",
        "paste-f365a5f98e8d9e4dffd0def3b7cdc75697fd27fc.jpg",
        "paste-f3695e94fbe25885492143f801beaae654778368.jpg",
        "paste-f3b0d5def21168abf7f0ddefa23018cd8e4bcba7.jpg",
        "paste-f4629e6ecef9fd0f4a560eda513f742a7e898788.jpg",
        "paste-f5dc75ba051a99e6cda769613ce5dcbd62bd33f6.jpg",
        "paste-f8800ab83180fbb26c810466f256d7bc2c620dcc.jpg",
        "paste-f8903d874b463936c8d15593be0d2481912eddf6.jpg",
        "paste-f8e30978e0904378086524d07374d530e021b55a.jpg",
        "paste-f8e8236894b0893eb9c92a554f6346add0873989.jpg",
        "paste-f8ed8289dd5f5db2ddd3e85afc10c84ad6e0bc7c.jpg",
        "paste-f9eff7714585e49ba4a418d7d136638a3f47bebc.jpg",
        "paste-fab6fadd9d2ea5b29bb71abf162638fd40021857.jpg",
        "paste-fb539a1e26aa7c27a7dd7a676278d3afbe88cb72.jpg",
        "paste-fba65d001b8e1a9d997ba993bd4509122620b436.jpg",
        "paste-fc20d98b336d1c6124d0f423d7f44ec2defc2908.jpg",
        "paste-fcd4ea04e9fb20e6fec8937d1ce6ab1e6fcab292.jpg",
        "paste-fcfc8f72cc2daf428023c2718a96fa9e8101440a.jpg",
        "paste-fd02b48b8683d7e584dfc6f659f9ec46c48ffaf7.jpg",
        "paste-fdbe8b903808c2cd9cb1ecf5f65c08658ac92fc4.jpg",
        "paste-fed19aea15e6b3acbb499478e1d6c7c0016b53d1.jpg",
        "paste-ffc35ace9eb5a491e00fddd9cb38fe63d1421704.jpg"
    ],
    "name": "Data Structures and Algorithms",
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n",
            "flds": [
                {
                    "description": "",
                    "font": "Arial",
                    "media": [],
                    "name": "Frente",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "description": "",
                    "font": "Arial",
                    "media": [],
                    "name": "Verso",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Básico",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tags": [],
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Verso}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "name": "Cartão 1",
                    "ord": 0,
                    "qfmt": "{{Frente}}"
                }
            ],
            "type": 0,
            "vers": []
        }
    ],
    "notes": [
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-e0a4be7b9804895b5ef5f1c3f6b7a615b3c2fe09.jpg\">",
                "<img src=\"paste-43d9956fcd57c2e2459153d6306d0a554ba06031.jpg\">"
            ],
            "guid": "r5lAd@DP1Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-b9f6668a2b1d50faac7e4d76ad781d4065689060.jpg\">",
                "<img src=\"paste-f8ed8289dd5f5db2ddd3e85afc10c84ad6e0bc7c.jpg\">"
            ],
            "guid": "dazGr+$epu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-1019cffdc21d039085adc66be5b24a928060db22.jpg\">",
                "<img src=\"paste-3247dea264178f5d22e247946aa1bfca23fc5bfa.jpg\">"
            ],
            "guid": "i/=?Dj,]n2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-54045d30f572c478b1e0ca22fe7174d914d20dad.jpg\">",
                "<img src=\"paste-8185a09237df01bfa309c5670ae016a5c89da1e7.jpg\">"
            ],
            "guid": "fwJlek[Ul:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-506cb1143e1ca5a2f34884d4d79cb9ae27c29f77.jpg\">",
                "<img src=\"paste-830fcd81093a7670a3140a1de9bea91b34b571d5.jpg\">"
            ],
            "guid": "GCK7-N~|~Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-1b2677abb3718cee7d30cd7f918b9f0dc2058e7a.jpg\">",
                "<img src=\"paste-c67e6489b051b882d1321ed022ed27f8fb02099a.jpg\">"
            ],
            "guid": ">=vy6hb2o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-07e070a38dc5b94908d78e90970a2c85f148f1a1.jpg\">",
                "<img src=\"paste-1d1112407221c3dcc2a0fac54938d5d1e40963ea.jpg\">"
            ],
            "guid": "B2gTE3|{AD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-3ab4c907cd64f3c98d7e8486a01b40b24490ae40.jpg\">",
                "<img src=\"paste-e57d18e37aba7192630e5696bed02dddaf6bd61e.jpg\">"
            ],
            "guid": "k}>UB_K*Xv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-58b21ec371ffde421dc7aaad4f8c3f13e309c60a.jpg\">",
                "<img src=\"paste-0a589a7abadbdaebcd06f45344dbbf3e8de900d5.jpg\">"
            ],
            "guid": "c%IE`9~GE7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Recursive routine to print an integer&nbsp;",
                "<img src=\"paste-755c925a62bfc10b6ed08fa50443777f99fa551d.jpg\">"
            ],
            "guid": "ILY;3Njnbv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the time complexity of binary search?",
                "O(log N)"
            ],
            "guid": "M<p=D4]vf}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of array as list",
                "Insert: O(N) at the beginning and O(1) at the end<br>Delete: O(N) at the beginning and O(1) at the end<br>Access: O(1)"
            ],
            "guid": "D_{.dlq$=x",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of simple linked list as list",
                "Insert: O(1) for head, tail, and known location. Otherwise O(i)<br>Delete: O(1) for head, tail, and known location. Otherwise O(i)<br>Find i: O(i)"
            ],
            "guid": "A)]3ChV{Si",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of stack",
                "Usually implemented using array<br>Insert (push): O(1)<br>Delete (pop): O(1)<br>Access (peek): O(1)"
            ],
            "guid": "s]b%IX8+hW",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you check for balancing symbols?<br>Ex: The sequence [()] is legal, but [(]) is wrong&nbsp;",
                "The simple algorithm uses a stack and is as follows:<br>Make an empty stack. Read characters until end of file. If the character is an opening<br>symbol, push it onto the stack. If it is a closing symbol and the stack is empty, report<br>an error. Otherwise, pop the stack. If the symbol popped is not the corresponding<br>opening symbol, then report an error. At end of file, if the stack is not empty, report an<br>error&nbsp;"
            ],
            "guid": "A8U|k~9qGB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you evaluate postfix expressions?<br>Ex: 4.99 1.06 ∗ 5.99 + 6.99 1.06 ∗ +&nbsp; is the postfix expression of 4.99 ∗ 1.06 + 5.99 + 6.99 ∗ 1.06&nbsp;",
                "Keep on pushing numbers to stack and when a mathematical symbol is encountered pop two values, evalute the result and push it to the stack.<br>Continue reading from the expression until you have one number in the stack."
            ],
            "guid": "JSMSLu9qpx",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you convert infix to postfix expression?<br>Ex: a + b * c + ( d * e + f ) * g&nbsp; &nbsp; to&nbsp; &nbsp; a b c * + d e * f + g * +&nbsp;",
                "Create a stack for storing mathematical symbols and an array for numbers<br>Read numbers from infix and add them to the array. When a symbol is encountered push it to the stack.<br>If the read symbol has a lower precedence than the symbol on the top of the stack, pop all the symbols and add them to the output, and then push the read symbol to the stack.<br>Treat parantheses as symbols of highest precedence.<br>Parantheses are not output until the closing one is encountered. When that happens, output all the symbols till the opening parantheses.<br>When the end is reached pop all the remaining symbols and add them to output."
            ],
            "guid": "Gr+%+P-P2[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implementation of queues",
                "Circular buffer using arrays<br>Insert (enqueue): O(1)<br>Delete (dequeue): O(1)<br>Access (peek): O(1)"
            ],
            "guid": "Hh*WD5Afa{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average running time of most operations in binary search trees?",
                "O(log N)"
            ],
            "guid": "t3+h,Gd9C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call A?<br>What is the relationship between A and E?<br>What is the relationship between J and E?<br>What do you call the connection between A and E?<br><img src=\"paste-76f6c98beee0811b370ab4201ee6b1b6088ad884.jpg\">",
                "A is the tree's root<br>A is E's parent node<br>J is E's child note<br>The connection between them is an edge"
            ],
            "guid": "Q$1P1@lD<8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call P &amp; Q?<br>What is the relationship between I &amp; J?<br>What is the relationship between E &amp; Q?<br>What is the relationship between Q &amp; E?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "P &amp; Q are leaves because they have no children<br>I &amp; J are siblings<br>E is Q's grandparent<br>Q is E's grandchild"
            ],
            "guid": "Jp_sM%;(Ne",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call all the nodes between A and Q?<br>How do you define its length?<br>What is a node's depth?<br>What is a node's height?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "The nodes between A &amp; Q are called the path between them.<br>The length is the number of edges on the path.<br>A node's depth is the length of a unique path to the root.<br>A node's height is the length of the longest path between the node and a leaf."
            ],
            "guid": "BQ#9fkXV[,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between A &amp; Q?<br>What is the relationship between Q &amp; A?<br>Why is the relationship between them is considered to be proper?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "A is an ancestor of Q.<br>Q is a descendant of A.<br>A is a proper ancestor of Q because A!=Q since each node can be its own ancestor and descendant."
            ],
            "guid": "x81oXD`|]t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which tree implementation reduces wasted space?",
                "<img src=\"paste-8ce8ae8a5c56ba3a06f0ef5f70b9f99f9774dbc4.jpg\"><br>Using linked-lists for siblings and connecting the parent to the first child only, avoiding the need for an array in the parent that point to each child."
            ],
            "guid": "szRjEjYz_o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is preorder traversal?",
                "In a preorder traversal, work at a node is performed before (pre) its children are processed. Ex: directory names are printed before the files inside them."
            ],
            "guid": "sloEhJ8_eQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is postorder traversal?",
                "In a postorder traversal, the work at a node is performed after (post) its children are evaluated.&nbsp;Ex: print file sizes before directory size."
            ],
            "guid": "AMMtnBcNHL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average depth of a binary tree?",
                "O(√N)&nbsp;"
            ],
            "guid": "M(}Skt2*S0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average depth of a binary search tree?",
                "O(log N)&nbsp;"
            ],
            "guid": "saN%I+T#9P",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum depth of a binary tree?",
                "N-1"
            ],
            "guid": "Q$c6}xE0A/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>expression tree</b>?",
                "The leaves of an expression tree are operands, such as constants or variable names, and the other nodes contain operators. This particular tree happens to be binary, because all the operators are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator.&nbsp;We can evaluate an expression tree, T, by applying the operator at the root to the values&nbsp;obtained by recursively evaluating the left and right subtrees.\n<br><img src=\"paste-82aada586bf6d3142f9365acc8f03f13b852596b.jpg\">"
            ],
            "guid": "n~(/V5q:zK",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which kind of traversal is used to convert an <b>expression tree</b> to infix expression?",
                "We can produce an (overly parenthesized) infix expression by recursively producing a parenthesized left expression, then printing out the operator at the root, and finally recursively producing a parenthesized right expression. This general strategy (left, node, right) is known as an inorder traversal; it is easy to remember because of the type of expression it produces."
            ],
            "guid": "h=`,D0m;wh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you construct a tree out of the postfix expression?",
                "We read our expression one symbol at a time. If the symbol is an operand, we create a one-node tree and push a pointer to it onto a stack. If the symbol is an operator, we pop (pointers) to two trees T1 and T2 from the stack (T1 is popped first) and form a new tree whose root is the operator and whose left and right children point to T2 and T1, respectively. A pointer to this new tree is then pushed onto the stack&nbsp;"
            ],
            "guid": "u%Yp)]]kd!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When a binary tree is considered to be a binary search tree?",
                "The property that makes a binary tree into a binary search tree is that for every node, X, in the tree, the values of all the items in its left subtree are smaller than the item in X, and the values of all the items in its right subtree are larger than the item in X.&nbsp;"
            ],
            "guid": "cwHxS[]Yk4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What kind of recursion can be replaced easily with a while loop?",
                "Tail recursion"
            ],
            "guid": "oL^3uFjx}I",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an alternative for inserting duplicates in a binary search tree?",
                "Including the frequency of occurence in the node instead of adding new child node which tend to make the tree very deep.<br>If counting the frequency is not enough we can use an auxiliary data structure to store all the duplicate instances in the node, such as a list or another tree."
            ],
            "guid": "p5fOSi@?Cv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find the minimum and maximum in a binary search tree?",
                "The minimum is found by recursively traversing the left item tell the left item of the current node is null.<br>The maximum is found the same way traversing to the right instead."
            ],
            "guid": "rjPBi_LYf/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you remove a node in a BST?",
                "1- If the node is a leaf we can just remove it<br>2- If the node has one child we bypass it by making the nod's parent point to to the node's child<br>3- If the node has two children we need to replace it with the smallest (left-most) child in the right sub-tree.<br><img src=\"paste-091b2dc77c1ea227303a50ab376e0c6c88af5112.jpg\">"
            ],
            "guid": "o*qQ[`#Z)l",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we call the sum of the depths of all nodes in a tree?",
                "It is called <b>internal path length</b>."
            ],
            "guid": "nj7G%Qd4NY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>self-adjusting tree</b>?",
                "Is a tree in which after every operation, a restructuring rule is applied that tends to make future operations efficient.<br>This does not mean though that the tree will be necessarily balanced after the self-adjustment."
            ],
            "guid": "FCfB>!B=*W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of self-adjusting trees?",
                "Although they cannot guarantee O(log N)&nbsp;for each single operation, any sequence of M operations on the tree takes total time O(M log N)&nbsp;in the worst case."
            ],
            "guid": "qe]u4x`>&s",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an AVL tree?",
                "An AVL (Adelson-Velskii and Landis) tree is a binary search tree with a balance condition. The balance condition must be easy to maintain, and it ensures that the depth of the tree is O(log N)&nbsp;.<br>An AVL tree is identical to a binary search tree, except that for every node in the tree,<br>the height of the left and right subtrees can differ by at most 1.&nbsp;"
            ],
            "guid": "xS7ZTX@g+=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the height of an AVL tree in practice?",
                "It is only slightly more than log N&nbsp;"
            ],
            "guid": "i=6dfTP$V=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is time complexity of operations in AVL trees?",
                "All the tree operations can be performed in O(log N) time, except possibly<br>insertion and deletion.&nbsp;"
            ],
            "guid": "F1~qM~jh81",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the name of the operation that restores the balance of AVL trees?",
                "The balance property has to be restored before the insertion step is considered over. It turns out that this can always be done with a simple modification to the tree, known as a <b>rotation</b>."
            ],
            "guid": "QS.C!X~mz$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you balance an <b>outside</b> insertion in AVL trees?",
                "The case in which the insertion occurs on the “outside” (i.e., left–left or right–right), is fixed by a single rotation of the tree."
            ],
            "guid": "br6zdGd1>7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you balance an <b>inside&nbsp;</b>insertion in AVL trees?",
                "The case in which the insertion occurs on the “inside” (i.e., left–right or right–left) is handled by&nbsp;<b>double rotation</b>.&nbsp;\n"
            ],
            "guid": "Aq@_*r:#nn",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform a <b>single rotation</b>?",
                "<img src=\"paste-252f1f33951d320e152e8fc4f9722865ee6e09e0.jpg\"><br><img src=\"paste-4d7d8a7f32e406a237944f3d2441c8209362db22.jpg\">"
            ],
            "guid": "Q89@7ALgdy",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform a <b>double-rotation</b>?",
                "<img src=\"paste-e5e4b96f5b0eede1f0f37e5f8990c46e7555e879.jpg\"><br><img src=\"paste-664f58cf5b203e5c3c013f1b81974a550a554747.jpg\">"
            ],
            "guid": "d4b$}M}*1T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does a <b>splay-tree</b>&nbsp;guarantee?",
                "a splay tree that guarantees that any M consecutive tree operations starting from an empty tree take at most O(M log N) time.<br>The problem with binary search trees is that it is possible, and not uncommon, for a whole sequence of bad accesses to take place. The cumulative running time then becomes noticeable. A search tree data structure with O(N) worst-case time, but a guarantee of at most O(M log N) for any M consecutive operations, is certainly satisfactory, because there are no bad sequences&nbsp;"
            ],
            "guid": "fpRFmJQr5.",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why do splay-trees have a simpler implementation and use less space?",
                "Because they do not require the maintenance of height or balance information."
            ],
            "guid": "BdfuC<7zs$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a splay-tree guarantee an O(M log N) amortized time for operations?",
                "The basic idea of the splay tree is that after a node is accessed, it is pushed to the root by a series of AVL tree rotations. Notice that if a node is deep, there are many nodes on the path that are also relatively deep, and by restructuring we can make future accesses cheaper on all these nodes. Thus, if the node is unduly deep, then we want this restructuring to have the side effect of balancing the tree (to some extent)."
            ],
            "guid": "ih~$PLR(0?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Descripe how splaying is done",
                "Let X be a (non-root) node on the access path at which we are rotating.<br>1- If the parent of X is the root of the tree, we merely rotate X and the root. This is the last rotation along the access path.<br>2- If X has both a parent (P) and a grandparent (G), and there are two cases, plus symmetries, to consider. The first case is the zig-zag case. Here X is a right child and P is a left child (or vice versa). If this is the case, we perform a double rotation, exactly like an AVL double rotation.<br><img src=\"paste-3555a7924e0d98470a5742f4d5329ee08cea8d3a.jpg\"><br>3- Otherwise, we have a zig-zig case: X and P are both left children (or, in the symmetric case, both right children). In that case, we transform the tree on the left of Figure 4.49 to the tree on the right&nbsp;<br><img src=\"paste-a8472babb1400070a6e3fc88c4e23e93b0f6f256.jpg\">"
            ],
            "guid": "Hsesqc&v!w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What effect does splaying has on the depth of the tree?",
                "Splaying not only moves the accessed node to the root but also has the effect of roughly halving the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels)&nbsp;"
            ],
            "guid": "Kj0-{K}NVw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which kind of traversal is used to list all the items in a BST in sorted order?",
                "In-order traversal"
            ],
            "guid": "g~Y,r0cOGY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the total time for listing all the items of a BST in sorted order?",
                "O(N)"
            ],
            "guid": "gcO=>Sp;fY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which order of traversal is to calculate the height(s) of the node(s)?",
                "Post-order traversal"
            ],
            "guid": "F:N?~r,HQz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which order of traversal is used to calculate the depth(s) of node(s)?",
                "Preorder traversal"
            ],
            "guid": "C>V#;(b.@{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call traversal that lists the nodes of depth d before depth d+1?",
                "Level-order traversal"
            ],
            "guid": "E-@2|zeTFc",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is used for level-order traversal?",
                "A queue."
            ],
            "guid": "i(1GvS8]Ot",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What trade-off are we making when considering B-trees instead of binary trees?",
                "We want to reduce the number of disk accesses to a very small constant, such as three or four. We are willing to write complicated code to do this, because machine instructions are essentially free, as long as we are not ridiculously unreasonable. It should probably be clear that a binary search tree will not work, since the typical AVL tree is close to optimal height. We cannot go below log N using a binary search tree. The solution is intuitively simple: If we have more branching, we have less height.&nbsp;"
            ],
            "guid": "pt5h)2nSxz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the height of an M-ary tree?",
                "A complete M-ary tree has height that is roughly:<br>&nbsp;<img src=\"paste-0b92770d84c2c2ca44a7d58cdd0ab83695b346d1.jpg\">"
            ],
            "guid": "H6Gk_kCkR!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a B-tree of order M?",
                "A B-tree of order M is an M-ary tree with the following properties:<br>1. The data items are stored at leaves.<br>2. The nonleaf nodes store up to M - 1 keys to guide the searching; key i represents the smallest key in subtree i + 1.<br>3. The root is either a leaf or has between two and M children.<br>4. All nonleaf nodes (except the root) have between [M/2]&nbsp;and M children.<br>5. All leaves are at the same depth and have between [L/2] and L data items, for some L<br><br>Rules 3 and 5 must be relaxed for the first L insertions&nbsp;<br><img src=\"paste-085e451dba547532239c93eb14e90dd6e17b7bcb.jpg\">"
            ],
            "guid": "pvTep/$0GZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you choose M and L parameters for a B-tree?",
                "M (number of branches) and L (number of data items in a leaf) can be chosen according to this rule:<br>Each node represents a disk block, so we choose M and L on the basis of the size of the items that are being stored.<br>We try to choose an M so that all the parent nodes would fit into a single disk block.<br>We choose L so that the whole leaf fits in a single disk block."
            ],
            "guid": "yz64*oy9`>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the approximate worst-case number of disk accesses required to retrieve an item in a B-tree?",
                "log<sub>M/2</sub>N"
            ],
            "guid": "gr;:S,7>RW",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What part of a B-tree can be cached to reduce the number of disk reads for all access operations?",
                "The root and the next level could be cached in main memory, so that over the long run, disk accesses would be needed only for level 3 and deeper.&nbsp;"
            ],
            "guid": "t/vq2wRsvB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is insertion and deletion done in a B-tree?",
                "1- If the leaf is not full, insert the item in the leaf and reorganize its data.<br>2- If the leaf is full, split it by creating new keys in its parent that point to a new leaf, and split the data equally between the new and the old leaf.<br>3- If after splitting a parent node is full, it can be split also.<br>4- If the root is also full, we split it and create another root, effectively increasing the height of the tree.<br>An alternative to splitting would be to offer the new data item for adoption in another leave which requires reordering the parent nodes.<br><br>Deletion is done the same way but in reverse (splitting is nodes is replaced by combining them)."
            ],
            "guid": "O{3zB6dy$?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of a <b>set</b>?",
                "An ordered container that does not allow duplicates."
            ],
            "guid": "etFj=%cVm{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between a list's/vector's and set's return type when inserting?",
                "Since sets do not allow duplicates, the insertion might fail and that is reflected in the returned value."
            ],
            "guid": "cT!E>##d]7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you speed up the insertion operation of a set?",
                "By passing a hint for the exact location an insert should be done. This would allow a time-complexity of O(1)"
            ],
            "guid": "oPJfYO=|,L",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call a collection of ordered entries that consists of keys and their values?",
                "A map"
            ],
            "guid": "j5~^wf/|!h",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>map</b>'s key?",
                "Keys must be unique, but several keys can map to the same values. Thus values need not be unique. The keys in the map are maintained in logically sorted order.&nbsp;"
            ],
            "guid": "s)KaTK}8`z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you make a <b>map</b> out of a set?",
                "The map behaves like a set instantiated with a pair, whose comparison function refers only to the key. Thus it supports begin, end, size, and empty, but the underlying iterator is a key-value pair.&nbsp;"
            ],
            "guid": "Cdug=hmN,S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How are sets and maps implemented (at least in C++)?",
                "C++ requires that set and map support the basic insert, erase, and find operations in logarithmic worst-case time. Consequently, the underlying implementation is a balanced&nbsp;binary search tree. Typically, an AVL tree is not used; instead, top-down red-black trees&nbsp;are often used.&nbsp;"
            ],
            "guid": "Qg^3Z6RiNT",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the set/map iteration problem?",
                "Maintain the extra links, for the next smaller and larger nodes, only for nodes that have nullptr left or right links by using extra Boolean variables to allow the routines to tell if a left link is being used as a standard binary search tree left link or a link to the next smaller node, and similarly for the right link. This&nbsp;idea is called a <b>threaded tree</b> and is used in many of the STL implementations.&nbsp;"
            ],
            "guid": "OWB$y<&?xv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "A solution for finding all the words with 1 letter difference",
                "We group the words by word length, and then apply the following algorithm on each group separately:<br><img src=\"paste-973c3547647fe0bcde1206b14fcafab34de4e4b0.jpg\"><br>"
            ],
            "guid": "q=*@A]T!BP",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a a search tree to order elements?",
                "By inserting elements into a search tree and then performing an inorder traversal, we obtain the elements in sorted order. This gives an O(N log N) algorithm to sort, which is a worst-case bound if any sophisticated search tree is used&nbsp;"
            ],
            "guid": "k6@/27?{VR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does a 2-3 tree refer to?",
                "A special case of the B-tree is the 2–3 tree (M = 3), which is another way to implement balanced search trees&nbsp;"
            ],
            "guid": "ufhp@tNB[R",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of hashing/hash tables?",
                "Hashing is a technique used for performing insertions, deletions, and finds in constant average time. Tree operations that require any ordering information among the elements are not supported efficiently. Thus, operations such as findMin, findMax, and the printing of the entire table in sorted order in linear time are not supported.&nbsp;"
            ],
            "guid": "hj<~)A.S?%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the role of a <b>hash function</b>?",
                "It maps into some number in the range 0 to TableSize - 1 and placed in the appropriate cell.&nbsp;It ideally should be simple to compute and should ensure that any two distinct keys get different cells. Since there are a finite number of cells and a virtually inexhaustible supply of keys, this is clearly<br>impossible, and thus we seek a hash function that distributes the keys evenly among the cells.&nbsp;"
            ],
            "guid": "C=B/;Ks{;i",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a simple hashing function for integer keys?",
                "Simply returning Key mod TableSize is generally a reasonable strategy, unless Key happens to have some undesirable properties."
            ],
            "guid": "L0rAl!ijxZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of the mod10 hashing function being a bad choice for integer keys.",
                "If the table size is 10 and the keys all end in zero, then the standard hash function is a bad choice.&nbsp;"
            ],
            "guid": "N*T~4e:%oZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the prefered property of the table size in a hashed table?",
                "It is a good idea to choose a table size that is a prime number."
            ],
            "guid": "H6_8x!BSp@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "A simple and well distributed hashing function for English words/strings.",
                "<img src=\"paste-fed19aea15e6b3acbb499478e1d6c7c0016b53d1.jpg\"><br><img src=\"paste-45ccbfdf0ff9a07d832050b595e3cabd4a9e3103.jpg\"><br>The code computes a polynomial function (of 37) by use of Horner’s rule."
            ],
            "guid": "N?fafI@#8C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Some strategies for hashing long strings.",
                "If the keys are very long, the hash function will take too long to compute. A common practice in this case is not to use all the characters. The length and properties of the keys would then influence the choice. For instance, the keys could be a complete street address. The hash function might include a couple of characters from the street address and perhaps a couple of characters from the city name and ZIP code. Some programmers implement their hash function by using only the characters in the odd spaces, with the idea that the time saved computing the hash function will make up for a slightly less evenly distributed function&nbsp;"
            ],
            "guid": "A8EcCrKL>j",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>hashing collision</b>?",
                "If, when an element is inserted, it hashes to the same value as an already inserted element, then we have a collision and need to resolve it. There are several methods for dealing with this,&nbsp;the simplest are: separate chaining and open addressing."
            ],
            "guid": "C?*bb]X>hc",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>separate chaining</b>?",
                "Separate chaining is to keep a list of all elements that hash to the same value. We can use the Standard Library list implementation. If space&nbsp;is tight, it might be preferable to avoid their use (since these lists are doubly linked and waste space).<br>Any scheme could be used besides linked lists to resolve the collisions; a binary search tree or even another hash table would work, but we expect that if the table is large and the hash function is good, all the lists should be short, so basic separate chaining makes no attempt to try anything complicated.\n<br><img src=\"paste-0361f4901f8e7233ddc5f94beefada26779e08e4.jpg\">"
            ],
            "guid": "oICc+8s_C=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the load factor, λ, of a hash table?",
                "It is the load factor, λ, of a hash table is the ratio of the number of elements&nbsp;<span style=\"color: var(--field-fg); background: var(--field-bg);\">in the hash table to the table size.</span>"
            ],
            "guid": "Q2S8l#A5uR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the number of links to be traversed in a map?",
                "1 + (λ/2)&nbsp;"
            ],
            "guid": "NB;ioA)I&p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the general rule for separate chaining hashing's table size?",
                "To make the table size about as large as the number of elements expected (in other words, let λ ≈ 1)&nbsp;"
            ],
            "guid": "zFucryM9df",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do&nbsp;<b>probing hash tables</b> do collision resolution<b>?</b>",
                "We try alternative cells until an empty cell is found. More formally, cells h0(x), h1(x), h2(x), . . . are tried in succession, where hi(x) = (hash(x) + f(i)) mod TableSize, with f(0) = 0. The function, f, is the <b>collision resolution strategy</b>. Because all the data go inside the table, a bigger table is needed&nbsp;in such a scheme than for separate chaining hashing."
            ],
            "guid": "iwQDO[$]tz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the target load factor for probing hash tables?",
                "λ = 0.5 for a hash table that doesn’t use separate chaining."
            ],
            "guid": "eTpFtmY:q]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is linear probing done?",
                "In linear probing, f is a linear function of i, typically f(i) = i. This amounts to trying cells sequentially (with wraparound) in search of an empty cell."
            ],
            "guid": "EmL_*}*HQ%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>primary clustering</b>?",
                "In linear probing, as long as the table is big enough, a free cell can always be found, but the time to do so can get quite large. Worse, even if the table is relatively empty, blocks of occupied cells start forming. This effect, means that any key that hashes into the cluster will require several attempts to resolve the collision, and then it will add to the cluster."
            ],
            "guid": "N2Rvd>>=~S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected number of probes using linear probing?",
                "It is roughly 1/2 (1 + 1/(1 - λ)<sup>2</sup>) for insertions and&nbsp;unsuccessful searches, and 1/2(1 + 1/(1 - λ)) for successful searches."
            ],
            "guid": "r@4$yZ^|]C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the unsuccessful search time in chaining hash tables?",
                "λ"
            ],
            "guid": "LsSE8,+S36",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative probing method eliminates the primary clustering of linear probing?",
                "Quadratic probing."
            ],
            "guid": "I,#:?VsBd7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which quadratic probing strategy function is popular?",
                "f(i) = i<sup>2</sup>"
            ],
            "guid": "Gtm~OEmaTg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What problem does quadratic probing suffer from?",
                "There is no guarantee of finding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime. This is because at most half of the table can be used as alternative locations to resolve collisions."
            ],
            "guid": "vH~&LXGV*;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you guarantee that an element can be always inserted when quadratic probing is used?",
                "By using a prime table size and making sure that the table is never more than half-full."
            ],
            "guid": "i2ybdxpG7&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can you delete an object from a probing hash table?",
                "Standard deletion cannot be performed in a probing hash table, because the cell might have caused a collision to go past it. Thus, probing hash tables require lazy deletion, although in this case there really is no laziness implied.&nbsp;"
            ],
            "guid": "EYbC7DgyRa",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>secondary clustering</b>?",
                "Although quadratic probing eliminates primary clustering, elements that hash to the same position will probe the same alternative cells. This is known as secondary clustering. Secondary clustering is a slight theoretical blemish. Simulation results suggest that it generally causes less than an extra half probe per search.&nbsp;"
            ],
            "guid": "w7%{MHF;GC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>double hashing</b>?",
                "It's a collision resolution method. For double hashing, one popular choice is f(i) = i·hash<sub>2</sub>(x). This formula says that we apply a second hash&nbsp;function to x and probe at a distance hash<sub>2</sub>(x), 2hash<sub>2</sub>(x), . . . , and so on."
            ],
            "guid": "Gp7@8Dl%k6",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the required properties of a function used for double hashing?",
                "It should never evaluate to zero and it should be able to probe all the cells."
            ],
            "guid": "g/FBwSTZcl",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Example of a good function used for double hashing.",
                "hash2(x) = R - (x mod R), with R a prime smaller than TableSize."
            ],
            "guid": "QZZ-+NY<gI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is rehashing done and why?",
                "If the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. This can happen if there are too many removals intermixed with insertions. A solution, then, is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table."
            ],
            "guid": "ChzH#|IyGu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of rehashing?",
                "This entire operation is called rehashing. This is obviously a very expensive operation; the running time is O(N), since there are N elements to rehash and the table size is roughly 2N."
            ],
            "guid": "w6C@m`aK4*",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When should rehashing be done?",
                "One alternative is to rehash as soon as the table is half full. The other extreme is to rehash only when an insertion fails. A third, middle-of-the-road strategy is to rehash when the table reaches a certain load factor. Since performance does degrade as the load factor increases, the third strategy, implemented with a good cutoff, could be best."
            ],
            "guid": "f=E_3gYeLs",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is used to implement unordered sets and maps?",
                "Hash tables."
            ],
            "guid": "gyRfh.1/I(",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use unordered maps to solve word-changing problem?",
                "1. A map in which the key is a word length, and the value is a collection of all words of that word length.<br>2. A map in which the key is a representative, and the value is a collection of all words with that representative.<br>3. A map in which the key is a word, and the value is a collection of all words that differ in only one character from that word.<br>Because the order in which word lengths are processed does not matter, the first map can be an unordered_map. Because the representatives are not even needed after the second map is built, the second map can be an unordered_map. The third map can also be an unordered_map, unless we want printHighChangeables to alphabetically list the subset of words that can be changed into a large number of other words.<br>The performance of an unordered_map can often be superior to a map, but it is hard to know for sure without writing the code both ways."
            ],
            "guid": "I^ffec8fCR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Given N balls placed randomly (uniformly) in N bins, what is the expected number of balls in the most occupied bin?",
                "For separate chaining, assuming a load factor of 1, this is one version of the classic balls and bins problem. The answer is well known to&nbsp;be Θ(log N/ log log N), meaning that on average, we expect some queries to take nearly logarithmic time."
            ],
            "guid": "ji^/^H8z]}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the probability that no bin has more than one ball if N balls are placed into M = N<sup>2</sup> bins?",
                "Less than 1/2"
            ],
            "guid": "ux7Ve;g/}Q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>perfect hashing</b>?",
                "It is a hashing method that uses a secondary hash table for collision resolution. Each secondary hash table will be constructed using a different hash function until it is collision free. The primary hash table can also be constructed several times if the number of collisions that are produced is higher than required."
            ],
            "guid": "+SYrg^2HF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the total size of the secondary hash tables if N items are placed in a primary hash table containing N bins?",
                "The total size of the secondary hash tables has expected value at most 2N.&nbsp;"
            ],
            "guid": "u0S{WGPXQL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition for perfect hashing to work?",
                "Perfect hashing works if the items are all known in advance."
            ],
            "guid": "K]fH_B`JDt",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call a perfect hashing algorithm that allows insertions and deletions?",
                "Dynamic&nbsp;schemes that allow insertions and deletions are called <b>dynamic perfect hashing</b>."
            ],
            "guid": "rv6{gvPo+2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the <b>power of two choices</b>.",
                "In the balls and bins problem, if at each toss&nbsp;two bins were randomly chosen and the item was tossed into the more empty bin (at the time), then the size of the largest bin would only be Θ(log log N), a significantly lower number than the expected Θ(log N/ log log N)."
            ],
            "guid": "h#E&mhKuz~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How <b>cuckoo hashing</b>&nbsp;works?",
                "Suppose we have N items. We maintain two tables, each more than half empty, and we have two independent hash functions that can assign each item to a position in each table. Cuckoo hashing maintains the invariant that an item is always stored in one of these two locations.<br><img src=\"paste-738d4a0f2e0f403309fb844cba67dce125eee98d.jpg\">"
            ],
            "guid": "IYP{H:W4P0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the implication of having an item in one of the two cuckoo tables?",
                "1- Search requires at most two table accesses.<br>2- No lazy deleting is required."
            ],
            "guid": "yw0h?}3-gZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert an item in a cuckoo hashing table?",
                "1- We use a hashing algorithm to determine the position in table 1, and if it's empty, insert the item.<br>2- If the cell is already occupied we preemptively displace the existing item to table 2 which uses a different hashing algorithm and insert the new item to table 1.<br>3- If the displaced item still causes a collision we continue displacement until an empty cell is found.<br>4- After a certain number of collisions the tables can be rehashed."
            ],
            "guid": "vGlrx;<0QC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between the load-factor and the number of displacements in a cuckoo hashing table?",
                "If the table’s load factor is below 0.5, an analysis shows that the probability of a cycle is very low, that the expected number of displacements is a small constant, and that it is extremely unlikely that a successful insertion would require more than O(log N) displacements.<br>However, if the table’s load factor is at 0.5 or higher, then the probability of a cycle becomes drastically higher.&nbsp;"
            ],
            "guid": "s%X5e!|~F,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the probability that a single insertion would require a new set of hash functions?",
                "O(1/N<sup>2</sup>)&nbsp;"
            ],
            "guid": "I$EE}opb)q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What extensions are proposed for cuckoo tables?",
                "1- Instead of two tables, we can use a higher number of tables, such as 3 or 4. While this increases the cost of a lookup, it also drastically increases the theoretical space utilization.<br>2- In some applications the lookups through separate hash functions can be done in parallel and thus cost little to no additional time.<br>3- Another extension is to allow each table to store multiple keys; again, this can increase space utilization and make it easier to do insertions and can be more cache-friendly.<br><img src=\"paste-fd02b48b8683d7e584dfc6f659f9ec46c48ffaf7.jpg\"><br>4- Often cuckoo hash tables are implemented as one giant table with two (or more) hash functions that probe the entire table, and some variations attempt to place an item in the second hash table immediately if there is an available spot, rather&nbsp;than starting a sequence of displacements.&nbsp;"
            ],
            "guid": "p`}zA}mWv!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of cuckoo hashing?",
                "The benefits of cuckoo hashing include the worst-case constant lookup and deletion times, the avoidance of lazy deletion and extra data, and the potential for parallelism.<br>However, cuckoo hashing is extremely sensitive to the choice of hash functions; the inventors of the cuckoo hash table reported that many of the standard hash functions that they attempted performed poorly in tests.&nbsp;Furthermore, although the insertion time is expected to be constant time as long as the load factor is below 1/2, deteriorates rapidly as the load factor gets close to 1/2. Using lower load factors or more than two hash functions seems like a reasonable alternative."
            ],
            "guid": "ffq;Cy@6tR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What's the main idea behind <b>Hopscotch hashing</b>?",
                "The idea of hopscotch hashing is to bound the maximal length of the probe sequence by a predetermined constant that is optimized to the underlying computer’s architecture. Doing so would give constant-time lookups in the worst case, and like cuckoo hashing, the lookup could be parallelized to simultaneously check the bounded set of possible locations.<br>Let MAX_DIST be the chosen bound on the maximum probe sequence. This means that item x must be found somewhere in the MAX_DIST positions listed in hash(x), hash(x) + 1, . . . , hash(x) + (MAX_DIST - 1). In order to efficiently process evictions, we maintain information that tells for each position x, whether the item in the alternate position is occupied by an element that hashes to position x.<br><img src=\"paste-91a2711d28eed5b2e0808c5a38ad33fa21df5ff8.jpg\">"
            ],
            "guid": "yI6{`NB<<0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of 0.5 and 0.9 load factors on Hopscotch hashing?",
                "The algorithm is deterministic in that given a hash function, either the items can be evicted or they can't. The latter case implies that the table is likely&nbsp;too crowded, and a rehash is in order; but this would happen only at extremely high load&nbsp;factors, exceeding 0.9. For a table with a load factor of 1/2, the failure probability is almost zero."
            ],
            "guid": "A;ZM7.n0RS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "For which applications Hopscotch hashing is most useful?",
                "The algorithm is especially promising for applications that make use of multiple processors and require significant parallelism and concurrency&nbsp;"
            ],
            "guid": "Bt/3l92W+_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the factors that guarantee a constant average cost per operation for hash tables?",
                "1. The hash function must be computable in constant time (i.e., independent of the number of items in the hash table).<br>2. The hash function must distribute its items uniformly among the array slots."
            ],
            "guid": "QhA&w*},Nm",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of <b>universal hash functions</b>?",
                "A family H of hash functions is universal, if for any x != y, the number of hash functions h in H for which h(x) = h(y) is at most |H|/M, where M represents TableSize.<br>Notice that this definition holds for each pair of items, rather than being averaged over all pairs of items. The definition above means that if we choose a hash function randomly from a universal family H, then the probability of a collision between any two distinct items is at most 1/M, and when adding into a table with N items, the probability of a collision at the initial point is at most N/M, or the load factor."
            ],
            "guid": "i`aB$[9zf2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which types of applications require universal hash functions?",
                "Although a strong motivation for the use of universal hash functions is to provide theoretical justification for the assumptions used in the classic hash table analyses, these functions can also be used in applications that require a high level of robustness, in which worst-case (or even substantially degraded) performance, perhaps based on inputs generated by a saboteur or hacker, simply cannot be tolerated."
            ],
            "guid": "kHB5]76CWF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of a k-universal family of hash functions?",
                "<img src=\"paste-ea8f40d52f5be8c12c22f202563ce9d1f9845b21.jpg\">"
            ],
            "guid": "dU1t%W=`_g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of a universal hash family.",
                "<img src=\"paste-2f4f0fa840ba16db307f5e7698a7b0fab753580d.jpg\">"
            ],
            "guid": "Cna!nkzGx=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the <b>Carter-Wegman trick</b>.",
                "<img src=\"paste-52605f4aa09c0442e5a62a1732730ad3cda84c58.jpg\"><br><img src=\"paste-44e816d412d434c74406f217b38f8056e78f4f1f.jpg\">"
            ],
            "guid": "H~Nolm)hT$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many disk accesses are required in <b>extendible hashing</b>?",
                "Extendible hashing allows a search to be performed in two disk accesses. Insertions also require few disk accesses."
            ],
            "guid": "uJH8a1O=$b",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which hashing algorithm reduces the number of disk accesses?",
                "A clever alternative, known as <b>extendible hashing</b>, allows a search to be performed in two disk accesses. Insertions also require few disk accesses."
            ],
            "guid": "tIv_3&]/P?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does extendible hashing relate to B-trees?",
                "A B-tree has depth O(log<sub>M/2</sub> N). As M increases, the depth of a B-tree decreases. We could in theory choose M to be so large that the depth of the B-tree would be 1. Then any search after the first would take one disk access, since, presumably, the root node could be stored in main memory. The problem with this strategy is that the branching factor is so high that it would take considerable processing to determine which leaf the data was in. If the time to perform this step could be reduced, then we would have a practical scheme. This is exactly the strategy used by extendible hashing.<br><img src=\"paste-838febabac9024fa3e6436eb3936e233a7540fa4.jpg\">"
            ],
            "guid": "d|sHz55oS+",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of the root and leaves in an extendible hashing tree?",
                "D will represent the number of bits used by the root, which is sometimes known as the directory. The number of entries in the directory is thus 2<sup>D</sup>.&nbsp;\nd<sub>L</sub>&nbsp;is the number of leading bits that all the elements of some leaf L have in common. d<sub>L</sub> will depend on the particular leaf, and d<sub>L</sub> ≤ D."
            ],
            "guid": "JBSJXE-8YY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a split done in extendible hashing?",
                "All the leaves not involved in the split are now pointed to by two adjacent directory entries. Thus, although an entire directory is rewritten, none of the other leaves is actually accessed."
            ],
            "guid": "y.ayY3tJC4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What edge cases we can have splitting an extendible hashing table's directories?",
                "First, it is possible that several directory splits will be required if the elements in a leaf agree in more than D + 1 leading bits. This is an easy detail to take care of, but must not be forgotten. Second, there is the possibility of duplicate keys; if there are more than M duplicates, then this algorithm does not work at all. In this case, some other arrangements need to be made.<br>These possibilities suggest that it is important for the bits to be fairly random. This can be accomplished by hashing the keys into a reasonably long integer—hence the name."
            ],
            "guid": "q*!d0r@ymF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected number of leaves in extendible hashing?",
                "The expected number of leaves is (N/M) log<sub>2</sub>e. Thus the average leaf is ln 2 = 0.69 full. This is the same as for B-trees, which is not entirely surprising, since for both data structures new nodes are created when the (M + 1)th entry is added."
            ],
            "guid": "tcCSq*#,w)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected size of the directory in extendible hashing?",
                "the expected size of the directory (in other words, 2D) is O(N<sup>1+1/M</sup>/M). If M is very small, then the directory can get unduly large. In this case, we can have the leaves contain pointers to the records instead of the actual records, thus increasing the value of M. This adds a second disk access to each search operation in order to maintain a smaller directory. If the directory is too large to fit in main memory, the second disk access would be needed anyway."
            ],
            "guid": "BMveuq:x~E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structures do compilers use to keep track of declared variables?",
                "Compilers use hash tables to keep track of declared variables in source code. The data structure is known as a <b>symbol table</b>. Hash tables are the ideal application for this problem. Identifiers are typically short, so the hash function can be computed quickly, and alphabetizing the variables is often unnecessary."
            ],
            "guid": "opZ+?[sY:]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does the performance of a binary search tree and a hash table compare?",
                "Although the resulting average time bounds of insert and contains operations&nbsp;are O(log N), binary search trees also support routines that require order and are thus more powerful. Using a hash table, it is not possible to find the minimum element. It is not possible to search efficiently for a string unless the exact string is known. A binary search tree could quickly find all items in a certain range; this is not supported by hash tables. Furthermore, the O(log N) bound is not necessarily that much more than O(1), especially since no multiplications or divisions are required by search trees.<br>On the other hand, the worst case for hashing generally results from an implementation error, whereas sorted input can make binary trees perform poorly. Balanced search trees are quite expensive to implement, so if no ordering information is required and there is any suspicion that the input might be sorted, then hashing is the data structure of choice."
            ],
            "guid": "o%[XJ)l|D|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe using hash tables in graph theory problems.",
                "A hash table is useful for any graph theory problem where the nodes have real names instead of numbers. Here, as the input is read, vertices are assigned integers from 1 onward by order of appearance. Again, the input is likely to have large groups of alphabetized entries. For example, the vertices could be computers. Then if one particular installation lists its computers as ibm1, ibm2, ibm3, . . . , there could be a dramatic effect on efficiency if a search tree is used."
            ],
            "guid": "k35b3y6z3}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a <b>transposition table </b>used in games?",
                "A common use of hash tables is in programs that play games. As the program searches through different lines of play, it keeps track of positions it has seen by computing a hash function based on the position (and storing its move for that position). If the same position recurs, usually by a simple transposition of moves, the program can avoid expensive recomputation."
            ],
            "guid": "HQX%zJ?ByY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structure we can use for an online spelling checker?",
                "If misspelling detection (as opposed to correction) is important, an entire dictionary can be prehashed and words can be checked in constant time. Hash tables are well suited for this, because it is not important to alphabetize words; printing out misspellings in the order they occurred in the document is certainly acceptable."
            ],
            "guid": "zTo6gvE6yv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is frequently used for caching?",
                "Hash tables are often used to implement caches, both in software (for instance, the cache in your Internet browser) and in hardware (for instance, the memory caches in modern computers). They are also used in hardware implementations of routers."
            ],
            "guid": "Nd,6qFBlze",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the minimum operations required by a <b>priority queue</b>?",
                "Insert and deleteMin, which finds, returns, and removes the minimum element in the priority queue."
            ],
            "guid": "mO[{h@:75~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structure is commonly used in greedy algorithms?",
                "Priority queue (heap)."
            ],
            "guid": "F(z3J^Y+S_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the structure of <b>heap</b>.",
                "A heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree.<br><img src=\"paste-54b9f2aa2a98dcd6f5a6c931853fd147a9e34b8b.jpg\">"
            ],
            "guid": "A}^P_|g!!G",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between a <b>complete binary tree</b>'s height and number of nodes?",
                "A complete binary tree of height h has between 2<sup>h</sup> and 2<sup>h+1</sup> - 1 nodes. This implies that the height of a complete binary tree is log N , which is clearly O(log N)."
            ],
            "guid": "J?z5ZMY&~A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What simple data-structure can be used to represent a complete binary tree? Why?",
                "Because a complete binary tree is so regular, it can be represented in an array and no links are necessary.<br><img src=\"paste-58c7f2867f1e1ad7f827df5bc8422c9ace7ab89b.jpg\">"
            ],
            "guid": "O[-7ZyO&|g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between the array index and heap traversal when implemented using an array?",
                "For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child (2i + 1), and the parent is in position i/2 . Thus, not only are links not required, but the operations required to traverse the tree are extremely simple and likely to be very fast on most computers.&nbsp;"
            ],
            "guid": "jilXnd4/0D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which property allows operations to be performed quickly in a <b>heap</b>?",
                "Since we want to be able to find the minimum quickly, it makes sense that the smallest element should be at the root. If we consider that any subtree should also be a heap, then any node should be smaller than all of its descendants.&nbsp;This is called <b>heap-order property</b>."
            ],
            "guid": "Ai-JUa0hHo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert a new item in a <b>heap</b>?",
                "To insert an element X into the heap, we create a hole in the next available location, since otherwise, the tree will not be complete. If X can be placed in the hole without violating heap order, then we do so and are done. Otherwise, we slide the element that is in the hole’s parent node into the hole, thus bubbling the hole up toward the root. We continue this process until X can be placed in the hole.&nbsp;This general strategy is known as a <b>percolate up</b>.<br><img src=\"paste-eeca0f36d8f4f60ea0a701e1ae371587d4a47451.jpg\">"
            ],
            "guid": "hq-14$laE$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average time for inserting into a <b>heap</b>?",
                "The time to do the insertion could be as much as O(log N), if the element to be inserted is the new minimum and is percolated all the way to the root. On average, the percolation terminates early; it has been shown that 2.607 comparisons are required on average to perform an insert, so the average insert moves an element up 1.607 levels."
            ],
            "guid": "rNxkov!Vh]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is deleteMin done in heaps?",
                "When the minimum is removed, a hole is created at the root. Since the heap now becomes one smaller, it follows that the last element X in the heap<br>must move somewhere in the heap. If X can be placed in the hole, then we are done. This is unlikely, so we slide the smaller of the hole’s children into the hole, thus pushing the hole down one level. We repeat this step until X can be placed in the hole. Thus, our action is to place X in its correct spot along a path from the root containing minimum children.<br><img src=\"paste-1d8b032be31297c586ad5c1164bd0dfddbade2ec.jpg\">"
            ],
            "guid": "fV!h!dgw4B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a common mistake when percolating down in <b>heap</b>s? And what are the possible solutions?",
                "A frequent implementation error in heaps occurs when there are an even number of elements in the heap, and the one node that has only one child is encountered. You must&nbsp;make sure not to assume that there are always two children, so this usually involves an extra test. In the code depicted in&nbsp; One extremely tricky solution is always to ensure that your algorithm thinks every node has two children. Do this by placing a sentinel, of value higher than any in the heap, at the spot after the heap ends, at the start of each percolate down when the heap size is even. You should think very carefully before attempting this, and you must put in a prominent comment if you do use this technique. Although this eliminates the need to test for the presence of a right child, you cannot eliminate the requirement that you test when you reach the bottom, because this would require a sentinel for every leaf."
            ],
            "guid": "x`GD#@*PmI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times for <b>deleteMin</b> in <b>heap</b>s?",
                "The worst-case running time for this operation is O(log N). On average, the element that is placed at the root is percolated almost to the bottom of the heap (which is the level it came from), so the average running time is O(log N)."
            ],
            "guid": "ia:#5EU)tL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find an item other than the minimum in a <b>heap</b>?",
                "A heap has very little ordering information, so there is no way to find any particular element without a linear scan through the entire heap.&nbsp;If it is important to know where elements are, some other data structure, such as a hash table, must be used in addition to the heap."
            ],
            "guid": "Iwr;Vp|OFh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>decreaseKey</b> in a <b>heap</b>?",
                "The decreaseKey(p, delta) operation lowers the value of the item at position p by a positive amount delta. Since this might violate the heap order, it must be fixed by a percolate up. This operation could be useful to system administrators: They can make their programs run with highest priority,"
            ],
            "guid": "dd^m*5d[dE",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>increaseKey</b>&nbsp;<b>in a heap</b>?",
                "The increaseKey(p, delta) operation increases the value of the item at position p by a positive amount delta. This is done with a percolate down. Many schedulers automatically drop the priority of a process that is consuming excessive CPU time."
            ],
            "guid": "h9u3JhuM4W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you remove an arbitrary item from a <b>heap</b>?",
                "The remove(p) operation removes the node at position p from the heap. This is done by first performing decreaseKey(p,∞) and then performing deleteMin(). When a process&nbsp;is terminated by a user (instead of finishing normally), it must be removed from the priority queue."
            ],
            "guid": "ob8MhCor:D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>buildHeap</b>&nbsp;operation?",
                "This constructor takes as input N items and places them into a heap. Obviously, this can be done with N successive inserts. Since each insert will take O(1) average and O(log N) worst-case time, the total running time of this algorithm would be O(N) average but O(N log N) worst-case."
            ],
            "guid": "udJgv#t_:A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the sum of heights in a <b>perfect binary tree</b>?",
                "For the perfect binary tree of height h containing 2<sup>h+1</sup>-1 nodes, the sum of the heights of the nodes is 2<sup>h+1</sup> - 1 - (h + 1)."
            ],
            "guid": "e<Kz%=c&Ya",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the time required to find the kth largest/smallest number when arrays + sorting are used?",
                "1- Sorting the whole array: O(N<sup>2</sup>)<br>2- Sorting an array of k elements and then inserting into the array the other items if they are within the range, or simply ignoring them: The running time is O(N·k). If k = [N/2], then both algorithms are O(N<sup>2</sup>). Notice that for any k, we can solve the symmetric problem of finding the (N - k + 1)th smallest element, so k = [N/2] is really the hardest case for these algorithms. This also happens to be the most interesting case, since this value of k is known as the median&nbsp;"
            ],
            "guid": "EX:n:fo&S)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a <b>heap</b>&nbsp;to get the kth smallest element?",
                "1- We read the N elements into an array. We then apply the buildHeap algorithm to this array. Finally, we perform k deleteMin operations. The last element extracted from the heap is our answer.&nbsp;If k = [N/2], then the running time is&nbsp;Θ(N log N).<br>2- At any point in time we will maintain a set S of the k largest elements. After the first k elements are read, when a new element is read it is compared with the kth largest element, which we denote by S<sub>k</sub>. Notice that S<sub>k</sub> is the smallest element in S. If the new element is larger, then it replaces S<sub>k</sub> in S. S will then have a new smallest element, which may or may not be the newly added element. At the end of the input, we find the smallest element in S and return it as the answer.&nbsp;The total time is O(N log k). This algorithm also gives a bound of Θ(N log N) for finding the median."
            ],
            "guid": "G.$AN&Wj}|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we simulate a teller queue in a bank?",
                "A simulation consists of processing events. The two events here are (a) a customer arriving and (b) a customer departing, thus freeing up a teller.&nbsp;The key is to advance the clock to the next event time at each stage. At any point, the next event that can occur is either (a) the next customer in the input file arrives or (b) one of the customers at a teller leaves. Since all the times when the events will happen are available, we just need to find the event that happens nearest in the future and process that event. If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to see whether there is another customer waiting. If so, we add that customer, process whatever statistics are required, compute the time when that customer will leave, and add that departure to the set of events waiting to happen. If the event is an arrival, we check for an available teller. If there is none, we place the arrival on the line (queue); otherwise we give the customer a teller, compute the customer’s departure time, and add the departure to the set of events waiting to happen.<br>The waiting line for customers can be implemented as a queue. Since we need to find the event nearest in the future, it is appropriate that the set of departures waiting to happen be organized in a priority queue. The next event is thus the next arrival or next departure (whichever is sooner); both are easily available."
            ],
            "guid": "xuo,Y|OyN=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the bank teller queueing problem when a heap is used?",
                "If there are&nbsp;C&nbsp;customers (and thus 2C&nbsp;events) and&nbsp;k&nbsp;tellers, then the running time&nbsp;of the simulation&nbsp;would be&nbsp;O(C&nbsp;log(k&nbsp;+&nbsp;1)) because computing and processing each event&nbsp;takes&nbsp;O(log&nbsp;H), where&nbsp;H&nbsp;=&nbsp;k&nbsp;+&nbsp;1 is the size of the heap."
            ],
            "guid": "bM[619)HdK",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>d-heap</b>?",
                "It's a simple generalization&nbsp;which is exactly like a binary heap except that all nodes have d children (thus, a binary heap is a 2-heap)."
            ],
            "guid": "dqd;|nd,xo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>d-heap</b>?",
                "A d-heap is much shallower than a binary heap, improving the running time of inserts to O(log<sub>d</sub> N). However, for large d, the deleteMin operation is more expensive, because even though the tree is shallower, the minimum of d children must be found, which takes d - 1 comparisons using a&nbsp; standard algorithm. This raises the time for this operation to O(d log<sub>d</sub> N). If d is a constant, both running times are, of course, O(log N). Although an array can still be used, the multiplications and divisions to find children and parents are now by d, which, unless d is a power of 2, seriously increases the running time, because we can no longer implement division by a bit shift.<br><img src=\"paste-42ca6ae3f94b1aab2f918c407b03d1eb87a22d67.jpg\">"
            ],
            "guid": "NZva&){^;H",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which applications is a d-heap useful?",
                "d-heaps are interesting in theory, because there are many algorithms where the number of insertions is much greater than the number of deleteMins (and thus a theoretical speedup is possible). They are also of interest when the priority queue is too large to fit entirely in main memory. In this case, a d-heap can be advantageous in much the same way as B-trees. Finally, there is evidence suggesting that 4-heaps may outperform binary heaps in practice."
            ],
            "guid": "mE:?g}lO)$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of heaps?",
                "The most glaring weakness of the heap implementation, aside from the inability to perform finds, is that combining two heaps into one is a hard operation. This extra operation is known as a merge."
            ],
            "guid": "fqSo,2c9I=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What change in the heap's data-structure we can make for merging to be faster? And what is the impact of this change on other heap operations?",
                "It seems difficult to design a data structure that efficiently supports merging (that is, processes a merge in o(N) time) and uses only an array, as in a binary heap. The reason for this is that merging would seem to require copying one array into another, which would take Θ(N) time for equal-sized heaps. For this reason, all the advanced data structures that support efficient merging require the use of a linked data structure. In practice, we can expect that this will make all the other operations slower."
            ],
            "guid": "PObZz|oRt>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the similarities and differences between a binary heap and a <b>leftist heap</b>?",
                "A leftist heap has both a structural property and an ordering property. Indeed, a leftist heap, like virtually all heaps used, has the same heap-order property we have already seen. Furthermore, a leftist heap is also a binary tree. The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced, but actually attempt to be very unbalanced."
            ],
            "guid": "ocTeOr;}OJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>null path length</b>.",
                "The null path length, npl(X), of any node X to be the length of the shortest path from X to a node without two children. Thus, the npl of a node with zero or one child is 0, while npl(nullptr) = -1."
            ],
            "guid": "IE[.51s5El",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the <b>leftist heap property</b>.",
                "The leftist heap property is that for every node X in the heap, the <b>null path length</b> of the left child is at least as large as that of the right child. This property is satisfied by only the tree on the left. This property actually goes out of its way to ensure that the tree is unbalanced, because it clearly biases the tree to get deep toward the left.<br><img src=\"paste-3d31de6c9e08eba3e99436ef3a4670c1695aa9d2.jpg\">"
            ],
            "guid": "r6jNH?<WH1",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum number of nodes on the right path in a <b>leftist tree</b>?",
                "[log(N + 1)] nodes."
            ],
            "guid": "jCHP:5;4{z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge two <b>leftist heap</b>s?",
                "If either of the two heaps is empty, then we can return the other heap. Otherwise, to merge the two heaps, we compare their roots.<br><img src=\"paste-657956ac05a39f3da757204cdae689ab6aa220be.jpg\"><br>First, we recursively merge the heap with the larger root with the right subheap of the heap with the smaller root.<br><img src=\"paste-782a3499ad72473942795f70ea31f726f7a7f6f0.jpg\"><br>Although the resulting heap satisfies the heap-order property, it is not leftist because the left subtree of the root has a null path length of 1 whereas the right subtree has a null path length of 2. Thus, the leftist property is violated at the root.<br><img src=\"paste-30963476b82950f7b2013dd41ecb653605a76af4.jpg\"><br>We can make the entire tree leftist by merely swapping the root’s left and right children and updating the null path length.<br><img src=\"paste-737ea761d23452b9f711d8f6cfb4b5590f0c863f.jpg\">"
            ],
            "guid": "QjF@LWazp2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the merging time of two <b>leftist heaps</b>?",
                "The time to perform the merge is proportional to the sum of the length of the right paths, because constant work is performed at each node visited during the recursive calls. Thus we obtain an O(log N) time bound to merge two leftist heaps."
            ],
            "guid": "Em+P8uSAzA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is deleteMin implemented in <b>leftist heap</b>s?",
                "We merely destroy the root, creating two heaps, which can then be merged. Thus, the time to perform a deleteMin is O(log N)."
            ],
            "guid": "i4CG+s7{^Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>skew heap</b>?",
                "It is a self-adjusting version of a leftist heap that is simple to implement. The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees and AVL trees. Skew heaps are binary trees with heap order, but there is no structural constraint on these trees. Unlike leftist heaps, no information is maintained about the null path length of any node. The right path of a skew heap can be arbitrarily long at any time."
            ],
            "guid": "LX/9yRYMy:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of operations in a <b>skew heap</b>?",
                "The right path of a skew heap can be arbitrarily long at any time, so the worst-case running time of all operations is O(N). However, as with splay trees, it can be shown that for any M consecutive operations, the total worst-case running time is O(M log N). Thus, skew heaps have O(log N) amortized cost per operation."
            ],
            "guid": "g-c&,xR$w&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge <b>skew heaps</b>?",
                "<img src=\"paste-12f3a7e6cc65fa617ab9239a27b31c0a0bb167e8.jpg\"><br>We can perform all operations nonrecursively, as with leftist heaps, by merging the right paths and swapping left and right children for every node on the right path, with&nbsp;the exception of the last.<br><img src=\"paste-2b624bc74bcabf69354245109a1ee8118c3d4cdb.jpg\"><br>After a few examples, it becomes clear that since all but the last node on the right path have their children swapped, the net effect is that this becomes the new left path.<br><img src=\"paste-482a887dd52a089d02bbac5cee63a8ef89aaa8b5.jpg\">"
            ],
            "guid": "iz>&[WUiA[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>binomial queues</b>?",
                "They support insert, deleteMin, and merge in O(log N) worst-case, while insertions take constant time on average.&nbsp;"
            ],
            "guid": "PMS%gfAMaY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a <b>binomial queue</b>&nbsp;differ from all other priority queue implementations?",
                "Binomial queues differ from all the priority queue implementations that we have seen in that a binomial queue is not a heap-ordered tree but rather a collection of heap-ordered trees, known as a <b>forest</b>. Each of the heap-ordered trees is of a constrained form known as a <b>binomial tree</b>."
            ],
            "guid": "hI*&@vk1EG",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the structure of a <b>binomial tree.</b>",
                "There is at most one binomial tree of every height. A binomial tree of height 0 is a one-node tree; a binomial tree, B<sub>k</sub>, of height k is formed by attaching a binomial tree, B<sub>k-1</sub>, to the root of another binomial tree, B<sub>k-1</sub>. From the diagram we see that a binomial tree, B<sub>k</sub>, consists of a root with children B<sub>0</sub>, B<sub>1</sub>, . . . , B<sub>k-1</sub>. Binomial trees of height k have exactly 2k nodes, and the number of nodes at depth d is the binomial coefficient&nbsp;<img src=\"paste-0689df9309716c497c8d2c47888d56626a9f111e.jpg\"><br><img src=\"paste-1773f101a1f0ea9c1655db9709db9f5b68d0fc4e.jpg\">"
            ],
            "guid": "l0<}AFOH#{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use <b>binomial trees</b>&nbsp;to represent a priority queue?",
                "If we impose heap order on the binomial trees and allow at most one binomial tree of any height, we can represent a priority queue of any size by a collection of binomial trees. For instance, a priority queue of size 13 could be represented by the forest B<sub>3</sub>, B<sub>2</sub>, B<sub>0</sub>. We might write this representation as 1101, which not only represents 13 in binary but also represents the fact that B<sub>3</sub>, B<sub>2</sub>, and B<sub>0</sub> are present in the representation and B<sub>1</sub> is not.<br>As an example, a priority queue of six elements could be represented as in this figure:<br><img src=\"paste-f4629e6ecef9fd0f4a560eda513f742a7e898788.jpg\">"
            ],
            "guid": "lMRRx5SS?L",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find the minimum element in a <b>binomial queue</b>?",
                "By scanning the roots of all the trees. Since there are at most log N different trees, the minimum can be found in O(log N) time. Alternatively, we can maintain knowledge of the minimum and perform the operation in O(1) time if we remember to update the minimum when it changes during other operations."
            ],
            "guid": "ferGjEqFmB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge two <b>binomial queues</b>?",
                "Consider the two binomial queues, H<sub>1</sub> and H<sub>2</sub>, with six and seven elements, respectively:<br><img src=\"paste-0c7f90d442aa367deca741aeca767f438140f62b.jpg\"><br>The merge is performed by essentially adding the two queues together. Let H<sub>3</sub> be the new binomial queue. Since H<sub>1</sub> has no binomial tree of height 0 and H<sub>2</sub> does, we can just use the binomial tree of height 0 in H<sub>2</sub> as part of H<sub>3</sub>. Next, we add binomial trees of height 1. Since both H<sub>1</sub> and H<sub>2</sub> have binomial trees of height 1, we merge them by making the larger root a subtree of the smaller, creating a binomial tree of height 2:<br><img src=\"paste-138eeba44a90e7cb5959060753c99daa3f8df68a.jpg\"><br>Thus, H3 will not have a binomial tree of height 1. There are now three&nbsp;binomial trees of height 2, namely, the original trees of H<sub>1</sub> and H<sub>2</sub> plus the tree formed by the previous step. We keep one binomial tree of height 2 in H<sub>3</sub> and merge the other two, creating a binomial tree of height 3. Since H<sub>1</sub> and H<sub>2</sub> have no trees of height 3, this tree becomes part of H<sub>3</sub> and we are finished.<br><img src=\"paste-a5d102b03a54da8e83d66ea65e5d696f73c78bb9.jpg\">"
            ],
            "guid": "NqbL8>=+u4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for merging two <b>binomial queues</b>?",
                "Since merging two binomial trees takes constant time with almost any reasonable implementation, and there are O(log N) binomial trees, the merge takes O(log N) time in the worst case. To make this operation efficient, we need to keep the trees in the binomial queue sorted by height, which is certainly a simple thing to do."
            ],
            "guid": "A%U8DM>op]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert into a <b>binomial queue</b>?",
                "Insertion is just a special case of merging, since we merely create a one-node tree and perform a merge. The worst-case time of this operation is likewise O(log N). More precisely, if the priority queue into which the element is being inserted has the property that the smallest nonexistent binomial tree is Bi, the running time is proportional to i + 1. An analysis will show that performing N inserts on an initially empty binomial queue will take O(N) worst-case time. Indeed, it is possible to do this operation using only N - 1 comparisons.<br><img src=\"paste-6bd4b2dd58a6ddbb0e3340d175173adaf0a4ebb5.jpg\">"
            ],
            "guid": "iSt57A2SfL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform <b>deleteMin</b>&nbsp;in a <b>binomial queue</b>?",
                "A deleteMin can be performed by first finding the binomial tree with the smallest root. Let this tree be B<sub>k</sub>, and let the original priority queue be H. We remove the binomial tree B<sub>k</sub> from the forest of trees in H, forming the new binomial queue H'. We also remove the root of B<sub>k</sub>, creating binomial trees B<sub>0</sub>, B<sub>1</sub>, . . . , B<sub>k-1</sub>, which collectively form priority queue H''. We finish the operation by merging H' and H''.<br><img src=\"paste-5e624e0405e2d43b055ff6b7555d216fd1473d81.jpg\"><br><img src=\"paste-cef2cc7a7f38f1000eb7ad0e9dc539c79d4d4f5b.jpg\"><br><img src=\"paste-3d570dea05c02022deb071443e38b9efd805201a.jpg\"><br>&nbsp;<img src=\"paste-dd5b8762ffb3881b64c53fb766ecc4464a913f70.jpg\"><br>For the analysis, note first that the deleteMin operation breaks the original binomial queue into two. It takes O(log N) time to find the tree containing the minimum element and to create the queues H' and H''. Merging these two queues takes O(log N) time, so the entire deleteMin operation takes O(log N) time."
            ],
            "guid": "s=eDI.&_*9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how a <b>binomial queue </b>is implemented.",
                "The deleteMin operation requires the ability to find all the subtrees of the root quickly, so the standard representation of general trees is required: The children of each node are kept in a linked list, and each node has a pointer to its first child (if any). This operation also requires that the children be ordered by the size of their subtrees. We also need to make sure that it is easy to merge two trees. When two trees are merged, one of the trees is added as a child to the other. Since this new tree will be the largest subtree, it makes sense to maintain the subtrees in decreasing sizes. Only then will we be able to merge two binomial trees, and thus two binomial queues, efficiently. The binomial queue will be an array of binomial trees. To summarize, then, each node in a binomial tree will contain the data, first child, and right sibling. The children in a binomial tree are arranged in decreasing rank.\n<br><img src=\"paste-6869b08b3c8739dd047ae93cdb095a962e991382.jpg\"><br><img src=\"paste-217ebe08300a935500b838ef8a1437350902853b.jpg\">"
            ],
            "guid": "n+`f3}@foO",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>comparison-based sorting</b>.",
                "It assumes the existence of the “&lt;” and “&gt;” operators, which can be used to place a consistent ordering on the input. Besides the assignment operator, these are the only operations allowed on the input data."
            ],
            "guid": "mm3D{0`6wo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>insertion sort</b>&nbsp;done?",
                "Insertion sort consists of N-1 passes. For pass p=1 through N-1, insertion sort ensures that the elements in positions 0 through p are in sorted order. Insertion sort makes use of the fact that elements in positions 0 through p-1 are already known to be in sorted order.<br><img src=\"paste-c24a64ed279d59ca0371c0b707e98ea0fe010904.jpg\">"
            ],
            "guid": "u,6**fhPC8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for <b>insertion sort</b>?",
                "Because of the nested loops, each of which can take N iterations, insertion sort is O(N<sup>2</sup>).&nbsp;On the other hand, if the input is presorted, the running time is O(N), because the test in the inner for loop always fails immediately. Indeed, if the input is almost sorted insertion sort will run quickly.&nbsp;the average case is Θ(N<sup>2</sup>)."
            ],
            "guid": "fV`zJ-iapj",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>inversion</b> in arrays and sorting algorithms?",
                "An inversion in an array of numbers is any ordered pair (i, j) having the property that i &lt; j but a[i] &gt; a[j]. For example: in the input list 34, 8, 64, 51, 32, 21 had nine inversions, namely (34, 8), (34, 32), (34, 21), (64, 51), (64, 32), (64, 21), (51, 32), (51, 21), and (32, 21).&nbsp;\n"
            ],
            "guid": "H_Kh4ebO{=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average number of inversions in an array of N distinct elements for sorting algorithms that perform only adjacent exchanges?",
                "N(N - 1)/4&nbsp;"
            ],
            "guid": "s)Mb`dh2Vb",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the required time for any algorithm that sorts by exchanging adjacent elements&nbsp;on average?",
                "Ω(N<sup>2</sup>)&nbsp;"
            ],
            "guid": "t4:9/A6Z}f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can a sorting algorithm run in a subquadratic time?",
                "For a sorting algorithm to run in subquadratic, or o(N<sup>2</sup>), time, it must do comparisons and, in particular, exchanges between elements that are far apart. A sorting algorithm makes progress by eliminating inversions, and to run efficiently, it must eliminate more than just one inversion per exchange."
            ],
            "guid": "eay@*nBUaU",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the other name of <b>shellsort</b>?",
                "Shellsort works by comparing elements that are distant; the distance between comparisons decreases as the algorithm runs until the last phase, in which adjacent elements are compared. For this reason, Shellsort is sometimes referred to as <b>diminishing increment sort</b>."
            ],
            "guid": "cDPJy:aILg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>shellsorting </b>done?",
                "Shellsort uses a sequence, h<sub>1</sub>, h<sub>2</sub>, . . . , h<sub>t</sub>, called the <b>increment sequence</b>. Any increment sequence will do as long as h<sub>1</sub> = 1, but some choices are better than others. After a <b>phase</b>, using some increment h<sub>k</sub>, for every i, we have a[i] ≤ a[i + h<sub>k</sub>] (where this makes sense); all elements spaced h<sub>k</sub> apart are sorted. The file is then said to be h<sub>k</sub>-sorted. An important property of Shellsort is that an h<sub>k</sub>-sorted file that is then h<sub>k</sub>-1-sorted remains h<sub>k</sub>-sorted. If this were not the case, the algorithm would likely be of little value, since work done by early phases would be undone by later phases.<br><img src=\"paste-8ffb06ee34372d426d7cf273de7fdd1e58f5b9d5.jpg\">"
            ],
            "guid": "A-1b`dQ{@R",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a common but poor example of an <b>increment sequence </b>in<b> shellsort</b>?",
                "h<sub>t</sub> = [N/2], and h<sub>k </sub>= [h<sub>k+1</sub>/2].&nbsp;"
            ],
            "guid": "j!kNlUxS{m",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst-case running time of <b>Shellsort</b> using Shell’s increments?",
                "Θ(N<sup>2</sup>)"
            ],
            "guid": "In#~!`_q,U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst-case running time of Shellsort using Hibbard’s increments?",
                "Θ(N<sup>3/2</sup>). His increments are of the form 1, 3, 7, . . . , 2<sup>k</sup> - 1."
            ],
            "guid": "Nq#dG$=e9.",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times of Sedgewick's increment sequences for <b>shellshort</b>?",
                "O(N<sup>4/3</sup>) worstcase running time and O(N<sup>7/6</sup>)&nbsp;average running time.&nbsp;The best of these is the sequence {1, 5, 19, 41, 109, . . .}, in which the terms are either of the form 9 · 4<sup>i</sup> - 9 · 2<sup>i</sup> + 1 or 4<sup>i</sup> - 3 · 2<sup>i</sup> + 1."
            ],
            "guid": "c./YzT7[!M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform <b>heapsort</b>?",
                "The basic strategy is to build a binary heap of N elements. This stage takes O(N) time. We then perform N deleteMin operations. The elements leave&nbsp;the heap smallest first, in sorted order. By recording these elements in a second array and then copying the array back, we sort N elements. Since each deleteMin takes O(log N) time, the total running time is O(N log N)."
            ],
            "guid": "h-N&!?js_-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of <b>heapsort</b>?",
                "The main problem with this algorithm is that it uses an extra array. Thus, the memory requirement is doubled. This could be a problem in some instances."
            ],
            "guid": "lv/WGqk$*t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we avoid having a second array in <b>heapsort</b>?",
                "A clever way to avoid using a second array makes use of the fact that after each deleteMin, the heap shrinks by 1. Thus the cell that was last in the heap can be used to store the element that was just deleted. As an example, suppose we have a heap with six elements. The first deleteMin produces a<sub>1</sub>. Now the heap has only five elements, so we can place a<sub>1</sub> in position 6. The next deleteMin produces a<sub>2</sub>. Since the heap will now only have four elements, we can place a<sub>2</sub> in position 5. Using this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order. If we want the elements in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger element than the child. Thus, we have a (max)heap.<br><img src=\"paste-cd90fe01f6286f6090f9c822ae1bffc81b3217f5.jpg\">"
            ],
            "guid": "c~)ccD>)JL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average number of comparisons used to heapsort a random permutation of N distinct items?",
                "2N log N - O(N log log N)."
            ],
            "guid": "C1gXA9_;&E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst running time for&nbsp;<b>mergesort</b>?",
                "O(N log N)"
            ],
            "guid": "zuY~J5ke(k",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>mergesort</b>&nbsp;done?",
                "The fundamental operation in this algorithm is merging two sorted lists. Because the lists are sorted, this can be done in one pass through the input, if the output is put in a third list. The basic merging algorithm takes two input arrays A and B, an output array C, and three counters, Actr, Bctr, and Cctr, which are initially set to the beginning of their respective arrays. The smaller of A[Actr] and B[Bctr] is copied to the next entry in C, and the appropriate counters are advanced. When either input list is exhausted, the remainder of the other list is copied to C.<br><img src=\"paste-0f8d9ec3dbd035273ebc6c4d7d998110aacaff01.jpg\"><br><img src=\"paste-fdbe8b903808c2cd9cb1ecf5f65c08658ac92fc4.jpg\"><br><img src=\"paste-4417a05617c84f5ec6d53b092b3ee788add37f83.jpg\"><br><img src=\"paste-b02ddc574bcd3b6c9c3729e1bc5d8dddd418af91.jpg\">"
            ],
            "guid": "PRE5Sl:8~|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which strategy describes <b>mergesort</b>?",
                "Divide-and-conquer strategy"
            ],
            "guid": "qmGp=S}s4|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the difference in performance between <b>mergesort</b> and other sorting algorithms with same time complexity depend on?",
                "The running time of mergesort&nbsp;depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array).&nbsp;For a language where copying objects is expensive a better alternative would be <b>quicksort</b>."
            ],
            "guid": "swU4:sfMLV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the fastest generic sorting algorithm?",
                "Quicksort has an average running time of O(N log N), and a worst running time of O(N<sup>2</sup>) which is highly unlikely in the optimized version."
            ],
            "guid": "k?Ab?twr?F",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we increase the probability of having a running time of O(N log N) in <b>quicksort</b>?",
                "By combining it with <b>heapsort</b>."
            ],
            "guid": "u`;t?5jJ@,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What kind of algorithm is <b>quicksort</b>?",
                "A divide-and conquer recursive algorithm."
            ],
            "guid": "Bg`vQGI@df",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>classic quicksort</b>.",
                "The classic quicksort algorithm to sort an array S consists of the following four easy steps:<br>1. If the number of elements in S is 0 or 1, then return.<br>2. Pick any element v in S. This is called the pivot.<br>3. Partition S - {v} (the remaining elements in S) into two disjoint groups: S1 = {x ∈ S - {v}|x ≤ v}, and S2 = {x ∈ S - {v}|x ≥ v}.<br>4. Return {quicksort(S1) followed by v followed by quicksort(S2)}.<br><img src=\"paste-a11a6986fac3d7e1674504516a6c8f7ae1a03d9f.jpg\">"
            ],
            "guid": "DP|`2-24V-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is <b>quicksort</b>&nbsp;considered to be faster than mergesort?",
                "Like mergesort, it recursively solves two subproblems and requires linear additional work, but, unlike mergesort, the subproblems are not guaranteed to be of equal size, which is potentially bad. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently. This efficiency more than makes up for the lack of equal-sized recursive calls."
            ],
            "guid": "L(AJPaG3UQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of choosing the first element as a pivot in <b>quicksort</b>?",
                "This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition, because either all the elements go into S<sub>1</sub> or they go into S<sub>2</sub>. Worse, this happens consistently throughout the recursive calls. The practical effect is that if the first element is used as the pivot and the input is presorted, then quicksort will take quadratic time to do essentially nothing at all. Moreover, presorted input (or input with a large presorted section) is quite frequent, so using the first element as pivot is an absolutely horrible idea and should be discarded immediately."
            ],
            "guid": "QB]{(@>>},",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of selecting a random pivot in <b>quicksort</b>?",
                "A safe course is merely to choose the pivot randomly. This strategy is generally perfectly safe, unless the random number generator has a flaw (which is not as uncommon as you might think), since it is very unlikely that a random pivot would consistently provide a poor partition. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all."
            ],
            "guid": "HiwzxOYPV@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of using <b>median-of-three</b> partitioning in <b>quicksort</b>?",
                "The median of a group of N numbers is the [N/2]th largest number. The best choice of pivot would be the median of the array. Unfortunately, this is hard to calculate and would slow down quicksort considerably. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. For instance, with input 8, 1, 4, 9, 6, 3, 5, 2, 7, 0, the left element is 8, the right element is 0, and the center (in position (left + right)/2 ) element is 6. Thus, the pivot would be v = 6. Using median-of-three partitioning clearly eliminates the bad case for sorted input (the partitions become equal in this case) and actually reduces the number of comparisons by 14%."
            ],
            "guid": "f/TL-NG[im",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an optimum <b>partitioning strategy</b> for <b>quicksort</b>.",
                "The first step is to get the pivot element out of the way by swapping it with the last element. i starts at the first element and j starts at the next-to-last element.<br><img src=\"paste-738c435a7a53bcb9399576d5c709440d3c2a26bc.jpg\"><br>What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are, of course, relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If&nbsp;i is to the left of j, those elements are swapped. The effect is to push a large element to the right and a small element to the left. In the example above, i would not move and j would slide over one place. The situation is as follows:<br><img src=\"paste-f5dc75ba051a99e6cda769613ce5dcbd62bd33f6.jpg\"><br>We then swap the elements pointed to by i and j and repeat the process until i and j cross:<br><img src=\"paste-87fcc822dc9ddd16c1dd90cabf81c7b2cd9bbf90.jpg\"><br>At this stage, i and j have crossed, so no swap is performed. The final part of the partitioning is to swap the pivot element with the element pointed to by i:<br><img src=\"paste-67067edfbf3988d40e6df9067564e6b098cb2ef6.jpg\"><br>When the pivot is swapped with i in the last step, we know that every element in a position p &lt; i must be small. This is because either position p contained a small element to start with, or the large element originally in position p was replaced during a swap. A similar argument shows that elements in positions p &gt; i must be large."
            ],
            "guid": "F%dn1($N;B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How to handle elements that are equal to the pivot&nbsp;in <b>quicksort</b>?",
                "i and j ought to do the same thing, since otherwise the partitioning step is biased. For instance, if i stops and j does not, then all elements that are equal to the pivot will wind up in S2.&nbsp;<br>It is better to do the unnecessary swaps (and end with a worst case of O(N log N))&nbsp;and create even subarrays than to risk wildly uneven subarrays (and a running time of O(N<sup>2</sup>)). Therefore, we should have both i and j stop if they encounter an element equal to the pivot."
            ],
            "guid": "D&o[8H4wc?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you deal with small (sub)arrays in <b>quicksort</b>?",
                "For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. Using this strategy can actually save about 15 percent in the running time (over doing no cutoff at all). A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two."
            ],
            "guid": "P.n3`0S(Hh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst running time of <b>quicksort</b>&nbsp;and when does it happen?",
                "A running time of O(N<sup>2</sup>) happens when the pivot is always the smallest number."
            ],
            "guid": "u<$DD-N_^r",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best case running time in <b>quicksort</b>&nbsp;and when does it happen?",
                "O(N log N) when the pivot is always in the middle."
            ],
            "guid": "g3B`]};B+2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>quickselect</b>&nbsp;done?",
                "Let |Si| denote the number of elements in Si.<br>1. If |S| = 1, then k = 1 and return the element in S as the answer. If a cutoff for small arrays is being used and |S| ≤ CUTOFF, then sort S and return the kth smallest element.<br>2. Pick a pivot element, v ∈ S.<br>3. Partition S - {v} into S1 and S2, as was done with quicksort.<br>4. If k ≤ |S1|, then the kth smallest element must be in S1. In this case, return quickselect(S1, k). If k = 1 + |S1|, then the pivot is the kth smallest element and we can return it as the answer. Otherwise, the kth smallest element lies in S2, and it is the (k - |S1| - 1)st smallest element in S2. We make a recursive call and return quickselect(S2, k - |S1| - 1)."
            ],
            "guid": "sE8(5dYXm#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times of <b>quickselect</b>?",
                "O(N<sup>2</sup>) worst running time.<br>O(N) average running time."
            ],
            "guid": "j4m56m{o-a",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average and lower bound of time for sorting algorithms that use only comparisons?",
                "Ω(N log N)"
            ],
            "guid": "m=WZA>yH39",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum number of leaves in a binary tree?",
                "Let T be a binary tree of depth d. Then T has at most 2<sup>d</sup> leaves."
            ],
            "guid": "fN;EuILI}G",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a the minimum depth for a binary tree?",
                "A binary tree with L leaves must have depth at least [log L]."
            ],
            "guid": "sMyx*}>fiw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons in the worst case of a sorting algorithms that uses only comparisons?",
                "[log(N!)]"
            ],
            "guid": "FchTaPbG@w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of the&nbsp;<b>information-theoretic</b> lower bound?",
                "The general theorem says that if there are P different possible cases to distinguish, and the questions are of the form YES/NO, then [log P]&nbsp;questions are always required in some case by any algorithm to solve the problem."
            ],
            "guid": "wx<]DnIG&,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the smallest item&nbsp;in a collection?",
                "N-1"
            ],
            "guid": "yhD|mzl%Tk",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the two smallest items in a collection?",
                "N + [log N]&nbsp;- 2"
            ],
            "guid": "d^^)FRw?ci",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the median of a collection?",
                "[3N/2] - O(log N)"
            ],
            "guid": "pvusr~,W<=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves in a decision tree that has all the leaves at depth <i>d</i>&nbsp;or higher?",
                "2<sup>d</sup>"
            ],
            "guid": "tO>wqJ}RP;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons to find the smallest element in a comparisons based algorithm?",
                "N - 1"
            ],
            "guid": "OSUFv0`2A7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves in a decision tree for finding the smallest of N elements?",
                "2<sup>N-1</sup>"
            ],
            "guid": ">Dj-TD9O+",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves of a decision tree for finding the kth smallest of N elements?",
                "<img src=\"paste-58c77b2aea46543e4080a8b4b0a83959d25a4ad7.jpg\">"
            ],
            "guid": "JBL7>L)TXP",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons in any comparison-based algorithm to find the kth smallest element?",
                "<img src=\"paste-3c7b264556f08af3bdb44584095b2fd85c2ca0db.jpg\">"
            ],
            "guid": "zv>E%O:*tC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the second smallest element?",
                "N + [log N] - 2&nbsp;"
            ],
            "guid": "J>I3W-FvR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the median?",
                "[3N/2] - O(log N)&nbsp;"
            ],
            "guid": "roNMVtvA&?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the minimum and maximum?",
                "[3N/2] - 2"
            ],
            "guid": "le|qI*_.|K",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition of <b>bucketsort</b>?",
                "For bucket sort to work, extra information must be available. The input A<sub>1</sub>, A<sub>2</sub>, . . . , A<sub>N</sub> must consist of only positive integers smaller than M.&nbsp;"
            ],
            "guid": "om1.jCH{dB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>bucketsorting</b>&nbsp;done?",
                "Keep an array called count, of size <i>M</i>, which is initialized to all 0s. Thus, count has <i>M</i> cells, or buckets, which are initially empty. When A<sub>i</sub> is read, increment count[A<sub>i</sub>] by 1. After all the input is read, scan the count array, printing out a representation of the sorted list."
            ],
            "guid": "ddz](5q102",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the runtime of <b>bucketsort</b>?",
                "This algorithm takes O(M + N); where M is the the maximum number in the range, and N is number of sorted values. If M is O(N), then the total is O(N)."
            ],
            "guid": "k[E~}U&1u&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Does <b>bucketsort</b>&nbsp;violate the comparisons lower bound?",
                "It turns out that it does not because it uses a more powerful operation than simple comparisons. By incrementing the appropriate bucket, the algorithm essentially performs an M-way comparison in unit time."
            ],
            "guid": "Bb*j0o?qnR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>radix sort</b>&nbsp;done?",
                "Suppose we have 10 numbers in the range 0 to 999 that we would like to sort. In general this is N numbers in the range 0 to b<sup>p-1</sup> for some constant p. Obviously we cannot use bucket sort; there would be too many buckets. The trick is to use several passes of bucket sort. The natural algorithm would be to bucket sort by the most significant “digit” (digit is taken to base b), then next most significant, and so on. But a simpler idea is to perform bucket sorts in the reverse order, starting with the least significant “digit” first. Of course, more than one number could fall into the same bucket, and unlike the original bucket sort, these numbers could be different, so we keep them in a list. Each pass is stable: Items that agree in the current digit retain the ordering determined in prior passes.<br>After the first pass, the items are sorted on the least significant digit, and in general, after the kth pass, the items are sorted on the k least significant digits. So at the end, the items are completely sorted.\n<br><img src=\"paste-903a902b305b6352c36549a4f9e3c471e8bc54d8.jpg\">"
            ],
            "guid": "C?!Xo+#e#7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>bucketsort</b>?",
                "O (p(N + b)) where p is the number of passes, N is the number of elements to sort, and b is the number of buckets."
            ],
            "guid": "viH~rp5}=)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use <b>radix sort </b>to sort strings?",
                "If all the strings have the same length L, then by using buckets for each character, we can implement a radix sort in O (NL)&nbsp;time. We assume that all characters are ASCII, residing in the first 256 positions of the Unicode character set. In each pass, we add an item to the appropriate bucket, and then after all the buckets are populated, we step through the buckets dumping everything back to the array. Notice that when a bucket is populated and emptied in the next pass, the order from the current pass is preserved."
            ],
            "guid": "nmVGRkfJ])",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an alternative implementation of <b>radix sort </b>that&nbsp;avoids using vectors to represent buckets?",
                "In <b>counting radix sort</b>, we maintain a count of how many items would go in each bucket; this information can go into an array count, so that count[k] is the number of items that are in bucket k. Then we can use another array offset, so that offset[k]&nbsp;represents the number of items whose value is strictly smaller than k. Then when we see a value k for the first time in the final scan, offset[k] tells us a valid array spot where it can be written to (but we have to use a temporary array for the write), and after that is done, offset[k] can be incremented. Counting radix sort thus avoids the need to keep<br>lists. As a further optimization, we can avoid using offset by reusing the count array. The modification is that we initially have count[k+1] represent the number of items that are in bucket k. Then after that information is computed, we can scan the count array from the smallest to largest index and increment count[k] by count[k-1]. It is easy to verify that after this scan, the count array stores exactly the same information that would have been stored in offset."
            ],
            "guid": "k8FwuBMl<~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which cases can <b>counting radix sort</b>&nbsp;be slower than using buckets?",
                "It can suffer from poor locality (out is filled in non-sequentially) and thus, surprisingly, it is not always faster than using a vector of vectors."
            ],
            "guid": "O~`?]av-:o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we extend <b>radix sort</b>&nbsp;to work with variable length strings?",
                "We can extend either version of radix sort (buckets or counting versions) to work with variable-length strings. The basic algorithm is to first sort the strings by their length. Instead of looking at all the strings,&nbsp;we can then look only at strings that we know are long enough. Since the string lengths are small numbers, the initial sort by length can be done by—bucket sort!"
            ],
            "guid": "P<k[JR@-yJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When does <b>radix sort </b>function especially well for strings?",
                "When the characters in the string are drawn from a reasonably small alphabet and when the strings either are relatively short or are very similar, because the O(N log N) comparison-based sorting algorithms will generally look only at a small number of characters in each string comparison, once the average string length starts getting large, radix sort’s advantage is minimized or evaporates completely."
            ],
            "guid": "zS+bg`;.+y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What category of sorting algorithms deals &nbsp;with very large amount of input?",
                "External sorting algorithms."
            ],
            "guid": "JF0wd!tI`g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why do we need <b>external sorting algorithms</b>?",
                "Most of the internal sorting algorithms take advantage of the fact that memory is directly addressable. Shellsort compares elements a[i] and a[i-h<sub>k</sub>] in one time unit. Heapsort compares elements a[i] and a[i*2+1] in one time unit. Quicksort, with median-of-three partitioning, requires comparing a[left], a[center], and a[right] in a constant number of time units. If the input is on a tape, then all these operations lose their efficiency, since elements on a tape can only be accessed sequentially. Even if the data are on a disk, there is still a practical loss of efficiency because of the delay required to spin the disk and move the disk head."
            ],
            "guid": "o)4Ue-ZFG*",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What properties of tapes affect the choice of sorting algorithms?",
                "The wide variety of mass storage devices makes external sorting much more device dependent than internal sorting. The algorithms that we will consider work on tapes, which are probably the most restrictive storage medium. Since access to an element on tape is done by winding the tape to the correct location, tapes can be efficiently accessed only in sequential order (in either direction)."
            ],
            "guid": "hhC9HK4SK(",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of the number of available tape-drives on sorting?",
                "We need two drives to do an efficient sort; the third drive simplifies matters. If only one tape drive is present, then we are in trouble: Any algorithm will require Ω(N<sup>2</sup>) tape accesses."
            ],
            "guid": "j8c|Q[qulN",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>basic/simple external sorting</b> done?",
                "The basic external sorting algorithm uses the merging algorithm from mergesort. Suppose we have four tapes, T<sub>a1</sub>, T<sub>a2</sub>, T<sub>b1</sub>, T<sub>b2</sub>, which are two input and two output tapes. Depending on the point in the algorithm, the a and b tapes are either input tapes or output tapes. Suppose the data are initially on T<sub>a1</sub>. Suppose further that the internal memory can hold (and sort) M records at a time. A natural first step is to read M records at a time from the input tape, sort the records internally, and then write the sorted records alternately to T<sub>b1</sub> and T<sub>b2</sub>. We will call each set of sorted records a run. When this is done, we rewind all the tapes.<br><img src=\"paste-caf8b352f892a58b68a9e7b4ad46bd79b4601262.jpg\"><br>If M = 3, then after the runs are constructed, the tapes will contain the data indicated in the following figure:<br><img src=\"paste-7b42642087ca77b8ce7a1a0796b5e802ba1fd873.jpg\"><br>Now T<sub>b1</sub> and T<sub>b2</sub> contain a group of runs. We take the first run from each tape and merge them, writing the result, which is a run twice as long, onto T<sub>a1</sub>. Recall that merging two sorted lists is simple; we need almost no memory, since the merge is performed as T<sub>b1</sub> and T<sub>b2</sub> advance. Then we take the next run from each tape, merge these, and write the result to T<sub>a2</sub>. We continue this process, alternating between T<sub>a1</sub> and T<sub>a2</sub>, until either T<sub>b1</sub> or T<sub>b2</sub> is empty. At this point either both are empty or there is one run left. In the latter case, we copy this run to the appropriate tape. We rewind all four tapes and repeat the same steps, this time using the a tapes as input and the b tapes as output. This will give runs of 4M. We continue the process until we get one run of length N.<br><img src=\"paste-5baf2e4075678e9bcee1aac407046b85f241d7cb.jpg\"><br><img src=\"paste-ba5aa4733fa77deb9fc04a339f78c672d5ada772.jpg\">"
            ],
            "guid": "DYq3^T,S>t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>simple/basic external sorting</b>?",
                "The algorithm will require [log(N/M)]&nbsp;passes, plus the initial run-constructing pass. For instance, if we have 10 million records of 128 bytes each, and four megabytes of internal memory, then the first pass will create 320 runs. We would then need nine more passes to complete the sort."
            ],
            "guid": "lk-L-%7tsw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you do <b>multiway sort</b>?",
                "If we have extra tapes, then we can expect to reduce the number of passes required to sort our input. We do this by extending the basic (two-way) merge to a k-way merge.&nbsp;Merging two runs is done by winding each input tape to the beginning of each run. Then the smaller element is found, placed on an output tape, and the appropriate input tape is advanced. If there are k input tapes, this strategy works the same way, the only difference being that it is slightly more complicated to find the smallest of the k elements. We can find the smallest of these elements by using a priority queue. To obtain the next element to write on the output tape, we perform a deleteMin operation. The appropriate input tape is advanced, and if the run on the input tape is not yet completed, we insert the new element into the priority queue.<br><img src=\"paste-47afc056d248665879e4b7ebd51d16ba73463b97.jpg\"><br>We then need two more passes of three-way merging to complete the sort.<br><img src=\"paste-f365a5f98e8d9e4dffd0def3b7cdc75697fd27fc.jpg\"><br><img src=\"paste-63996316413844fd2e89e376fb4f25f9ce1cf015.jpg\">"
            ],
            "guid": "nEFLj]kw5-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>multiway sort</b>?",
                "After the initial run construction phase, the number of passes required using k-way merging is [log<sub>k</sub>(N/M)], because the runs get k times as large in each pass."
            ],
            "guid": "uvD]{[XdTf",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the benefit of <b>polyphase merge</b>?",
                "The <i>k</i>-way merging strategy requires the use of 2<i>k</i> tapes. This could be prohibitive for some applications. It is possible to get by with only <i>k</i> + 1&nbsp;tapes."
            ],
            "guid": "QkSXoxg1ka",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>polyphase sort</b>&nbsp;done?",
                "Suppose we have three tapes, T<sub>1</sub>, T<sub>2</sub>, and T<sub>3</sub>, and an input file on T<sub>1</sub> that will produce 34 runs. Suppose we put 21 runs on T<sub>2</sub> and 13 runs on T<sub>3</sub>. We would then merge 13 runs onto T<sub>1</sub> before T<sub>3</sub> was empty. At this point, we could rewind T<sub>1</sub> and T<sub>3</sub>, and merge T<sub>1</sub>, with 13 runs, and T<sub>2</sub>, which has 8 runs, onto T<sub>3</sub>. We could then merge 8 runs until T<sub>2</sub> was empty, which would leave 5 runs left on T<sub>1</sub> and 8 runs on T<sub>3</sub>. We could then merge T<sub>1</sub> and T<sub>3</sub>, and so on.<br><img src=\"paste-fcd4ea04e9fb20e6fec8937d1ce6ab1e6fcab292.jpg\">"
            ],
            "guid": "lOI_1_HOC;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best way to distribute runs in <b>polyphase sort</b>?",
                "If the number of runs is a Fibonacci number F<sub>N</sub>, then the best way to distribute them is to split them into two Fibonacci numbers F<sub>N-1</sub> and F<sub>N-2</sub>. Otherwise, it is necessary to pad the tape with dummy runs in order to get the number of runs up to a Fibonacci number."
            ],
            "guid": "t1P-43m,1$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>replacement selection</b>&nbsp;done?",
                "Initially, M records are read into memory and placed in a priority queue. We perform a deleteMin, writing the smallest (valued) record to the output tape. We read the next record from the input tape. If it is larger than the record we have just written, we can add it to the priority queue. Otherwise, it cannot go into the current run. Since the priority queue is smaller by one element, we can store this new element in the dead space of the priority queue until the run is completed and use the element for the next run. Storing an element in the dead space is similar to what is done in heapsort. We continue doing this until the size of the priority queue is zero, at which point the run is over. We start a new run by building a new priority queue, using all the elements in the dead space.<br><img src=\"paste-91103479777465e4fdeb255e7b0eebaeec1a30de.jpg\">"
            ],
            "guid": "Jm)`MKtqkw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which cases <b>replacement selection</b>&nbsp;is useful?",
                "It is possible for replacement selection to do no better than the standard algorithm. However, the input is frequently sorted or nearly sorted to start with, in which case replacement selection produces only a few very long runs. This kind of input is common for external sorts and makes replacement selection extremely valuable."
            ],
            "guid": "cxhzi/!}g<",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>relation</b>.",
                "A relation R is defined on a set S if for every pair of elements (a, b), a, b ∈ S, a R b is either true or false. If a R b is true, then we say that a is related to b."
            ],
            "guid": "lxi~zw]@Qu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define an <b>equivalence relation</b>.",
                "An equivalence relation is a relation R that satisfies three properties:<br>1. (Reflexive) a R a, for all a ∈ S.<br>2. (Symmetric) a R b if and only if b R a.<br>3. (Transitive) a R b and b R c implies that a R c."
            ],
            "guid": "P>G7G2nGL^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why ≤ relationship is not an <b>equivalence relationship</b>?",
                "Although it is reflexive, since a ≤ a, and transitive, since a ≤ b and b ≤ c implies a ≤ c, it is not symmetric, since a ≤ b does not imply b ≤ a."
            ],
            "guid": "zkYTeb8;+A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Examples of <b>equivalence relation</b>.",
                "Electrical connectivity, where all connections are by metal wires, is an equivalence relation. The relation is clearly reflexive, as any component is connected to itself. If a is electrically connected to b, then b must be electrically connected to a, so the relation is symmetric. Finally, if a is connected to b and b is connected to c, then a is connected to c. Thus electrical connectivity is an equivalence relation.<br>Two cities are related if they are in the same country. It is easily verified that this is an equivalence relation. Suppose town a is related to b if it is possible to travel from a to b by taking roads. This relation is an equivalence relation if all the roads are two-way."
            ],
            "guid": "I3,WsD;XH@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>equivalence class</b>?",
                "The equivalence class of an element a ∈ S is the subset of S that contains all the elements that are related to a. Notice that the equivalence classes form a partition of S: Every member of S appears in exactly one equivalence class. To decide if a ∼ b, we need only to check whether a and b are in the same equivalence class. This provides our strategy to solve the equivalence problem."
            ],
            "guid": "m*r2_07#mv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>disjoint</b>&nbsp;sets.",
                "When we have a collection of N sets, each with one element. This initial representation is that all relations (except reflexive relations) are false. Each set has a different element, so that Si ∩ Sj = ∅; this makes the sets disjoint."
            ],
            "guid": "y-.BVE`HNs",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the disjoint set <b>union/find algorithm</b>.",
                "There are two permissible operations. The first is find, which returns the name of the set (that is, the equivalence class) containing a given element. The second operation adds relations. If we want to add the relation a ∼ b, then we first see if a and b are already related. This is done by performing finds on both a and b and checking whether they are in the same equivalence class. If they are not, then we apply union.1 This operation merges&nbsp;the two equivalence classes containing a and b into a new equivalence class. From a set point of view, the result of ∪ is to create a new set Sk = S<sub>i</sub> ∪S<sub>j</sub>, destroying the originals and preserving the disjointness of all the sets."
            ],
            "guid": "v~!B7c=<&4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of <b>union/find algorithm</b>?",
                "This algorithm is dynamic because, during the course of the algorithm, the sets can change via the union operation. The algorithm must also operate online: When a find is performed, it must give an answer before continuing. Another possibility would be an offline algorithm. Such an algorithm would be allowed to see the entire sequence of unions and finds. The answer it provides for each find must still be consistent with all the unions that were performed up until the find, but the algorithm can give all its answers after it has seen all the questions."
            ],
            "guid": "CHzmdfjs/D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Do we perform any operations comparing the relative values of elements&nbsp;in <b>union/find algorithm</b>?",
                "We do not perform any operations comparing the relative values of elements but merely require knowledge of their location. For this reason, we can assume that all the elements have been numbered sequentially from 0 to N - 1 and that the numbering can&nbsp;be determined easily by some hashing scheme. Thus, initially we have S<sub>i</sub> = {i} for i = 0 through N - 1."
            ],
            "guid": "GwV>EypCW0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the name of the set returned by find in <b>union/find algorithm</b>?",
                "The name of the set returned by find is actually fairly arbitrary. All that really matters is that find(a)==find(b) is true if and only if a and b are in&nbsp;the same set."
            ],
            "guid": "f;*Ad3To6o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the strategies of using union/find algorithm to solve graph theory problems?",
                "There are two strategies to solve this problem. One ensures that the find instruction can be executed in constant worst-case time, and the other ensures that the union instruction can be executed in constant worst-case time. It has recently been shown that both cannot be done simultaneously in constant worst-case time."
            ],
            "guid": "xOPEo_[U8u",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we implement a fast <b>find</b>&nbsp;operation in a <b>union/find algorithm</b>?",
                "We maintain, in an array, the name of the equivalence class for each element. Then find is just a simple O(1) lookup. Suppose we want to perform union(a,b). Suppose that a is in equivalence class <i>i</i> and <i>b</i> is in equivalence class <i>j</i>. Then we scan down the array, changing all <i>i</i>s to <i>j</i>. Unfortunately, this scan takes Θ(N). Thus, a sequence of N - 1 unions (the maximum, since then everything is in one set) would take Θ(N<sup>2</sup>) time. If there are Θ(N<sup>2</sup>) find operations, this performance is fine, since the total running time would then amount to O(1) for each union or find operation over the course of the algorithm. If there are fewer finds, this bound is not acceptable.<br>One idea is to keep all the elements that are in the same equivalence class in a linked list. This saves time when updating, because we do not have to search through the entire array. This by itself does not reduce the asymptotic running time, because it is still possible to perform Θ(N<sup>2</sup>) equivalence class updates over the course of the algorithm. If we also keep track of the size of each equivalence class, and when performing unions&nbsp;we change the name of the smaller equivalence class to the larger, then the total time spent for N - 1 merges is O(N log N). The reason for this is that each element can have its equivalence class changed at most log N times, since every time its class is changed, its new equivalence class is at least twice as large as its old. Using this strategy, any sequence of M finds and up to N - 1 unions takes at most O(M + N log N) time."
            ],
            "guid": "n;[)&BE*/M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we use to represent <b>disjoint sets</b>&nbsp;and why?",
                "Since the problem does not require that a find operation return any specific name, just that finds on two elements return the same answer if and only if they are in the same set. One idea might be to use a tree to represent each set, since each element in a tree has the same root. Thus, the root can be used to name the set. We will represent each set by a tree."
            ],
            "guid": "qnU#|/v@h;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we use to store a <b>disjoint set</b> and why?",
                "The name of a set is given by the node at the root. Since only the name of the parent is required, we can assume that this tree is stored implicitly in an array: Each entry s[i] in the array represents the parent of element i. If <i>i</i> is a root, then s[i] = -1."
            ],
            "guid": "zC5!&f[M%|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we perform a <b>union</b>&nbsp;in disjoint sets?",
                "<img src=\"paste-4b526a7803daa2d53b8d65b68011d4e7a4e1868d.jpg\"><br>To perform a union of two sets, we merge the two trees by making the parent link of one tree’s root link to the root node of the other tree. It should be clear that this operation takes constant time. The figures below represent the forest after each of union(4,5), union(6,7), union(4,6), where we have adopted the convention that the new root after the union(x,y) is x.<br><img src=\"paste-fba65d001b8e1a9d997ba993bd4509122620b436.jpg\"><br><img src=\"paste-a6916e3524fa8b221406150c1bfe4ba991b05208.jpg\"><br><img src=\"paste-87776b5f92b11e71cc42e802fe70751b1fa7886d.jpg\"><br>The implicit representation of the last forest is shown below:<br><img src=\"paste-07f33a00b349b29d8793850932732e3b650a8f31.jpg\">"
            ],
            "guid": "puvLU__~%?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a <b>find(x) </b>operation implemented in disjoint sets and what's its running time?",
                "A find(x) on element x is performed by returning the root of the tree containing x. The time to perform this operation is proportional to the depth of the node representing x, assuming, of course, that we can find the node representing x in constant time. Using the strategy above, it is possible to create a tree of depth N - 1, so the worst-case running&nbsp;time of a find is Θ(N). Typically, the running time is computed for a sequence of M intermixed instructions. In this case, M consecutive operations could take Θ(MN) time in the worst case."
            ],
            "guid": "su:/{E$w4q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>union-by-size</b>.",
                "<img src=\"paste-ab4f56d3d662b0d8adaf1725fc24d46e908fbda4.jpg\"><br>The unions above were performed rather arbitrarily, by making the second tree a subtree of the first. A simple improvement is always to make the smaller tree a subtree of the larger, breaking ties by any method.<br><img src=\"paste-8e804226c87e81ef0ae579d897618d88159d2fd8.jpg\">"
            ],
            "guid": "e~=~F|XKw7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum depth and running time when unions are done by size?",
                "The depth of any node is never more than log N. The running time for a find operation is O(log N), and a sequence of M operations takes O(M log N).<br><img src=\"paste-fc20d98b336d1c6124d0f423d7f44ec2defc2908.jpg\">"
            ],
            "guid": "n=Qj&o86%3",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>union-by-size</b>&nbsp;implemented?",
                "To implement this strategy, we need to keep track of the size of each tree. Since we are really just using an array, we can have the array entry of each root contain the <i>negative</i> of&nbsp;the size of its tree. Thus, initially the array representation of the tree is all -1s. When a union is performed, check the sizes; the new size is the sum of the old. Thus, union-by-size is not at all difficult to implement and requires no extra space. It is also fast, on average. For virtually all reasonable models, it has been shown that a sequence of M operations requires O(M) average time if union-by-size is used. This is because when random unions are performed, generally very small (usually one-element) sets are merged with large sets throughout the algorithm."
            ],
            "guid": "B~Zc30psCL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative to union-by-size guarantees a maximum depth of O(log N)?",
                "In <b>union-by-height,</b>&nbsp;we keep track of the height, instead of the size, of each tree and perform unions by making the shallow tree a subtree of the deeper tree. This is an easy algorithm, since the height of a tree increases only when two equally deep trees are joined (and then the height goes up by one). Thus, union-by-height is a trivial modification of union-by-size. Since heights of zero would not be negative, we actually store the negative of height, minus an additional 1. Initially, all entries are -1.<br><img src=\"paste-457b62ee8e12e5c8aed91c0f1ee159930cc655c3.jpg\">"
            ],
            "guid": "jjS{o0WJ`$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the shortcomings of the <b>union/find algorithm</b>?",
                "The worst case of O(M log N) can occur fairly easily and naturally. For instance, if we put all the sets on a queue and repeatedly dequeue the first two sets and enqueue the union, the worst case occurs. If there are many more finds than unions, this running time is worse than that of the quick-find algorithm. Moreover, it should be clear that there are probably no more improvements possible for the union algorithm. This is based on the observation that any method to perform the unions will yield the same worst-case trees, since it must break ties arbitrarily."
            ],
            "guid": "Q$Tay|tjnN",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>path compression</b>&nbsp;done?",
                "Path compression is performed during a find operation and is independent of the strategy used to perform unions. Suppose the operation is find(x). Then the effect of path compression is that every node on the path from x to the root has its parent changed to the root.<br>After find(14), the following worst case<br><img src=\"paste-c3ece164e96901bb96c7095fb58bfad0ad9721d9.jpg\"><br>changes to<br><img src=\"paste-bd40421692b51039e97b050744017ef77be4b7c4.jpg\">"
            ],
            "guid": "f14/hIonm,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When does<b> </b>the <b>path compression </b>work best and what's its impact?",
                "When unions are done arbitrarily, path compression is a good idea, because there is an abundance of deep nodes and these are brought near the root by path compression. It has been proven that when path compression is done in this case, a sequence of <i>M</i>&nbsp;operations requires at most O(M log N) time. It is still an open problem to determine what the average-case behavior is in this situation."
            ],
            "guid": "Kh`yl]W)Z,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What algorithm is <b>path compression</b>&nbsp;compatible with?",
                "Path compression is perfectly compatible with union-by-size, and thus both routines can be implemented at the same time. Since doing union-by-size by itself is expected to execute a sequence of <i>M</i> operations in linear time, it is not clear that the extra pass involved in path compression is worthwhile on average."
            ],
            "guid": "m!QYydYN1$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What algorithm is&nbsp;<b>path compression</b>&nbsp;not compatible with and why?",
                "Path compression is not entirely compatible with union-by-height, because path compression can change the heights of the trees. It is not at all clear how to recompute them efficiently. The answer is do not! Then the heights stored for each tree become estimated heights (sometimes known as ranks), but it turns out that union-by-rank (which is what this has now become) is just as efficient in theory as union-by-size. Furthermore, heights<br>are updated less often than sizes. As with union-by-size, it is not clear whether path compression is worthwhile on average."
            ],
            "guid": "Kj<X1F<Pn:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst case for union-by-rank and path compression?",
                "When both heuristics are used, the algorithm is almost linear in the worst case. Specifically, the time required in the worst case is Θ(Mα(M, N)) (provided M ≥ N), where α(M, N) is an incredibly slowly growing function that for all intents and purposes is at most 5 for any problem instance. However, α(M, N) is not a constant, so the running time is not linear."
            ],
            "guid": "luY{0sAg`/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How are <b>disjoint sets</b>&nbsp;used to create mazes?",
                "A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit). We then continually choose a wall randomly, and knock it down if the cells that the wall separates are not already connected to each other. If we repeat this process until the starting and ending cells are connected, then we have a maze. It is actually better to continue knocking down walls until every cell is reachable from every other cell (this generates more false leads in the maze).<br>We illustrate the algorithm with a 5-by-5 maze. The figure below shows the initial configuration. We use the union/find data structure to represent sets of cells that are connected to each other. Initially, walls are everywhere, and each cell is in its own equivalence class.<br><img src=\"paste-4a5cd15f5efd5d6e9747aed1bfdfa65c1ba9fc97.jpg\"><br>This figure shows a later stage of the algorithm, after a few walls have been knocked down:<br><img src=\"paste-8c3366c4c273a57f0546432ae3c0ce7b47fe1b32.jpg\"><br>Suppose, at this stage, the wall that connects cells 8 and 13 is randomly targeted. Because 8 and 13 are already connected (they are in the same set), we would not remove the wall, as it would simply trivialize the maze. Suppose that cells 18 and 13 are randomly targeted next. By performing two find operations, we see that these are in different sets; thus 18 and 13 are not already connected. Therefore, we knock down the wall that separates them.<br><img src=\"paste-3328f29f8fbae67b793800c5aaf275c9f6b419f8.jpg\"><br>Notice that as a result of this operation, the sets containing 18 and 13 are combined via a union operation. This is because everything that was connected to 18 is now connected to everything that was connected to 13. At the end of the algorithm everything is connected, and we are done.<br><img src=\"paste-8ef2fbc44c81120e0c6247cc1ea09721ab957da7.jpg\">"
            ],
            "guid": "Ids/4BHkUS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the maze generation algorithm?",
                "The running time of the algorithm is dominated by the union/find costs. The size of the union/find universe is equal to the number of cells. The number of find operations is proportional to the number of cells, since the number of removed walls is one less than the number of cells, while with care, we see that there are only about twice the number of walls as cells in the first place. Thus, if N is the number of cells, since there are two finds&nbsp;per randomly targeted wall, this gives an estimate of between (roughly) 2N and 4N find operations throughout the algorithm. Therefore, the algorithm's running time can be taken as O(N log∗ N), and this algorithm quickly generates a maze."
            ],
            "guid": "wR(R4,lRc}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>graph</b>.",
                "A graph <i>G </i>= (<i>V</i>,<i> E</i>) consists of a set of <b>vertices</b>, <i>V</i>, and a set of <b>edges</b>, <i>E</i>. Each edge is a pair (v, w), where v, w ∈ V. Edges are sometimes referred to as arcs. If the pair is ordered, then the graph is <b>directed</b>. Directed graphs are sometimes referred to as <b>digraphs</b>. Vertex <i>w</i> is adjacent to <i>v</i> if and only if (<i>v</i>, <i>w</i>) ∈ <i>E</i>. In an undirected graph with edge (<i>v</i>, <i>w</i>), and hence (<i>w</i>, <i>v</i>), <i>w</i> is adjacent to <i>v</i> and <i>v</i> is adjacent to <i>w</i>. Sometimes an edge has a third component, known as either a <b>weight</b> or a <b>cost</b>."
            ],
            "guid": "P-^L>CE3O;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>path</b> in a graph.",
                "A path in a graph is a sequence of vertices <i>w1</i>, <i>w2</i>, <i>w3</i>, . . . , <i>wN</i> such that (<i>wi</i>, <i>w</i><sub>i+1</sub>) ∈ <i>E</i> for 1 ≤ i &lt; <i>N</i>. The length of such a path is the number of edges on the path, which is equal to <i>N</i> - 1. We allow a path from a vertex to itself; if this path contains no edges, then the path length is 0. This is a convenient way to define an otherwise special case. If the graph contains an edge (<i>v</i>, <i>v</i>) from a vertex to itself, then the path <i>v</i>, <i>v</i> is sometimes referred to as a <b>loop</b>. The graphs we will consider will generally be loopless. A <b>simple path</b> is a path such that all vertices are distinct, except that the first and last could be the same."
            ],
            "guid": "jlaI[@$SRv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>cycle</b>&nbsp;in a graph.",
                "A <b>cycle</b> in a directed graph is a path of length at least 1 such that w<sub>1</sub> = w<sub>N</sub>; this cycle is simple if the path is simple. For undirected graphs, we require that the edges be distinct. The logic of these requirements is that the path u, v, u in an undirected graph should not be considered a cycle, because (u, v) and (v, u) are the same edge. In a directed graph, these are different edges, so it makes sense to call this a cycle. A directed graph is <b>acyclic</b> if it has no cycles. A directed acyclic graph is sometimes referred to by its abbreviation, <b>DAG</b>."
            ],
            "guid": "Fo@/R$2W%f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>connected</b>&nbsp;graph.",
                "An undirected graph is <b>connected</b> if there is a path from every vertex to every other vertex. A directed graph with this property is called <b>strongly connected</b>. If a directed graph is not strongly connected, but the underlying graph (without direction to the arcs) is connected, then the graph is said to be <b>weakly connected</b>. A <b>complete graph</b> is a graph in which there is an edge between every pair of vertices."
            ],
            "guid": "KZ;jDJiA%A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you model an airport system using a graph.",
                "Each airport is a vertex, and two vertices are connected by an edge if there is a nonstop flight from the airports that are represented by the vertices. The edge could have a weight, representing the time, distance, or cost of the flight. It is reasonable to assume that such a graph is directed, since it might take longer or cost more (depending on local taxes, for example) to fly in different directions. We would probably like to make sure that the airport system is strongly connected, so that it is always possible to fly from any airport to any other airport. We might also like to quickly determine the best flight between any two airports. “Best” could mean the path with the fewest number of edges or could be taken with respect to one, or all, of the weight measures."
            ],
            "guid": "g$>rPS8V:^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you model traffic flow using a graph.",
                "Traffic flow can be modeled by a graph. Each street intersection represents a vertex, and each street is an edge. The edge costs could represent, among other things, a speed limit or a capacity (number of lanes). We could then ask for the shortest route or use this information to find the most likely location for bottlenecks."
            ],
            "guid": "Iusiw?E_GF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a two-dimensional array to represent a <b>graph</b>?",
                "This is known as an adjacency matrix representation. For each edge (u, v), we set A[u][v] to true; otherwise the entry in the array is false. If the edge has a weight associated with it, then we can set A[u][v] equal to the weight and use either a very large or a very small weight as a sentinel to indicate nonexistent edges. For instance, if we were looking for the cheapest airplane route, we could represent nonexistent flights with a cost of ∞. If we were looking, for some strange reason, for the most expensive airplane route, we could use -∞ (or perhaps 0) to represent nonexistent edges."
            ],
            "guid": "C[PW2>is?0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of using an <b>adjacency matrix</b> to represent a <b>graph</b>?",
                "Although this has the merit of extreme simplicity, the space requirement is Θ(|V|<sup>2</sup>), which can be prohibitive if the graph does not have very many edges. An adjacency matrix is an appropriate representation if the graph is dense: |E| = Θ(|V|<sup>2</sup>). In most of the applications that we shall see, this is not true. For instance, suppose the graph represents a street map. Assume a Manhattan-like orientation, where almost all the streets run either<br>north-south or east-west. Therefore, any intersection is attached to roughly four streets, so if the graph is directed and all streets are two-way, then |E| ≈ 4|V|. If there are 3,000 intersections, then we have a 3,000-vertex graph with 12,000 edge entries, which would require an array of size 9,000,000. Most of these entries would contain zero. This is intuitively bad, because we want our data structures to represent the data that are actually there and not the data that are not present."
            ],
            "guid": "uVMZ?Hh1K=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best way to represent <b>sparse graphs</b>?",
                "If the graph is not dense, in other words, if the graph is sparse, a better solution is an adjacency list representation. For each vertex, we keep a list of all adjacent vertices. The space requirement is then O(|E| + |V|), which is linear in the size of the graph.&nbsp;If the edges have weights, then this additional information is also stored in the adjacency lists.<br><img src=\"paste-cb4c91bbb05e86bff147aad9a00eba584049bbeb.jpg\">"
            ],
            "guid": "vmAPh%8f/@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What options do we have for representing an&nbsp;<b>adjacency list</b>?",
                "There are several alternatives for maintaining the adjacency lists. First, observe that the lists themselves can be maintained in either vectors or lists. However, for sparse graphs, when using vectors, the programmer may need to initialize each vector with a smaller capacity than the default; otherwise, there could be significant wasted space.<br>Because it is important to be able to quickly obtain the list of adjacent vertices for any vertex, the two basic options are to use a map in which the keys are vertices and the values are adjacency lists, or to maintain each adjacency list as a data member of a Vertex class. The first option is arguably simpler, but the second option can be faster, because it avoids repeated lookups in the map.<br>In the second scenario, if the vertex is a string (for instance, an airport name, or the name of a street intersection), then a map can be used in which the key is the vertex name and the value is a Vertex (typically a pointer to a Vertex), and each Vertex object keeps a list of (pointers to the) adjacent vertices and perhaps also the original string name."
            ],
            "guid": "I7.n6o@Hvj",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>topological sort</b>.",
                "A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from v<sub>i</sub> to v<sub>j</sub>, then v<sub>j</sub> appears after v<sub>i</sub> in the ordering. The graph represents the course prerequisite structure at a state university in Miami. A directed edge (v, w) indicates that course v must be completed before course w may be attempted. A topological ordering of these courses is any course sequence that does not violate the prerequisite requirement.<br><img src=\"paste-17866d9c05d567f5b8aab12cef9e74a9604c2d4e.jpg\">"
            ],
            "guid": "iz4).2~AbQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which case <b>topological order</b> is not possible?",
                "It is clear that a topological ordering is not possible if the graph has a cycle, since for two vertices v and w on the cycle, v precedes w and w precedes v. Furthermore, the ordering is not necessarily unique; any legal ordering will do. In the graph below, v<sub>1</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, v<sub>3</sub>, v<sub>7</sub>, v<sub>6</sub> and v<sub>1</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, v<sub>7</sub>, v<sub>3</sub>, v<sub>6</sub> are both topological orderings.<br><img src=\"paste-e5e3b9da0df9c8bf4a2cc600b2530c17aa65f2ae.jpg\">"
            ],
            "guid": "srk/(A4ApV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a strategy to find a <b>topological order.</b>",
                "A simple algorithm to find a <b>topological ordering</b> is first to find any vertex with no incoming edges. We can then print this vertex, and remove it, along with its edges, from the graph. Then we apply this same strategy to the rest of the graph."
            ],
            "guid": "zb4XEltT{^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>indegree</b>&nbsp;and how is it calculated?",
                "We define the <b>indegree</b> of a vertex v as the number of edges (u, v). We compute the indegrees of all vertices in the graph."
            ],
            "guid": "vJas#rB[i^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use <b>indegree</b>s&nbsp;to perform <b>toplogy sorting</b>?",
                "Assuming that the indegree for each&nbsp;vertex is stored, and that the graph is read into an adjacency list, we can then apply the following algorithm to generate a topological ordering:<br><img src=\"paste-48c72e2bd7fcaa1e3ca1123c2f664a2f98e163be.jpg\"><br>The function <b>findNewVertexOfIndegreeZero</b> scans the array of vertices looking for a vertex with indegree 0 that has not already been assigned a topological number. It returns NOT_A_VERTEX if no such vertex exists; this indicates that the graph has a cycle. Because <b>findNewVertexOfIndegreeZero</b> is a simple sequential scan of the array of vertices, each call to it takes O(|V|) time. Since there are |V| such calls, the running time of the algorithm is O(|V|<sup>2</sup>)."
            ],
            "guid": "HO,?xlK~@Z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we optimize scanning the vertices array in <b>topological sort</b>?",
                "By paying more careful attention to the data structures, it is possible to do better. The cause of the poor running time is the sequential scan through the array of vertices. If the&nbsp;graph is sparse, we would expect that only a few vertices have their indegrees updated during each iteration. However, in the search for a vertex of indegree 0, we look at (potentially) all the vertices, even though only a few have changed.<br>We can remove this inefficiency by keeping all the (unassigned) vertices of indegree 0 in a special box. The <b>findNewVertexOfIndegreeZero</b> function then returns (and removes) any vertex in the box. When we decrement the indegrees of the adjacent vertices, we check each vertex and place it in the box if its indegree falls to 0.<br>To implement the box, we can use either a stack or a queue; we will use a queue. First, the indegree is computed for every vertex. Then all vertices of indegree 0 are placed on an initially empty queue. While the queue is not empty, a vertex v is removed, and all vertices adjacent to v have their indegrees decremented. A vertex is put on the queue as soon as its indegree falls to 0. The topological ordering then is the order in which the vertices dequeue.<br><img src=\"paste-1a3a3e924c2529e0d3ae6665efac1ed9e27c9040.jpg\"><br><img src=\"paste-4ff51f5cbde13fe4ada98cb70b204f67f7312e7d.jpg\">"
            ],
            "guid": "EGje*=I9(O",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the queue/stack based optimized version of the <b>topological sort</b>?",
                "The time to perform this algorithm is O(|E| + |V|) if adjacency lists are used. This is apparent when one realizes that the body of the for loop is executed at most once per edge. Computing the indegrees can be done with the following code; this same logic shows that the cost of this computation is O(|E| + |V|), even though there are nested loops.<br><img src=\"paste-403714400f63227de204855151ee2448cb59f6b7.jpg\"><br>The queue operations are done at most once per vertex, and the other initialization steps, including the computation of indegrees, also take time proportional to the size of the graph."
            ],
            "guid": "EBL9Yvg`Q)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>weighted</b>&nbsp;and <b>unweighted path length</b>.",
                "The input is a weighted graph: Associated with each edge (v<sub>i</sub>, v<sub>j</sub>) is a cost c<sub>i</sub>,j to traverse the edge. The cost of a path v<sub>1&nbsp;</sub>v<sub>2</sub> . . . v<sub>N</sub> is:<br><img src=\"paste-297dd7624892ca12ddf629143366e2f8d1c4d8ac.jpg\"><br>This is referred to as the <b>weighted path length</b>. The <b>unweighted path length</b> is merely the number of edges on the path, namely, N - 1."
            ],
            "guid": "C$>gjGCE^@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the <b>Single-Source Shortest-Path Problem</b>.",
                "Given as input a weighted graph, G = (V, E), and a distinguished vertex, s, find the shortest weighted path from s to every other vertex in G."
            ],
            "guid": "k?qYM6{2@T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the problems related to edges of negative cost.",
                "The graph below shows the problems that negative edges can cause. The path from v<sub>5</sub> to v<sub>4</sub> has cost 1, but a shorter path exists by following the loop v<sub>5</sub>, v<sub>4</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, which has cost -5. This path is still not the shortest, because we could stay in the loop arbitrarily long. Thus, the shortest path between these two points is undefined. Similarly, the shortest path from v<sub>1</sub> to v<sub>6</sub> is undefined, because we can get into the same loop. This loop is known as a <b>negative-cost cycle</b>; when one is present in the graph, the shortest paths are not defined. Negative-cost edges are not necessarily bad, as the cycles are, but their presence seems to make the problem harder. For convenience, in the absence of a negative-cost cycle, the shortest path from s to s is zero.<br><img src=\"paste-237767969ba2af0d632b596cf4807653c7968c4a.jpg\">"
            ],
            "guid": "Ol=.gD982^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Examples of problems that require solving the <b>shortest path</b>.",
                "If the vertices represent computers; the edges represent a link between computers; and the costs represent communication costs (phone bill per a megabyte of data), delay costs (number of seconds required to transmit a megabyte), or a combination of these and other factors, then we can use the shortest-path algorithm to find the cheapest way to send electronic news from one computer to a set of other computers.<br>We can model airplane or other mass transit routes by graphs and use a shortestpath algorithm to compute the best route between two points. In this and many practical applications, we might want to find the shortest path from one vertex, s, to only one other vertex, t. Currently there are no algorithms in which finding the path from s to one vertex is any faster (by more than a constant factor) than finding the path from s to all vertices."
            ],
            "guid": "tYkYD/GJ|[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the various <b>shortest-path problems</b>?",
                "First, we will consider the unweighted shortest-path problem which solves it in O(|E|+|V|). Next, to solve the weighted shortest-path problem if we assume that there are no negative edges, the running time for this algorithm is O(|E| log |V|) when implemented&nbsp;with reasonable data structures.<br>If the graph had negative edges, a simple solution unfortunately, has a poor time bound of O(|E| · |V|). Finally, we will solve the weighted problem for the special case of acyclic graphs in linear time."
            ],
            "guid": "CP=D[Y@HuH",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>unweighted shortest-path</b>&nbsp;algorithm.",
                "Figure below shows an unweighted graph, G. Using some vertex, s, which is an input parameter, we would like to find the shortest path from s to all other vertices. We are only interested in the number of edges contained on the path, so there are no weights on the edges. This is clearly a special case of the weighted shortest-path problem, since we could assign all edges a weight of 1.<br><img src=\"paste-9bb862de47a6f79008059b54abc38c38cbfcb6ba.jpg\"><br>For now, suppose we are interested only in the length of the shortest paths, not in the actual paths themselves. Keeping track of the actual paths will turn out to be a matter of simple bookkeeping.&nbsp;<br>Suppose we choose s to be v<sub>3</sub>. Immediately, we can tell that the shortest path from s to v<sub>3</sub> is then a path of length 0. We can mark this information, obtaining the graph in this figure:<br><img src=\"paste-fab6fadd9d2ea5b29bb71abf162638fd40021857.jpg\"><br>Now we can start looking for all vertices that are a distance 1 away from s. These can be found by looking at the vertices that are adjacent to s. If we do this, we see that v<sub>1</sub> and v<sub>6</sub> are one edge from s.<br><img src=\"paste-e47f2bd70eed027f29a4e1a7b9380fabd4c0008f.jpg\"><br>We can now find vertices whose shortest path from s is exactly 2, by finding all the vertices adjacent to v<sub>1</sub> and v<sub>6</sub> (the vertices at distance 1), whose shortest paths are not&nbsp;already known. This search tells us that the shortest path to v<sub>2</sub> and v<sub>4</sub> is 2.<br><img src=\"paste-61808f2da8a62723216eb4bb649e3c6a7eec98ca.jpg\"><br>Finally we can find, by examining vertices adjacent to the recently evaluated v2 and v4, that v5 and v7 have a shortest path of three edges.<br><img src=\"paste-2b6d43694ad888ce4151e2888746d9f2f9952e72.jpg\">"
            ],
            "guid": "A7{0-<r=}%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which strategy is used for finding the <b>unweighted shortest-path</b>?",
                "The strategy for searching a graph is known as <b>breadth-first search</b>. It operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. This is much the same as a level-order traversal for trees.&nbsp;"
            ],
            "guid": "kFv*/YI8_X",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an implementation of solving the <b>unweightest shortest-path</b>&nbsp;problem.",
                "For each vertex, we will keep track of three pieces of information. First, we will keep its distance from s in the entry d<sub>v</sub>. Initially all vertices are unreachable except for s, whose path length is 0. The entry in p<sub>v</sub> is the bookkeeping variable, which will allow us to print the actual paths. The entry known is set to true after a vertex is processed. Initially, all entries are not known, including the start vertex. When a vertex is marked known, we have&nbsp;a guarantee that no cheaper path will ever be found, and so processing for that vertex is essentially complete.<br>The basic algorithm can be described in figure below. It mimics the diagrams by declaring as known the vertices at distance d = 0, then d = 1,<br>then d = 2, and so on, and setting all the adjacent vertices w that still have dw = ∞ to a distance dw = d + 1. By tracing back through the pv variable, the actual path can be printed.\n<br><img src=\"paste-b52bfdd9f0120194a607f158e4f12b9b7456ac10.jpg\">"
            ],
            "guid": "q6XiIDuR0B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of a simple <b>unweighted shortest-path</b>&nbsp;solution?",
                "he running time of the algorithm is O(|V|<sup>2</sup>), because of the doubly nested for loops. An obvious inefficiency is that the outside loop continues until NUM_VERTICES-1, even if all the vertices become known much earlier. Although an extra test could be made to avoid this, it does not affect the worst-case running time, as can be seen by generalizing what happens when the input is the graph below with start vertex v<sub>9</sub>.<br><img src=\"paste-304edf77b40cd69e88519ef4305ec8031f846847.jpg\">"
            ],
            "guid": "n.&zTjVIv9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a refined version of the <b>unweighted shortest-path</b>&nbsp;algorithm.",
                "A very simple but abstract solution is to keep two boxes. Box #1 will have the unknown vertices with d<sub>v</sub> = currDist, and box #2 will have d<sub>v</sub> = currDist + 1. The test to find an appropriate vertex v can be replaced by finding any vertex in box #1. After updating w (inside the innermost if block), we can add w to box #2. After the outermost for loop terminates, box #1 is empty, and box #2 can be transferred to box #1 for the next pass of the for loop.<br>We can refine this idea even further by using just one queue. At the start of the pass, the queue contains only vertices of distance currDist. When we add adjacent vertices of distance currDist + 1, since they enqueue at the rear, we are guaranteed that they will not be processed until after all the vertices of distance currDist have been processed. After the last vertex at distance currDist dequeues and is processed, the queue only contains vertices of distance currDist + 1, so this process perpetuates. We merely need to begin the process by placing the start node on the queue by itself.<br><img src=\"paste-f14bc5e16577f912a4939e6d87d4d1751328634b.jpg\"><br>In the pseudocode, we have assumed that the start vertex, s, is passed as a parameter. Also, it is possible that the queue might empty prematurely, if some vertices are unreachable from the start node. In this case, a distance of INFINITY will be reported for these nodes, which is perfectly reasonable. Finally, the known data member is not used; once a vertex is processed it can never enter the queue again, so the fact that it need not be reprocessed is implicitly marked. Thus, the known data member can be discarded."
            ],
            "guid": "nOaYe,q-;9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the refined implementation of <b>unweighted shortest-path</b>&nbsp;algorithm?",
                "Using the same analysis as was performed for topological sort, we see that the running time is O(|E| + |V|), as long as adjacency lists are used."
            ],
            "guid": "i-l4:]M3.q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the general method to solve the single-source shortest-path problem known as?",
                "It is known as <b>Dijkstra's algorithm</b>. This thirty-year-old solution is a prime example of a greedy algorithm."
            ],
            "guid": "s/!`,+T:Qb",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is <b>Dijkstra's algorithm</b> considered to be a <b>greedy algorithm</b>?",
                "Greedy algorithms generally solve a problem in stages by doing what appears to be the best thing at each stage. For example, to make change in U.S. currency, most people count out the quarters first, then the dimes, nickels, and pennies. This greedy algorithm gives change using the minimum number of coins."
            ],
            "guid": "N@<=Bz>VKM",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the main problem of <b>greedy algorithms</b>?",
                "The main problem with greedy algorithms is that they do not always work. The addition of a 12-cent piece breaks the coin-changing algorithm for returning 15 cents, because the answer it gives (one 12-cent piece and three pennies) is not optimal (one dime and one nickel)."
            ],
            "guid": "Dg`h;,?76l",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Dijkstra's algorithm</b>.",
                "Dijkstra's algorithm proceeds in stages, just like the unweighted shortest-path algorithm. At each stage, Dijkstra's algorithm selects a vertex, v, which has the smallest d<sub>v&nbsp;</sub>among all the unknown vertices and declares that the shortest path from s to v is known. The remainder of a stage consists of updating the values of d<sub>w</sub>. In the unweighted case, we set d<sub>w</sub> = d<sub>v</sub> + 1 if d<sub>w</sub> = ∞. Thus, we essentially lowered the value of dw if vertex v offered a shorter path. If we apply the same logic to the weighted case, then we should set d<sub>w</sub> = d<sub>v</sub> + c<sub>v</sub>,<sub>w</sub> if this new value for dw would be an improvement.<br>Put simply, the algorithm decides whether or not it is a good idea to use v on the path to w. The original cost, d<sub>w</sub>, is the cost without using v; the cost calculated above is the cheapest path using v (and only known vertices).&nbsp;<br><img src=\"paste-21ea0184d8b75a195a2743d8e6aada2dc0271859.jpg\">"
            ],
            "guid": "ze5vEnS?9T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition for <b>Dijkstra's algorithm</b>&nbsp;to work?",
                "A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If any edge has negative cost, the algorithm could produce the wrong answer."
            ],
            "guid": "L8pK%[4!q>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Dijkstra's</b>&nbsp;<b>algorithm</b>&nbsp;when sequential scanning is used?",
                "If we use the obvious algorithm of sequentially scanning the vertices to find the minimum d<sub>v</sub>, each phase will take O(|V|) time to find the minimum, and thus O(|V|<sup>2</sup>) time will be spent finding the minimum over the course of the algorithm. The time for updating d<sub>w</sub> is constant per update, and there is at most one update per edge for a total of O(|E|). Thus, the total running time is O(|E|+|V|<sup>2</sup>) = O(|V|<sup>2</sup>). If the graph is dense, with |E| = Θ(|V|<sup>2</sup>), this algorithm is not only simple but also essentially optimal, since it runs in time linear in the number of edges. If the graph is sparse, with |E| = Θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue."
            ],
            "guid": "Bt2#;;8gQ%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the selection of vertex&nbsp;<i>v </i>correspond to in <b>Dijkstra's algorithm</b>?",
                "It is a deleteMin operation, since once the unknown minimum vertex is found, it is no longer unknown and must be removed from future consideration."
            ],
            "guid": "h:6LAnL{3o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What methods can be used to update <i>w </i>in <b>Dijkstra's algorithm</b>?",
                "One way treats the update as a decreaseKey operation. The time to find the minimum is then O(log |V|), as is the time to perform updates, which amount to decreaseKey operations. This gives a running time of O(|E| log |V| + |V| log |V|) = O(|E| log |V|), an improvement&nbsp;over the previous bound for sparse graphs. Since priority queues do not efficiently support the find operation, the location in the priority queue of each value of d<sub>i</sub> will need to be maintained and updated whenever di changes in the priority queue. If the priority queue is implemented by a binary heap, this will be messy. If a pairing heap is used, the code is not too bad.<br>An alternate method is to insert w and the new value d<sub>w</sub> into the priority queue every time w's distance changes. Thus, there may be more than one representative for each vertex in the priority queue. When the deleteMin operation removes the smallest vertex from the priority queue, it must be checked to make sure that it is not already known and, if&nbsp;it is, it is simply ignored and another deleteMin is performed. Although this method is superior from a software point of view, and is certainly much easier to code, the size of the priority queue could get to be as large as |E|. This does not affect the asymptotic time bounds, since |E| ≤ |V|<sup>2</sup> implies that log |E| ≤ 2 log |V|. Thus, we still get an O(|E| log |V|) algorithm. However, the space requirement does increase, and this could be important in some applications. Moreover, because this method requires |E| deleteMins instead of only |V|, it is likely to be slower in practice&nbsp;"
            ],
            "guid": "Hu|fsmM`Se",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is important to use a priority queue based implementation of <b>Dijkstra's algorithm</b>&nbsp;with computer mail and mass transit commutes?",
                "For the typical problems, the graphs are typically very sparse because most vertices have only a couple of edges."
            ],
            "guid": "uesJ_-8*R3",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Dijkstra's </b>algorithm when Fibonacci heaps are used?",
                "When this is used, the running time is O(|E|+|V| log |V|). Fibonacci heaps have good theoretical time bounds but a fair amount of overhead, so it is not clear whether using Fibonacci heaps is actually better in practice than Dijkstra’s algorithm with binary heaps. To date, there are no meaningful average-case results for this problem."
            ],
            "guid": "d`.WUs:<(2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why doesn't <b>Dijkstra's algorithm </b>work with negative edge cost?",
                "The problem is that once a vertex, u, is declared known, it is possible that from some other unknown vertex, v, there is a path back to u that is very negative. In such a case, taking a path from s to v back to u is better than going from s to u without using v."
            ],
            "guid": "CaO8[7.&Lt",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why adding a constant to each neagtive edge does not solve the case of using <b>Dijkstra's algorithm </b>with negative edges?",
                "A tempting solution is to add a constant to each edge cost, thus removing negative edges, calculate a shortest path on the new graph, and then use that result on the original. The naive implementation of this strategy does not work because paths with many edges become more weighty than paths with few edges."
            ],
            "guid": "K|L?4:13h/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the negative edges problem when calculating shortest-path?",
                "A combination of the weighted and unweighted algorithms will solve the problem, but at the cost of a drastic increase in running time. We forget about the concept of known vertices, since our algorithm needs to be able to change its mind. We begin by placing <i>s</i> on a queue. Then, at each stage, we dequeue a vertex <i>v</i>. We find all vertices w adjacent to v such that d<sub>w</sub> &gt; d<sub>v</sub> + c<sub>v,w</sub>. We update d<sub>w</sub> and p<sub>w</sub>, and place <i>w</i> on a queue if it is not&nbsp;already there. A bit can be set for each vertex to indicate presence in the queue. We repeat the process until the queue is empty.<br><img src=\"paste-9b37081e9fd399acab9728744b6c7f4689c5efb4.jpg\">"
            ],
            "guid": "MJax~8bH1N",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the shortest-path calculation when there are negative edges?",
                "Although the algorithm works if there are no negative-cost cycles, it is no longer true that the code in the inner for loop is executed once per edge. Each vertex can dequeue at most |V| times, so the running time is O(|E| · |V|) if adjacency lists are used. This is quite an increase from Dijkstra’s algorithm, so it is fortunate that, in practice, edge costs are nonnegative. If negative-cost cycles are present, then the algorithm as written will loop indefinitely. By stopping the algorithm after any vertex has dequeued |V| + 1 times, we can guarantee termination."
            ],
            "guid": "RiNYjgR51e",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we improve Dijkstra's algorithm if the graph is known to be <b>acyclic</b>?",
                "We can improve Dijkstra's algorithm by changing the order in which vertices are declared known, otherwise known as the <i>vertex selection&nbsp;rule</i>. The new rule is to select vertices in topological order. The algorithm can be done in one pass, since the selections and updates can take place as the topological sort is being performed.<br>This selection rule works because when a vertex <i>v</i> is selected, its distance, d<sub>v</sub>, can no longer be lowered, since by the topological ordering rule it has no incoming edges emanating from unknown nodes. There is no need for a priority queue with this selection rule; the running time is&nbsp;O(|E| + |V|), since the selection takes constant time.&nbsp;"
            ],
            "guid": "Oo>9s?-{ux",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define&nbsp;<b>critical path analysis.</b>",
                "It is an important use of acyclic graphs. Each node represents an activity that must be performed, along with the time it takes to complete the activity. This graph is thus known as an <b>activity-node graph</b>. The edges represent precedence relationships: An edge (v, w) means that activity v must be completed before activity w may begin. Of course, this implies that the graph must be acyclic. We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers.<br><img src=\"paste-33c4a58f14918968a81a7d1a077a91c3746a3c23.jpg\"><br>This type of a graph could be (and frequently is) used to model construction projects. In this case, there are several important questions which would be of interest to answer. First, what is the earliest completion time for the project? We can see from the graph that 10 time units are required along the path A, C, F, H. Another important question is to determine which activities can be delayed, and by how long, without affecting the minimum completion time. For instance, delaying any of A, C, F, or H would push the completion&nbsp;time past 10 units. On the other hand, activity B is less critical and can be delayed up to two time units without affecting the final completion time."
            ],
            "guid": "Df82WMO*Ob",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we need to perform the calculations of&nbsp;<b>critical path analysis</b>?",
                "To perform these calculations, we convert the activity-node graph to an <b>event-node graph</b>. Each event corresponds to the completion of an activity and all its dependent activities. Events reachable from a node v in the event-node graph may not commence until after the event v is completed. This graph can be constructed automatically or by hand. Dummy edges and nodes may need to be inserted in the case where an activity depends on several others. This is necessary in order to avoid introducing false dependencies (or false lack of dependencies).<br><img src=\"paste-3769af4173894b3f6570ee5d6055100c042181d1.jpg\"><br><img src=\"paste-3b34a29d8f42307be988c1ae4f9d00b888ac8aec.jpg\">"
            ],
            "guid": "q^8.xINu+Z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we calculate the earliest completion time from an <b>event-node graph</b>?",
                "<img src=\"paste-931e71a5385ce9c9266c2573c9037f48e51822f7.jpg\"><br>To find the earliest completion time of the project, we merely need to find the length of the longest path from the first event to the last event. For general graphs, the longest-path problem generally does not make sense, because of the possibility of <b>positive-cost cycles</b>. These are the equivalent of negative-cost cycles in shortest-path problems. If positive-cost cycles are present, we could ask for the longest simple path, but no satisfactory solution is known for this problem. Since the event-node graph is acyclic, we need not worry about cycles. In this case, it is easy to adapt the shortest-path algorithm to compute the earliest&nbsp;completion time for all nodes in the graph. If EC<sub>i</sub> is the earliest completion time for node i, then the applicable rules are:<br><img src=\"paste-152e70520fb2a034923e9d517afcdb5983babe79.jpg\"><br><img src=\"paste-dad1f31867490f93a1aed8be5274a5a74e92b0ee.jpg\">"
            ],
            "guid": "p{)zvS`pt~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we calculate the latest completion time from an&nbsp;<b>event-node graph</b>?",
                "<img src=\"paste-25950947b1193801a715541288690e4ea087d281.jpg\"><br>We can compute the latest time, LC<sub>i</sub>, that each event can finish without affecting the final completion time. The formulas to do this are:<br><img src=\"paste-9f97deafddb34a4d4a7b617ff14735b8dc724dfb.jpg\"><br><img src=\"paste-599e081c9312f6717ea86e336f828e1e0a8b0658.jpg\">"
            ],
            "guid": "uk}&$7$fNJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for calculating earliest- and latest-completion times?",
                "These values can be computed in linear time by maintaining, for each vertex, a list of all adjacent and preceding vertices. The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order.&nbsp;"
            ],
            "guid": "kBw#7JVzwz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>slack time.</b>",
                "The slack time for each edge in the event-node graph represents the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion. It is easy to see that:<br>Slack<sub>(v,w)</sub> = LC<sub>w</sub> - EC<sub>v</sub> - c<sub>v,w<br></sub><img src=\"paste-4d93a61a959ebebddcacddf9c400e114160c7004.jpg\"><br>The figure shows the slack (as the third entry) for each activity in the event-node graph. For each node, the top number is the earliest completion time and the bottom entry is the latest completion time."
            ],
            "guid": "U4_Jg(QqI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>critical path.</b>",
                "Some activities have zero slack. These are critical activities, which must finish on schedule. There is at least one path consisting entirely of zero-slack edges; such a path is a critical path."
            ],
            "guid": "C/$,I3r5`w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <b>world ladders</b>&nbsp;problem.",
                "In a word ladder each word is formed by changing one character in the ladder’s previous word. For instance, we can convert zero to five by a sequence of one-character substitutions as follows: zero hero here hire fire five.<br>This is an unweighted shortest problem in which each word is a vertex, and two vertices have edges (in both directions) between them if they can be converted to each other with a one-character substitution."
            ],
            "guid": "tBH[G(I{7U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the <b>word ladders</b> problem?",
                "We start with a routine that creates a map in which the keys are words, and the values are vectors containing the words that can result from a one-character transformation. As such, this map represents the graph, in adjacency list format, and we only need to write one routine to run the single-source unweighted shortest-path algorithm and a second routine to output the sequence of words, after the&nbsp;single-source shortest-path algorithm has completed.<br>The first routine is findChain, which takes the map representing the adjacency lists and the two words to be connected and returns a map in which the keys are words, and the corresponding value is the word prior to the key on the shortest ladder starting at first. In other words, if the starting word is zero, the value for key five is fire, the value for key fire is hire, the value for key hire is here, and so on. Clearly this provides enough information for the second routine, getChainFromPreviousMap, which can work its way backward.<br>findChain is a direct implementation of unweighted shortest-path algorithm, and for simplicity, it assumes that first is a key in adjacentWords.<br>getChainFromPrevMap uses the prev map and second, which presumably is a key in the map and returns the words used to form the word ladder by working its way backward through prev. This generates the words backward, so the reverse algorithm is used to fix the problem."
            ],
            "guid": "d&+}J;-$GI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we generalize a <b>world ladders</b>&nbsp;solution to include deletion or addition of a character?",
                "To compute the adjacency&nbsp;list requires only a little more effort: every time a representative for word <i>w</i> in group <i>g</i> is computed, we check if the representative is a word in group <i>g</i> - 1. If it is, then the representative is adjacent to <i>w</i> (it is a single-character deletion), and <i>w</i> is adjacent to the representative (it is a single-character addition). It is also possible to assign a cost to a character deletion or insertion (that is higher than a simple substitution), and this yields a weighted shortest-path problem that can be solved with Dijkstra’s algorithm."
            ],
            "guid": "tgiVen(,o2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>network flow problems</b>?",
                "Suppose we are given a directed graph G = (V, E) with edge capacities c<sub>v,w</sub>. These capacities could represent the amount of water that could flow through a pipe or the amount of traffic that could flow on a street between two intersections. We have two vertices: s, which we call the source, and t, which is the sink. Through any edge, (v, w), at most c<sub>v,w</sub> units of “flow” may pass. At any vertex, v, that is not either s or t, the total flow coming in must equal the total flow going out. The maximum-flow problem is to determine the maximum amount of flow that can pass from s to t. As an example, for the graph in figure on the left the maximum flow is 5, as indicated by the graph on the&nbsp;right. Although this example graph is acyclic, this is not a requirement; our (eventual) algorithm will work even if the graph has a cycle.<br><img src=\"paste-b787a56dae16ba09df0eb0a52952d2fbc1df94d3.jpg\">"
            ],
            "guid": "iUGhwU}P*S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we prove the maximum flow in a <b>network flow problem</b>?",
                "Looking at the graph, we see that s has edges of capacities 4 and 2 leaving it, and t has edges of capacities 3 and 3 entering it. So perhaps the maximum flow could be 6 instead of 5. However, the figure shows how we can prove that the maximum flow is 5. We cut the graph into two parts; one part contains s and some other vertices; the other part contains t. Since flow must cross through the cut, the total capacity of all edges (u, v) where u is in s’s partition and v is in t’s partition is a bound on the maximum flow. These edges are (a, c) and (d, t), with total capacity 5, so the maximum flow cannot exceed 5. Any graph has a large number of cuts; the cut with minimum total capacity provides a bound on the maximum flow, and as it turns out (but it is not immediately obvious), the minimum cut capacity is exactly equal to the maximum flow.<br><img src=\"paste-15c1861be7d9e204ebda7c33baf4c216925e9126.jpg\">"
            ],
            "guid": "Eugi!4`L$=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we generate the initial stages of the graphs in a&nbsp;<b>simple maximum-flow algorithm</b>?",
                "A first attempt to solve the problem proceeds in stages. We start with our graph, G, and construct a <b>flow graph</b> G<sub>f</sub>. G<sub>f</sub> tells the flow that has been attained at any stage in the algorithm. Initially all edges in G<sub>f</sub> have no flow, and we hope that when the algorithm terminates, G<sub>f</sub> contains a maximum flow. We also construct a graph, G<sub>r</sub>, called the <b>residual graph</b>. G<sub>r</sub> tells, for each edge, how much more flow can be added. We can calculate this by subtracting the current flow from the capacity for each edge. An edge in G<sub>r</sub> is known as a residual edge.<br>At each stage, we find a path in G<sub>r</sub> from s to t. This path is known as an <b>augmenting path</b>. The minimum edge on this path is the amount of flow that can be added to every edge on the path. We do this by adjusting G<sub>f</sub> and recomputing G<sub>r</sub>. When we find no path from s to t in G<sub>r</sub>, we terminate.&nbsp;<br><img src=\"paste-bec8bb8f3d76d5e94512ff2af9266de35b9bf0f1.jpg\">"
            ],
            "guid": "O(`ho0&[L-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Demonstrate why a greedy algorithm for solving <b>network-flow problems</b>&nbsp;does not work.",
                "There are many paths from s to t in the residual graph. Suppose we select s, b, d, t. Then we can send two units of flow through every edge on this path. We will adopt the convention that once we have filled (saturated) an edge, it is removed from the residual graph.<br><img src=\"paste-ecb98386498b5511cdf857a7bab1e676112fbd5c.jpg\"><br>Next, we might select the path s, a, c, t, which also allows two units of flow. Making the required adjustments gives the graphs in figure.<br><img src=\"paste-d020223182cf852dfaf0421f11d742d1a6ace915.jpg\"><br>The only path left to select is s, a, d, t, which allows one unit of flow. The resulting graphs are shown in figure. The algorithm terminates at this point, because t is unreachable from s. The resulting flow of 5 happens to be the maximum. To see what the problem is, suppose that with our initial graph, we chose the path s, a, d, t. This path allows three units of flow and thus seems to be a good choice. The result of this choice, however, leaves only one path from s to t in the residual graph; it allows one more unit of flow, and thus, our algorithm has&nbsp;failed to find an optimal solution. This is an example of a greedy algorithm that does not work.<br><img src=\"paste-73c77fb390f3082d2b4a28bd834ef1cc47b924db.jpg\">"
            ],
            "guid": "sY$1f3||Cx",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What addition do we need to make to the greedy algorithm to make it work with <b>network flow problems</b>?",
                "In order to make this algorithm work, we need to allow the algorithm to change its mind. To do this, for every edge (v, w) with flow f<sub>v,w</sub> in the flow graph, we will add an edge in the residual graph (w, v) of capacity f<sub>v,w</sub>. In effect, we are allowing the algorithm to undo its decisions by sending flow back in the opposite direction. This is best seen by example. Starting from our original graph and selecting the augmenting path s, a, d, t, we obtain the graphs in figure.<br><img src=\"paste-eddb3c7a16cb90abc3bca2d3c86eb42a17a95330.jpg\"><br>Notice that in the residual graph, there are edges in both directions between a and d. Either one more unit of flow can be pushed from a to d, or up to three units can be pushed back—we can undo flow. Now the algorithm finds the augmenting path s, b, d, a, c, t, of flow 2. By pushing two units of flow from d to a, the algorithm takes two units of flow away from the edge (a, d) and is essentially changing its mind.<br><img src=\"paste-7780261789fd30bc60d1aa75bd7935bbe972ac36.jpg\"><br>There is no augmenting path in this graph, so the algorithm terminates. Note that the same result would occur if the augmenting path s, a, c, t was chosen which allows one unit of flow, because then a subsequent augmenting path could be found."
            ],
            "guid": "E&1q:#a!}[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we prove that the correct implementation of the <b>network flow problem</b>'s algorithm terminates?",
                "It is easy to see that if the algorithm terminates, then it must terminate with a maximum flow. Termination implies that there is no path from s to t in the residual graph. So cut the residual graph, putting the vertices reachable from s on one side and the unreachables (which include t) on the other side. Clearly any edges in the original graph G that cross the cut must be saturated; otherwise, there would be residual flow remaining on one of the edges, which would then imply an edge that crosses the cut (in the wrong disallowed direction) in Gr. But that means that the flow in G is exactly equal to the capacity of a cut in G; hence, we have a maximum flow.<br><img src=\"paste-bf409e64da466adc427ef0c6bbc48625f0fea0bd.jpg\"><br>If the edge costs in the graph are integers, then the algorithm must terminate; each augmentation adds a unit of flow, so we eventually reach the maximum flow, though there&nbsp;is no guarantee that this will be efficient."
            ],
            "guid": "H?T5V0QZLg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why the solution of a <b>network flow problem</b> can be inefficient? And what solutions do we have for that?",
                "If the edge costs in the graph are integers, then the algorithm&nbsp;must&nbsp;terminate; each&nbsp;augmentation adds a unit of flow, so we eventually reach the maximum flow, though there&nbsp;is no guarantee that this will be efficient. In particular, if the capacities are all integers and&nbsp;the maximum flow is&nbsp;f, then, since each augmenting path increases the flow value by at&nbsp;least 1,&nbsp;f&nbsp;stages suffice, and the total running time is&nbsp;O(f&nbsp;·|E|), since an augmenting path can&nbsp;be found in&nbsp;O(|E|) time by an unweighted shortest-path algorithm.<br><img src=\"paste-d8032a3f82abcab07b4450da7cc0aa5bb34bb74a.jpg\"><br>The maximum flow is seen by inspection to be 2,000,000 by sending 1,000,000 down each side. Random augmentations could continually augment along a path that includes the edge connected by a and b. If this were to occur repeatedly, 2,000,000 augmentations would be required, when we could get by with only 2.<br>A simple method to get around this problem is always to choose the augmenting path that allows the largest increase in flow. Finding such a path is similar to solving a weighted shortest-path problem, and a single-line modification to Dijkstra's algorithm will do the trick. If capmax is the maximum edge capacity, then one can show that O(|E| log capmax) augmentations will suffice to find the maximum flow. In this case, since O(|E| log |V|) time is used for each calculation of an augmenting path, a total bound of O(|E|<sup>2</sup> log |V| log capmax) is obtained. If the capacities are all small integers, this reduces to O(|E|<sup>2</sup> log |V|).<br>Another way to choose augmenting paths is always to take the path with the least number of edges, with the plausible expectation that by choosing a path in this manner, it is less likely that a small, flow-restricting edge will turn up on the path. With this rule, each augmenting step computes the shortest unweighted path from s to t in the residual graph, so assume that each vertex in the graph maintains d<sub>v</sub>, representing the shortest-path<br>distance from s to v in the residual graph. Each augmenting step can add new edges into the residual graph, but it is clear that no d<sub>v</sub> can decrease, because an edge is added in the opposite direction of an existing shortest path.<br>Each augmenting step saturates at least one edge. Suppose edge (u, v) is saturated; at that point, u had distance du and v had distance d<sub>v</sub> = d<sub>u</sub> + 1; then (u, v) was removed from&nbsp;the residual graph, and edge (v, u) was added. (u, v) cannot reappear in the residual graph again, unless and until (v, u) appears in a future augmenting path. But if it does, then the distance to u at that point must be d<sub>v</sub> + 1, which would be 2 higher than at the time (u, v) was previously removed.<br>This means that each time (u, v) reappears, u's distance goes up by 2. This means that any edge can reappear at most |V|/2 times. Each augmentation causes some edge to reappear so the number of augmentations is O(|E||V|). Each step takes O(|E|), due to the unweighted shortest-path calculation, yielding an O(|E|2|V|) bound on the running time."
            ],
            "guid": "y3X1~PZ@m;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are some special solutions related to the <b>network flow problems</b>?",
                "Further data structure improvements are possible to this algorithm, and there are several, more complicated, algorithms. A long history of improved bounds has lowered the current best-known bound for this problem to O(|E||V|). There are also a host of very good bounds for special cases. For instance, O(|E||V|<sup>1/2</sup>) time finds a maximum flow in a graph, having the property that all vertices except the source and sink have either a single incoming edge of capacity 1 or a single outgoing edge of capacity 1. These graphs occur in many applications.<br>The analyses required to produce these bounds are rather intricate, and it is not clear how the worst-case results relate to the running times encountered in practice. A related, even more difficult problem is the <b>min-cost flow</b> problem. Each edge has not only a capacity but also a cost per unit of flow. The problem is to find, among all maximum flows, the one flow of minimum cost. Both of these problems are being actively researched.&nbsp;"
            ],
            "guid": "Q8rnqx]d0b",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>minimum spanning tree</b> of an undirected graph.",
                "Informally, a minimum spanning tree of an undirected graph G is a tree formed from graph edges that connects all the vertices of G at lowest total cost. A minimum spanning tree exists if and only if G is connected.<br><img src=\"paste-fcfc8f72cc2daf428023c2718a96fa9e8101440a.jpg\">"
            ],
            "guid": "yp({S`Hd{D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>minimum spanning tree</b>?",
                "The number of edges in the minimum spanning tree is |V| - 1. The minimum spanning tree is a tree because it is acyclic, it is spanning because it covers every vertex, and it is minimum for the obvious reason. If we need to wire a house with a minimum of cable (assuming no other electrical constraints), then a minimum spanning tree problem needs to be solved.<br>For any spanning tree, T, if an edge, e, that is not in T is added, a cycle is created. The removal of any edge on the cycle reinstates the spanning tree property. The cost of the spanning tree is lowered if e has lower cost than the edge that was removed. If, as a spanning tree is created, the edge that is added is the one of minimum cost that avoids creation of a cycle, then the cost of the resulting spanning tree cannot be improved, because any replacement edge would have cost at least as much as an edge already in the spanning tree. This shows that greed works for the minimum spanning tree problem.&nbsp;"
            ],
            "guid": "q.-X!sG+yn",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Prim's algorithm</b>.",
                "One way to compute a minimum spanning tree is to grow the tree in successive stages. In each stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the tree.<br>At any point in the algorithm, we can see that we have a set of vertices that have already been included in the tree; the rest of the vertices have not. The algorithm then finds, at each stage, a new vertex to add to the tree by choosing the edge (u, v) such that the cost of (u, v) is the smallest among all edges where u is in the tree and v is not.<br><img src=\"paste-e5b9d5be1e4aa03baaaab156b48896cb6d36aaa3.jpg\"><br><img src=\"paste-239d9916b6aba59b35f1f5bb4dcdd8afdd6ca6df.jpg\">"
            ],
            "guid": "om2SD^mQ#f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between <b>Prim's algorithm </b>and Daijkstra's?",
                "We can see that Prim's algorithm is essentially identical to Dijkstra's algorithm for shortest paths. As before, for each vertex we keep values d<sub>v</sub> and pv and an indication of whether it is known or unknown. d<sub>v</sub> is the weight of the shortest edge connecting v to a known vertex,&nbsp;and p<sub>v</sub>, as before, is the last vertex to cause a change in d<sub>v</sub>. The rest of the algorithm is exactly the same, with the exception that since the definition of d<sub>v</sub> is different, so is the update rule. For this problem, the update rule is even simpler than before: After a vertex, v, is selected, for each unknown w adjacent to v, d<sub>w</sub> = min(d<sub>w</sub>, c<sub>w,v</sub>)."
            ],
            "guid": "m[)14Xtmi|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What's the running time of&nbsp;<b>Prim's algorithm?</b>",
                "The running time is O(|V|<sup>2</sup>) without heaps, which is optimal for dense graphs, and O(|E| log |V|) using binary heaps, which is good for<br>sparse graphs.&nbsp;"
            ],
            "guid": "K8?O%;3KuR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Kruskal's algorithm</b>.",
                "It's a minimum spanning tree algorithm that uses greedy strategy is to continually select the edges in order of smallest weight and accept an edge if it does not cause a cycle.<br><img src=\"paste-16fe74b80b60d6ad24d282cdccfcc6f34223e0cf.jpg\"><br>Formally, Kruskal’s algorithm maintains a forest—a collection of trees. Initially, there are |V| single-node trees. Adding an edge merges two trees into one. When the algorithm terminates, there is only one tree, and this is the minimum spanning tree.<br>The algorithm terminates when enough edges are accepted. It turns out to be simple to decide whether edge (u, v) should be accepted or rejected. The appropriate data structure is the union/find algorithm. The invariant we will use is that at any point in the process, two vertices belong to the same set if and only if they are connected in the current spanning forest. Thus, each vertex is initially in its own set. If u and v are in the same set, the edge is rejected, because since they are already connected, adding (u, v) would form a cycle. Otherwise, the edge is accepted, and a union is performed on the two sets containing u and v. It is easy to see that this maintains the set invariant, because once the edge (u, v) is added to the spanning forest, if w was connected to u and x was connected to v, then x and w must now be connected, and thus belong in the same set.<br>The edges could be sorted to facilitate the selection, but building a heap in linear time is a much better idea. Then deleteMins give the edges to be tested in order. Typically, only a small fraction of the edges need to be tested before the algorithm can terminate, although it is always possible that all the edges must be tried. For instance, if there was an extra vertex v<sub>8</sub> and edge (v<sub>5</sub>, v<sub>8</sub>) of cost 100, all the edges would have to be examined.<br><img src=\"paste-a2a51799c5613b8b570d6ecbad2c8cb1eab3fe29.jpg\">"
            ],
            "guid": "M03}#|=J#2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Kruskal's algorithm</b>?",
                "The worst-case running time of this algorithm is O(|E| log |E|), which is dominated by the heap operations. Notice that since |E| = O(|V|<sup>2</sup>), this running time is&nbsp;actually O(|E| log |V|). In practice, the algorithm is much faster than this time bound would indicate."
            ],
            "guid": "I%^GFD??pD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between <b>depth-first search </b>and <b>preorder traversal</b>?",
                "Depth-first search is a generalization of preorder traversal. Starting at some vertex, v, we process v and then recursively traverse all vertices adjacent to v. If this process is performed on a tree, then all tree vertices are systematically visited in a total of O(|E|) time, since |E| = Θ(|V|). If we perform this process on an arbitrary graph, we need to be careful to avoid cycles. To do this, when we visit a vertex, v, we mark it visited, since now we have been there, and recursively call depth-first search on all adjacent vertices that are not already marked. We implicitly assume that for undirected graphs every edge (v, w) appears twice in the adjacency lists: once as (v, w) and once as (w, v).<br><img src=\"paste-7d2051b8ac2b10122dc64d304e395c3a17ed7dd9.jpg\"><br>For each vertex, the data member visited is initialized to false. By recursively calling the procedures only on nodes that have not been visited, we guarantee that we do not loop indefinitely. If the graph is undirected and not connected, or directed and not strongly connected, this strategy might fail to visit some nodes. We then search for an unmarked node,&nbsp;apply a depth-first traversal there, and continue this process until there are no unmarked nodes.&nbsp;Because this strategy guarantees that each edge is encountered only once, the total time to perform the traversal is O(|E| + |V|), as long as adjacency lists are used.<br>An efficient way of implementing this is to begin the depth-first search at v<sub>1</sub>. If we need to restart the depth-first search, we examine the sequence v<sub>k</sub>, v<sub>k+1</sub>, . . . for an unmarked vertex, where v<sub>k-1</sub> is the vertex where the last depth-first search was started. This guarantees that throughout the algorithm, only O(|V|) is spent looking for vertices where new depth-first search trees can be started."
            ],
            "guid": "BD}|U5?GSS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>depth-first spanning forest</b>.",
                "We graphically illustrate these steps with a depth-first spanning tree. The root of the tree is A, the first vertex visited. Each edge (v, w) in the graph is present in the tree. If, when we process (v, w), we find that w is unmarked, or if, when we process (w, v), we find that v is unmarked, we indicate this with a tree edge. If, when we process (v, w), we find that w is already marked, and when processing (w, v), we find that v is already marked, we draw a dashed line, which we will call a back edge, to indicate that this “edge” is not really part of the tree. The depth-first search of the graph:<br><img src=\"paste-2f7652a98b89ce1f1e8b4f09806ef411108d5590.jpg\">is shown in figure:<br><img src=\"paste-bb7204bb9d0e4afb17de88c08de0e0d33468ee01.jpg\"><br>The tree will simulate the traversal we performed. A preorder numbering of the tree, using only tree edges, tells us the order in which the vertices were marked. If the graph is not connected, then processing all nodes (and edges) requires several calls to dfs, and each generates a tree. This entire collection is a <b>depth-first spanning forest</b>."
            ],
            "guid": "po>cM<;S=D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>biconnected</b>&nbsp;graphs.",
                "A connected undirected graph is biconnected if there are no vertices whose removal disconnects the rest of the graph.&nbsp;If the nodes&nbsp;are computers and the edges are links, then if any computer goes down, network mail is&nbsp;unaffected, except, of course, at the down computer. Similarly, if a mass transit system is biconnected, users always have an alternate route should some terminal be disrupted."
            ],
            "guid": "qZIxcu6]t>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are <b>articulation points</b>?",
                "If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as articulation points. These nodes are critical in many applications. The graph in figure is not biconnected: C and D are articulation points. The removal of C would disconnect G, and the removal of D would disconnect E and F, from the rest of the graph.<br><img src=\"paste-4df8a6fac3ee8ff605b2587946a6a1fc2eacc76d.jpg\">"
            ],
            "guid": "KvR/fP.4[[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we find <b>articulation points</b>?",
                "We will assume that Vertex contains the data members visited (initialized to false), num, low, and parent. We will also keep a (Graph) class variable called counter, which is initialized to 1, to assign the preorder traversal numbers, num. We also leave out the easily implemented test for the root.<br>As we have already stated, this algorithm can be implemented by performing a preorder traversal to compute Num and then a postorder traversal to compute Low. A third traversal can be used to check which vertices satisfy the articulation point criteria. Performing three traversals, however, would be a waste. The first pass is shown here:<br><img src=\"paste-1a59eee151ca60f3537025c1eec3f001ce7b1b16.jpg\"><br>The second and third passes, which are postorder traversals, can be implemented by the code:<br><img src=\"paste-ad2b64415d5ed9a3da90cacf77ed1e7d8baf2c42.jpg\"><br>The last if statement handles a special case. If w is adjacent to&nbsp;v, then the recursive call to w will find v adjacent to w. This is not a back edge, only an edge that has already been considered and needs to be ignored. Otherwise, the procedure computes the minimum of the various low and num entries, as specified by the algorithm. There is no rule that a traversal must be either preorder or postorder. It is possible to do processing both before and after the recursive calls. The procedure below combines the two routines assignNum and assignLow in a straightforward manner to produce the procedure findArt.&nbsp;<br><img src=\"paste-d806069343b9645943b424d12f6de09a5780e059.jpg\">"
            ],
            "guid": "sN?qeu!_lR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use depth-first search to find <b>articulation points</b>?",
                "Depth-first search provides a linear-time algorithm to find all articulation points in a connected graph. First, starting at any vertex, we perform a depth-first search and number the nodes as they are visited. For each vertex, v, we call this preorder number Num(v). Then, for every vertex, v, in the depth-first search spanning tree, we compute the lowest numbered vertex, which we call Low(v), that is reachable from v by taking zero or more&nbsp;tree edges and then possibly one back edge (in that order). The depth-first search tree in the figure shows the preorder number first, and then the lowest-numbered vertex reachable under the rule described above.<br><img src=\"paste-d1fed0acf4019f8220b18c977e226cb8bc0da754.jpg\"><br>The lowest-numbered vertex reachable by A, B, and C is vertex 1 (A), because they can all take tree edges to D and then one back edge back to A. We can efficiently compute Low by performing a postorder traversal of the depth-first spanning tree. By the definition of Low, Low(v) is the minimum of<br>1. Num(v)<br>2. the lowest Num(w) among all back edges (v, w)<br>3. the lowest Low(w) among all tree edges (v, w)<br><br>The first condition is the option of taking no edges, the second way is to choose no tree edges and a back edge, and the third way is to choose some tree edges and possibly a&nbsp;back edge. This third method is succinctly described with a recursive call. Since we need to evaluate Low for all the children of v before we can evaluate Low(v), this is a postorder traversal. For any edge (v, w), we can tell whether it is a tree edge or a back edge merely by checking Num(v) and Num(w). Thus, it is easy to compute Low(v): We merely scan down v's adjacency list, apply the proper rule, and keep track of the minimum. Doing all the computation takes O(|E| + |V|) time.<br>All that is left to do is to use this information to find articulation points. The root is an articulation point if and only if it has more than one child, because if it has two children, removing the root disconnects nodes in different subtrees, and if it has only one child, removing the root merely disconnects the root. Any other vertex v is an articulation point if and only if v has some child w such that Low(w) ≥ Num(v). Notice that this condition is always satisfied at the root, hence the need for a special test. The if part of the proof is clear when we examine the articulation points that the algorithm determines, namely, C and D. D has a child E, and Low(E) ≥ Num(D), since both are 4. Thus, there is only one way for E to get to any node above D, and that is by going through D. Similarly, C is an articulation point, because Low(G) ≥ Num(C). To prove that this algorithm is correct, one must show that the only if part of the assertion is true (that is, this finds all articulation points). We leave this as an exercise. As a second example, the figure below shows the result of applying this algorithm on the same graph, starting the depth-first search at C.<br><img src=\"paste-317cc996cd21a24e3569047e5b49a1b0e711b2fa.jpg\">"
            ],
            "guid": "B`JB3RXlb8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of <b>Euler circuit problem</b>.",
                "<img src=\"paste-181f50b65b3affa7adc52bb6aff32958c3dd24a2.jpg\"><br>A popular puzzle is to reconstruct these figures using a pen, drawing each line exactly once. The pen may not be lifted from the paper while the drawing is being performed. As an extra challenge, make the pen finish at the same point at which it started.<br>The first figure can be drawn only if the starting point is the lower left- or right-hand corner, and it is not possible to finish at the starting point. The second figure is easily drawn with the finishing point the same as the starting point, but the third figure cannot be drawn at all within the parameters of the puzzle."
            ],
            "guid": "D@daN$:auA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the generic solution to <b>Euler circuit problem</b>.",
                "<img src=\"paste-9e186cf04192205626e56fee41b87802d390944e.jpg\"><br>We can convert this problem to a graph theory problem by assigning a vertex to each intersection. Then the edges can be assigned in the natural manner, as in the figure:<br><img src=\"paste-72fcfd247d80544e3dfa7a5f5eaecd86313786cd.jpg\"><br>After this conversion is performed, we must find a path in the graph that visits every edge exactly once. If we are to solve the “extra challenge,” then we must find a cycle that visits every edge exactly once. This graph problem was solved in 1736 by Euler and marked the beginning of graph theory. The problem is thus commonly referred to as an Euler path (sometimes Euler tour) or Euler circuit problem, depending on the specific problem statement. The Euler tour and Euler circuit problems, though slightly different, have the same basic solution."
            ],
            "guid": "QoXT@SE-R;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the conditions of an Euler circuit that must end on its starting vertex?",
                "The first observation that can be made is that an Euler circuit, which must end on its starting vertex, is possible only if the graph is connected and each vertex has an even degree (number of edges). This is because, on the Euler circuit, a vertex is entered and then left. If any vertex v has odd degree, then eventually we will reach the point where only one edge into v is unvisited, and taking it will strand us at v. If exactly two vertices have odd degree, an Euler tour, which must visit every edge but need not return to its starting vertex, is still possible if we start at one of the odd-degree vertices and finish at the other. If more than two vertices have odd degree, then an Euler tour is not possible.<br>The observations of the preceding paragraph provide us with a necessary condition for the existence of an Euler circuit. It does not, however, tell us that all connected graphs that satisfy this property must have an Euler circuit, nor does it give us guidance on how to find one. It turns out that the necessary condition is also sufficient. That is, any connected graph, all of whose vertices have even degree, must have an Euler circuit. Furthermore, a circuit can be found in linear time."
            ],
            "guid": "reRT7~*O_m",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What solutions work and what doesn't for <b>Euler's circuit</b>?",
                "We can assume that we know that an Euler circuit exists, since we can test the necessary and sufficient condition in linear time. Then the basic algorithm is to perform a depth-first search. There are a surprisingly large number of “obvious” solutions that do not work. Some of these are presented in the exercises.<br>The main problem is that we might visit a portion of the graph and return to the starting point prematurely. If all the edges coming out of the start vertex have been used up, then part of the graph is untraversed. The easiest way to fix this is to find the first vertex on this path that has an untraversed edge and perform another depth-first search. This will give another circuit, which can be spliced into the original. This is continued until all edges have been traversed."
            ],
            "guid": "=lzZs5LA)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What data-structures are used in <b>Euler's circuits</b>?",
                "To make this algorithm efficient, we must use appropriate data structures. To make splicing simple, the path should be maintained as a linked list. To avoid repetitious scanning of adjacency lists, we must maintain, for each adjacency list, a pointer to the last edge scanned. When a path is spliced in, the search for a new vertex from which to perform the next depth-first search must begin at the start of the splice point. This guarantees that&nbsp;he total work performed on the vertex search phase is O(|E|) during the entire life of the algorithm. With the appropriate data structures, the running time of the algorithm is O(|E| + |V|)."
            ],
            "guid": "d5(Ox@64gw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which algorithm is similar to <b>Euler circuit problem</b>&nbsp;in using undirected graphs?",
                "A very similar problem is to find a simple cycle, in an undirected graph, that visits every vertex. This is known as the <b>Hamiltonian cycle problem</b>. Although it seems almost identical to the Euler circuit problem, no efficient algorithm for it is known."
            ],
            "guid": "GYx`.ggah]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What strategy can be used to traverse <b>directed graphs</b>?<br><img src=\"paste-451428472900b303aadce2d023f2e69744d73d1a.jpg\">",
                "Using the same strategy as with undirected graphs, directed graphs can be traversed in linear time, using depth-first search. If the graph is not strongly connected, a depth-first search starting at some node might not visit all nodes. In this case, we repeatedly perform depth-first searches, starting at some unmarked node, until all vertices have been visited.<br>We arbitrarily start the depth-first search at vertex B. This visits vertices B, C, A, D, E, and F. We then restart at some unvisited vertex. Arbitrarily, we start at H, which visits J and I. Finally, we start at G, which is the last vertex that needs to be visited.<br><img src=\"paste-be4e723174991ade35931612a711a714d14ca119.jpg\">"
            ],
            "guid": "bd+3&Zj%zr",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>depth-first spanning tree</b>.",
                "<img src=\"paste-82a2d8672aa3c143217af2b898eeacbcfe7905ee.jpg\"><br>The dashed arrows in the depth-first spanning forest are edges (v, w) for which w was already marked at the time of consideration. In undirected graphs, these are always back edges, but, as we can see, there are three types of edges that do not lead to new vertices. First, there are <b>back edges</b>, such as (A, B) and (I, H). There are also <b>forward edges</b>, such as (C, D) and (C, E), that lead from a tree node to a descendant. Finally, there are <b>cross edges</b>, such as (F, C) and (G, F), which connect two tree nodes that are not directly related. Depthfirst search forests are generally drawn with children and new trees added to the forest from left to right. In a depth-first search of a directed graph drawn in this manner, cross edges always go from right to left. Some algorithms that use depth-first search need to distinguish between the three types of nontree edges. This is easy to check as the depth-first search is being performed, and it is left as an exercise."
            ],
            "guid": "yxw*i(LB?z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the uses for <b>depth-first spanning trees</b>?",
                "One use of depth-first search is to test whether or not a directed graph is acyclic. The rule is that a directed graph is acyclic if and only if it has no back edges. A topological sort can also be used to determine whether a graph is acyclic. Another way to perform topological sorting is to assign the vertices topological numbers N, N - 1, . . . , 1 by postorder traversal of the depth-first spanning forest. As long as the graph is acyclic, this ordering will be consistent."
            ],
            "guid": "zK@Wn&a:#p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you test whether a directed graph is <b>strongly connected</b>?<br><img src=\"paste-3425bbeea1747147eafd63ef514039faf2ba9f19.jpg\">",
                "First, a depth-first search is performed on the input graph G. The vertices of G are numbered by a postorder traversal of the depth-first spanning forest, and then all edges in G are reversed, forming Gr.<br><img src=\"paste-83e52836349ced9e73bcc3a2886148228c1ee7cd.jpg\"><br>The algorithm is completed by performing a depth-first search on Gr, always starting a new depth-first search at the highest-numbered vertex. Thus, we begin the depth-first search of Gr at vertex G, which is numbered 10. This leads nowhere, so the next search is started at H. This call visits I and J. The next call starts at B and visits A, C, and F. The next calls after this are dfs(D) and finally dfs(E). The resulting depth-first spanning forest is shown in figure:<br>Each of the trees (this is easier to see if you completely ignore all nontree edges) in this depth-first spanning forest forms a strongly connected component. Thus, for our example, the strongly connected components are {G}, {H, I, J}, {B, A, C, F}, {D}, and {E}.<br><img src=\"paste-6618833ff5f21027e6a65c66038288137dad1e7e.jpg\">"
            ],
            "guid": "n#,5g*l%&N",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between the running times of graph theory problems?",
                "All these problems have polynomial running times, and with the exception of the network flow problem, the running time is either linear or only slightly more than linear (O(|E| log |E|)). The Euler circuit problem, which finds a path that touches every edge exactly once, is solvable in linear time. The Hamiltonian cycle problem asks for a simple cycle that contains every vertex. No linear algorithm is known for this problem. The single-source unweighted shortest-path problem for directed graphs is also solvable in linear time. No linear-time algorithm is known for the corresponding longestsimple-path problem. The situation for these problem variations is actually much worse than we have described. Not only are no linear algorithms known for these variations, but there are no known algorithms that are guaranteed to run in polynomial time. The best known&nbsp;algorithms for these problems could take exponential time on some inputs.&nbsp;"
            ],
            "guid": "j%G+rW@>]M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the first step in classifying problems?",
                "When classifying problems, the first step is to examine the boundaries. We have already seen that many problems can be solved in linear time. We have also seen some O(log N) running times, but these either assume some preprocessing (such as input already being read or a data structure already being built) or occur on arithmetic examples. For instance, the gcd algorithm, when applied on two numbers M and N, takes O(log N) time. Since the numbers consist of log M and log N bits, respectively, the gcd algorithm is really taking time that is linear in the amount or size of input. Thus, when we measure running time, we will be concerned with the running time as a function of the amount of input. Generally, we cannot expect better than linear running time.<br>At the other end of the spectrum lie some truly hard problems. These problems are so hard that they are impossible. This does not mean the typical exasperated moan, which means that it would take a genius to solve the problem. Just as real numbers are not sufficient to express a solution to x<sup>2</sup> &lt; 0, one can prove that computers cannot solve every problem that happens to come along. These \"impossible\" problems are called undecidable problems.&nbsp;"
            ],
            "guid": "ysyQ_2~mp,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>halting problem</b>.",
                "A particular undecidable problem is the halting problem. Is it possible to have your C++ compiler have an extra feature that not only detects syntax errors but also all infinite loops? This seems like a hard problem, but one might expect that if some very clever programmers spent enough time on it, they could produce this enhancement. The intuitive reason that this problem is undecidable is that such a program might have a hard time checking itself. For this reason, these problems are sometimes called <b>recursively undecidable</b>."
            ],
            "guid": "ruE#c09&J~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define NP class of problems.",
                "NP stands for <b>nondeterministic polynomial-time</b>. A deterministic machine, at each point in time, is executing an instruction. Depending on the instruction, it then goes to some next instruction, which is unique. A nondeterministic machine has a choice of next steps.<br>A simple way to check if a problem is in NP is to phrase the problem as a yes/no question. The problem is in NP if, in polynomial time, we can prove that any “yes” instance is correct. We do not have to worry about “no” instances, since the program always makes the right choice. Thus, for the Hamiltonian cycle problem, a “yes” instance would be any simple circuit in the graph that includes all the vertices. This is in NP, since, given the path, it is a simple matter to check that it is really a Hamiltonian cycle. Appropriately phrased questions, such as “Is there a simple path of length &gt; K?” can also easily be checked and are in NP. Any path that satisfies this property can be checked trivially."
            ],
            "guid": "L&0*lh)3Fw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Are all decidable problems in NP?",
                "Not all decidable problems are in NP. Consider the problem of determining whether a graph does not have a Hamiltonian cycle. To prove that a graph has a Hamiltonian cycle is a relatively simple matter—we just need to exhibit one. Nobody knows how to show, in polynomial time, that a graph does not have a Hamiltonian cycle. It seems that one must enumerate all the cycles and check them one by one. Thus the non–Hamiltonian cycle problem is not known to be in NP."
            ],
            "guid": "yF0/OK%S!1",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>NP complete problems</b>.",
                "Among all the problems known to be in NP, there is a subset, known as the <b>NP-complete problems</b>, which contains the hardest. An NP-complete problem has the property that any problem in NP can be <b>polynomially reduced</b> to it. A problem, P1, can be reduced to P2 as follows: Provide a mapping so that any instance of P1 can be transformed to an instance of P2. Solve P2, and then map the answer back to the original. As an example, numbers are entered into a pocket calculator in decimal. The decimal numbers are converted to binary, and all calculations are performed in binary. Then the final answer is converted back to decimal for display. For P1 to be polynomially reducible to P2, all the work associated with the transformations must be performed in polynomial time."
            ],
            "guid": "LB{P[H(nhd",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why <b>NP-complete</b> <b>problems</b> are the hardest NP problems?",
                "Because a problem that is NP-complete can essentially be used as a subroutine for any problem in NP, with only a polynomial amount of overhead. Thus, if any NP-complete problem has a polynomial-time solution, then every problem in NP must have a polynomial-time solution. This makes the NP-complete problems the hardest of all NP problems. Suppose we have an NP-complete problem, P1. Suppose P2 is known to be in NP.<br>Suppose further that P1 polynomially reduces to P2, so that we can solve P1 by using P2 with only a polynomial time penalty. Since P1 is NP-complete, every problem in NP polynomially reduces to P1. By applying the closure property of polynomials, we see that every problem in NP is polynomially reducible to P2: We reduce the problem to P1 and then reduce P1 to P2. Thus, P2 is NP-complete."
            ],
            "guid": "s-u@Pm[N?)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you describe <b>traveling salesman problem&nbsp;</b>in terms of Hamiltonian cycle problem?",
                "Suppose that we already know that the Hamiltonian cycle problem is NP-complete. The traveling salesman problem is as follows:<br>Given a complete graph, G = (V, E), with edge costs, and an integer K, is there a simple cycle that visits all vertices and has total cost ≤ K?<br>The problem is different from the Hamiltonian cycle problem, because all |V|(|V|-1)/2 edges are present and the graph is weighted. This problem has many important applications. For instance, printed circuit boards need to have holes punched so that chips, resistors, and other electronic components can be placed. This is done mechanically. Punching the hole is a quick operation; the time-consuming step is positioning the hole puncher. The time required for positioning depends on the distance traveled from hole to hole. Since we would like to punch every hole (and then return to the start for the next board), and minimize the total amount of time spent traveling, what we have is a traveling salesman problem.<br>The traveling salesman problem is NP-complete. It is easy to see that a solution can be checked in polynomial time, so it is certainly in NP. To show that it is NP-complete, we polynomially reduce the Hamiltonian cycle problem to it. To do this we construct a new graph, G'. G' has the same vertices as G. For G', each edge (v, w) has a weight of 1 if (v, w) ∈ G, and 2 otherwise. We choose K = |V|.<br>It is easy to verify that G has a Hamiltonian cycle if and only if G' has a traveling salesman tour of total weight |V|.<br><img src=\"paste-8944a416c2aff3101e8de7ba91df5eb68e35932a.jpg\">"
            ],
            "guid": "I`660C`]Kq",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you prove that a problem is&nbsp;<b>NP complete</b>?",
                "To prove that some new problem is NP-complete, it must be shown to be in NP, and then an appropriate NP-complete problem must be transformed into it. Although the transformation to a traveling salesman problem was rather straightforward, most transformations are actually quite involved and require some tricky constructions. Generally, several different NP-complete&nbsp;problems are considered before the problem that actually provides the reduction."
            ],
            "guid": "BGp)+J%!9j",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <b>satisfiability problem</b>.",
                "The satisfiability problem takes as input a Boolean expression and asks whether the expression has an assignment to the variables that gives a value of true.<br>Satisfiability is certainly in NP, since it is easy to evaluate a Boolean expression and check whether the result is true. In 1971, Cook showed that satisfiability was NP-complete by directly proving that all problems that are in NP could be transformed to satisfiability. To do this, he used the one known fact about every problem in NP: Every problem in NP can be solved in polynomial time by a nondeterministic computer. The formal model for a computer is known as a <b>Turing machine</b>. Cook showed how the actions of this machine could be simulated by an extremely complicated and long, but still polynomial, Boolean formula. This Boolean formula would be true if and only if the program which was being run by the Turing machine produced a “yes” answer for its input.<br>Once satisfiability was shown to be NP-complete, a host of new NP-complete problems, including some of the most classic problems, were also shown to be NP-complete."
            ],
            "guid": "I0)y&UdV9W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give some examples of <b>NP complete problems</b>.",
                "In addition to the satisfiability, Hamiltonian circuit, traveling salesman, and longest path problems, some of the more well-known NP complete problems which we have not discussed are bin packing, knapsack, graph coloring, and clique. The list is quite extensive and includes problems from operating systems (scheduling and security), database systems, operations research, logic, and especially graph theory."
            ],
            "guid": "iL&pFV[6v>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>greedy algorithms</b>.",
                "Greedy algorithms work in phases. In each phase, a decision is made that appears to be good, without regard for future consequences. Generally, this means that some <i>local optimum</i> is chosen. This “take what you can get now” strategy is the source of the name for this class of algorithms. When the algorithm terminates, we hope that the local optimum is equal to the <i>global optimum</i>. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. If the absolute best answer is not required, then simple greedy algorithms are sometimes used to generate approximate answers, rather than using the more complicated algorithms generally required to generate an exact answer. Examples include: Dijkstra’s, Prim’s, and Kruskal’s algorithms."
            ],
            "guid": "l(B.He!$%}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give examples where <b>greedy algorithms</b>&nbsp;are not optimum.",
                "There are several real-life examples of greedy algorithms. The most obvious is the coinchanging problem. To make change in U.S. currency, we repeatedly dispense the largest denomination. Thus, to give out seventeen dollars and sixty-one cents in change, we give out a ten-dollar bill, a five-dollar bill, two one-dollar bills, two quarters, one dime, and one penny. By doing this, we are guaranteed to minimize the number of bills and coins. This algorithm does not work in all monetary systems, but fortunately, we can prove that it does work in the American monetary system. Indeed, it works even if two-dollar bills and fifty-cent pieces are allowed.<br>Traffic problems provide an example where making locally optimal choices does not always work. For example, during certain rush hour times in Miami, it is best to stay off the prime streets even if they look empty, because traffic will come to a standstill a mile down the road, and you will be stuck. Even more shocking, it is better in some cases to make a temporary detour in the direction opposite your destination in order to avoid all traffic bottlenecks."
            ],
            "guid": "iKnOnB6t3k",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>simple scheduling problem.</b>",
                "We are given jobs j<sub>1</sub>, j<sub>2</sub>, . . . , j<sub>N</sub>, all with known running times t<sub>1</sub>, t<sub>2</sub>, . . . , t<sub>N</sub>, respectively. We have a single processor. What is the best way to schedule these jobs in order to minimize the average completion time?"
            ],
            "guid": "AyIE%5(*W_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>nonpreemptive scheduling</b>?",
                "Once a job is started, it must run to completion."
            ],
            "guid": "kq(.&!L|WO",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Suppose we have the four jobs and associated running times shown in figure, what is the best schedule to reduce average completion time?<br><img src=\"paste-f3b0d5def21168abf7f0ddefa23018cd8e4bcba7.jpg\">",
                "One possible schedule is shown in:<br><img src=\"paste-294e9e4c42d34e7e10d1e612f669fd6a65f7ddfe.jpg\"><br>Because j<sub>1</sub> finishes in 15 (time units), j<sub>2</sub> in 23, j<sub>3</sub> in 26, and j<sub>4</sub> in 36, the average completion time is 25. A better schedule, which yields a mean completion time of 17.75, is shown in:<br><img src=\"paste-85df9bb180cfc08df6e8d8ab3bcedb63a460dc6b.jpg\"><br>The schedule given in the figure above is arranged by shortest job first."
            ],
            "guid": "E=_ILBC>:?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the optimum way to schedule jobs for optimum average completion time?",
                "The schedule should be arranged by shortest job first. We can show that this will always yield an optimal schedule. Let the jobs in the schedule be j<sub>i1</sub>, j<sub>i2</sub>, . . . , j<sub>iN</sub>. The first job finishes in time t<sub>i1</sub>. The second job finishes after t<sub>i1</sub> + t<sub>i2</sub>, and the third job finishes after t<sub>i1</sub> + t<sub>i2</sub> + t<sub>i3</sub>. From this, we see that the total cost, C, of the schedule is:<br><img src=\"paste-02088ec5250f2c0c58a5a71ea2db2de66c4a419f.jpg\"><br>Notice that in in the second equation, the first sum is independent of the job ordering, so only the second sum affects the total cost. Suppose that in an ordering there exists some x &gt; y such that t<sub>ix</sub> &lt; t<sub>iy</sub>. Then a calculation shows that by swapping j<sub>ix</sub> and j<sub>iy</sub>, the second sum increases, decreasing the total cost. Thus, any schedule of jobs in which the times are not monotonically nondecreasing must be suboptimal. The only schedules left are those in which the jobs are arranged by smallest running time first, breaking ties arbitrarily.<br>This result indicates the reason the operating system scheduler generally gives precedence to shorter jobs."
            ],
            "guid": "MgYd13{B-}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the solution to the job scheduling problem for multi-processors with <i>P </i>= 3.<br><img src=\"paste-e1d7f5dd1983461c2c5b91de785fe0d78dc7bba9.jpg\">",
                "The figure shows an optimal arrangement to minimize mean completion time. Jobs j<sub>1</sub>, j<sub>4</sub>, and j<sub>7</sub> are run on Processor 1. Processor 2 handles j<sub>2</sub>, j<sub>5</sub>, and j<sub>8</sub>, and Processor 3 runs the remaining jobs. The total time to completion is 165, for an average of 165/9 = 18.33.<br><img src=\"paste-3acaddd2a21ffbf8d1e25b6029c2a82d765b1590.jpg\"><br>The algorithm to solve the multiprocessor case is to start jobs in order, cycling through processors. It is not hard to show that no other ordering can do better, although if the number of processors, P, evenly divides the number of jobs, N, there are many optimal orderings. This is obtained by, for each 0 ≤ i &lt; N/P, placing each of the jobs j<sub>iP+1</sub> through j<sub>(i+1)P</sub> on a different processor. The figure below shows a second optimal solution.<br><img src=\"paste-25b4262438225c1a4904f9c40f8e610451902b88.jpg\">"
            ],
            "guid": "wjNpXgS}>)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is the number of processors related to the number of optimal solutions in the<b> job scheduling problem</b>?",
                "Even if&nbsp;P&nbsp;does not divide&nbsp;N&nbsp;exactly, there can still be many optimal solutions, even if&nbsp;all the job times are distinct."
            ],
            "guid": "DGPdHGABt7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we minimize the final completion time for the following jobs?<br><img src=\"paste-b8cc3746e416849bba85b54c74221dfa8a28038d.jpg\">",
                "In our example above, these completion times when optimized for average job processing time is be 40 and 38, respectively. The figure below shows that the minimum final completion time is 34, and this clearly cannot be improved, because every processor is always busy.<br><img src=\"paste-14f64e311620cf3515281c2b84a6b9fca61eb254.jpg\"><br>Although this schedule does not have minimum mean completion time, it has merit in that the completion time of the entire sequence is earlier. If the same user owns all these jobs, then this is the preferable method of scheduling. Although these problems are very similar, this new problem turns out to be NP-complete; it is just another way of phrasing the knapsack or bin packing problems. Thus, minimizing the final completion time is apparently much harder than minimizing the mean completion time."
            ],
            "guid": "rmQq&{@X-p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<b>File compression</b>&nbsp;is considered to be an application of which type of algorithms?",
                "Greedy algorithms."
            ],
            "guid": "JxTMwNsf$,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the general strategy of <b>file compression</b>?",
                "The general strategy is to allow the code length to vary from character to character and to ensure that the frequently occurring characters have short codes. Notice that if all the characters occur with the same frequency, then there are not likely to be any savings."
            ],
            "guid": "c1$t1pFzl^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you calculate the cost of each character when compressing data?<br><img src=\"paste-e7d80da2e19b615613012e61b9e58ef76e6b2746.jpg\">",
                "If character c<sub>i</sub> is at depth d<sub>i</sub> and occurs f<sub>i</sub> times, then the cost of the code is equal to Σ d<sub>i</sub>f<sub>i</sub>.<br><img src=\"paste-c324d8bccbf3e161653077e8b916ed0ce4af91ca.jpg\">"
            ],
            "guid": "ONgz)zu^mv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How an optimum representation of text as a tree looks like?",
                "The tree should be a full tree: All nodes either are leaves or have two children. An optimal code will always have this property, since otherwise, nodes with only one child could move up a level."
            ],
            "guid": "e1Ib`IC>^}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>prefix code</b>.",
                "In prefix code, it does not matter if the character codes are different lengths, as long as no character code is a prefix of another character code.&nbsp;Conversely, if a character is contained in a nonleaf node, it is no longer possible to guarantee that the decoding will be unambiguous."
            ],
            "guid": "II7X3}0JoA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you define <b>Huffman encoding</b> in terms of binary trees?",
                "Our basic problem is to find the full binary tree of minimum total cost, where all characters are contained in the leaves."
            ],
            "guid": "IrWh9bsGAd",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Huffman's algorithm</b>.",
                "We will assume that the number of characters is C.&nbsp;We maintain a forest of trees. The weight of a tree is equal to the sum of the frequencies of its leaves. <i>C</i>-1 times, select the two trees, <i>T</i><sub>1 </sub>and <i>T</i><sub>2</sub>, of smallest weight, breaking ties arbitrarily, and form a new tree with subtrees <i>T</i><sub>1</sub> and <i>T</i><sub>2</sub>. At the beginning of the algorithm, there are <i>C</i> single-node trees—one for each character. At the end of the algorithm there is one tree, and this is the optimal Huffman coding tree.&nbsp;"
            ],
            "guid": "i2Xp?1k5cH",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the solution using <b>Huffman's algorithm&nbsp;</b>for the following inital forest.<br><img src=\"paste-a4c59bc90b61f37366056f4ebee728aabe35175a.jpg\">",
                "The two trees of lowest weight are merged together, creating the forest shown in figure:<br><img src=\"paste-2d0f19178b203a065eee932cf47cf1d25492b039.jpg\"><br>We will name the new root <i>T</i>1, so that future merges can be stated unambiguously. We have made s the left child arbitrarily; any tiebreaking procedure can be used. The total weight of the new tree is just the sum of the weights of the old trees, and can thus be easily computed. It is also a simple matter to create the new tree, since we merely need to get a new node, set the left and right pointers, and record the weight.<br>Now there are six trees, and we again select the two trees of smallest weight. These happen to be <i>T</i>1 and t, which are then merged into a new tree with root <i>T</i>2 and weight 8.&nbsp;This is shown in figure:<br><img src=\"paste-df8aa052447d7ab7c4aa0372948a520c696dff63.jpg\"><br>The third step merges <i>T</i>2 and a, creating <i>T</i>3, with weight 10 + 8 = 18.<br><img src=\"paste-39302a9e4c4b19eff0ea931cf45e5d285f0e52e6.jpg\"><br>After the third merge is completed, the two trees of lowest weight are the single-node trees representing i and the blank space. The figure shows how these trees are merged into the new tree with root T4.<br><img src=\"paste-93664f8e30cb0c31d519b90192ed2274a22eedf8.jpg\"><br>The fifth step is to merge the trees with roots <i>e</i> and <i>T</i>3, since these trees have the two smallest weights. The result of this step is shown in the figure:<br><img src=\"paste-20dd187c1e762f5399fbf9c20782c56555431ef4.jpg\"><br>Finally, the optimal tree is obtained by merging the two remaining trees. Figure 10.19 shows this optimal tree, with root T6.<br><img src=\"paste-d187e1f6869401a55297a83f9931340a0c38c65c.jpg\">"
            ],
            "guid": "N5#FBI2y{g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of the tree generated by <b>Huffman's algorithm</b>?",
                "First, it is not hard to show by contradiction that the tree must be full, since we have already seen how a tree that is not full is improved.&nbsp;Next, we must show that the two least frequent characters α and β must be the two deepest nodes (although other nodes may be as deep). Again, this is easy to show by contradiction, since if either α or β is not a deepest node, then there must be some γ that is (recall that the tree is full). If α is less frequent than γ , then we can improve the cost by swapping them in the tree.<br>We can then argue that the characters in any two nodes at the same depth can be swapped without affecting optimality. This shows that an optimal tree can always be found that contains the two least frequent symbols as siblings; thus, the first step is not a mistake."
            ],
            "guid": "B-;Zt_H6l!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What type of algorithm is <b>Huffman's</b> and why?",
                "The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees."
            ],
            "guid": "Drv#&C%uR5",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Huffman's algorithm</b>?",
                "If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C log C), since there will be one buildHeap, 2C - 2 deleteMins, and C - 2 inserts,&nbsp;on a priority queue that never has more than C elements. A simple implementation of the priority queue, using a list, would give an O(C<sup>2</sup>) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. In such an application, virtually all the running time will be spent on the disk I/O required to read the input file and write out the compressed version."
            ],
            "guid": "oe?dsmM!l5",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the the problems associated with <b>Huffman's algorithm</b> and file I/O?",
                "There are two details that must be considered. First, the encoding information must be transmitted at the start of the compressed file, since otherwise it will be impossible to decode. For small files, the cost of transmitting this table will override any possible savings in compression, and the result will probably be file expansion. Of course, this can be detected and the original left intact. For large files, the size of the table is not significant.<br>The second problem is that, as described, this is a two-pass algorithm. The first pass collects the frequency data, and the second pass does the encoding. This is obviously not a desirable property for a program dealing with large files. Some alternatives are described in the references."
            ],
            "guid": "O{hOqT}F>L",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>bin-packing problem</b>.",
                "We are given N items of sizes s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>N</sub>. All sizes satisfy 0 &lt; s<sub>i</sub> ≤ 1. The problem is to pack these items in the fewest number of bins, given that each bin has unit capacity.<br><img src=\"paste-ffc35ace9eb5a491e00fddd9cb38fe63d1421704.jpg\"><br>There are two versions of the bin packing problem. The first version is <b>online bin packing</b>. In this version, each item must be placed in a bin before the next item can be processed. The second version is the <b>offline bin packing problem</b>. In an offline algorithm, we do not need to do anything until all the input has been read."
            ],
            "guid": "uQ2W$4Gi;b",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum worst case for <b>online bin packing algorithm</b>?",
                "4/3"
            ],
            "guid": "GF9g5osM|U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>next fit</b>&nbsp;algorithm.",
                "Probably the simplest <b>online bin packing algorithm</b> is next fit. When processing any item, we check to see whether it fits in the same bin as the last item. If it does, it is placed there; otherwise, a new bin is created. This algorithm is incredibly simple to implement and runs in linear time."
            ],
            "guid": "sddK2Nt.mq",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the bounds of <b>next fit </b>algorithm?",
                "Let M be the optimal number of bins required to pack a list I of items. Then next fit never uses more than 2<i>M</i> bins. There exist sequences such that next fit uses 2<i>M</i> - 2 bins. Although next fit has a reasonable performance guarantee, it performs poorly in practice, because it creates new bins when it does not need to.&nbsp;\n"
            ],
            "guid": "g@xkb<uEzD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>first fit</b>&nbsp;algorithm.",
                "The first fit strategy is to scan the bins in order and place the new item in the first bin that is large enough to hold it. Thus, a new bin is created only when the results of previous placements have left no other alternative."
            ],
            "guid": "EoPDe}~GIT",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>first fit </b>algorithm?",
                "A simple method of implementing first fit would process each item by scanning down the list of bins sequentially. This would take O(N<sup>2</sup>). It is possible to implement first fit to run in O(N log N)."
            ],
            "guid": "mMhulsVQC~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the bounds of <b>first fit</b>&nbsp;algorithm.",
                "Let M be the optimal number of bins required to pack a list I of items. Then first fit never uses more than 17/10<i>M</i> + 10/7 bins. There exist sequences such that first fit uses&nbsp;17/10(<i>M</i> - 1) bins. When first fit is run on a large number of items with sizes uniformly distributed between 0 and 1, empirical results show that first fit uses roughly 2 percent more bins than optimal. In many cases, this is quite acceptable.&nbsp;"
            ],
            "guid": "fEyqH(4!a[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>best fit</b>.",
                "Instead of placing a new item in the first spot that is found, it is placed in the tightest spot among all bins. Best fit for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8\n<br>:<img src=\"paste-9a8ad76effc5c2481303a0a57fc7d17f6c641910.jpg\"><br>Notice that the item of size 0.3 is placed in B3, where it fits perfectly, instead of B<sub>2</sub>. One might expect that since we are now making a more educated choice of bins, the performance guarantee would improve. This is not the case, because the generic bad cases are the same."
            ],
            "guid": "xHNBqkCD8D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the running time and bounds of <b>best fit</b>?",
                "Best fit is never more than roughly 1.7 times as bad as optimal, and there are inputs for which it (nearly) achieves this bound. Nevertheless, best fit is also simple to code, especially if an O(N log N) algorithm is required, and it does perform better for random inputs."
            ],
            "guid": "J!>!3I=}GJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the major problem with all <b>online bin packing algorithms</b>? And what is the solution?",
                "The major problem with all the online algorithms is that it is hard to pack the large items, especially when they occur late in the input. The natural way around this is to sort the items, placing the largest items first. We can then apply first fit or best fit, yielding the algorithms <b>first fit decreasing</b> and <b>best fit decreasing</b>, respectively."
            ],
            "guid": "BG4&Ib-8[2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the other name of <b>first fit decreasing</b>&nbsp;algorithm?",
                "Since it is possible that the item sizes are not distinct, some authors prefer to call the algorithm <b>first fit nonincreasing</b>."
            ],
            "guid": "N|hKOm5Pq[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Let the N items have (sorted in decreasing order) input sizes s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>N</sub>, respectively, and suppose that the optimal packing is M bins. What is the maximum size of all items that <b>first fit decreasing</b> places in extra bins?",
                "1/3"
            ],
            "guid": "r/IS)Mr)$`",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum number of objects placed in extra bins?",
                "M - 1"
            ],
            "guid": "d=PAr8em]Q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Let <i>M</i> be the optimal number of bins required to pack a list <i>I</i> of items. What is the maximum number of bins used by <b>first fit decreasing</b>&nbsp;algorithm?",
                "(4M + 1)/3"
            ],
            "guid": "c(?7RnEK5v",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the bounds of <b>first bin decreasing</b>&nbsp;algorithm?",
                "The first fit decreasing never uses more than 11/9 M + 6/9 bins. There exist sequences such that first fit decreasing uses 11/9 M + 6/9 bins."
            ],
            "guid": "B)=}[!]a&[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the performance of <b>first bin decreasing</b>&nbsp;algorithm.",
                "In practice, first fit decreasing performs extremely well. If sizes are chosen uniformly over the unit interval, then the expected number of extra bins is Θ(√M). Bin packing is a fine example of how simple greedy heuristics can give good results."
            ],
            "guid": "r;X1FE*qfQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>divide and conquer</b>&nbsp;algorithms.",
                "Divide: Smaller problems are solved recursively (except, of course, base cases).<br>Conquer: The solution to the original problem is then formed from the solutions to the subproblems.<br>Traditionally, routines in which the text contains at least two recursive calls are called divide-and-conquer algorithms, while routines whose text contains only one recursive call&nbsp;are not. We generally insist that the subproblems be disjoint (that is, essentially nonoverlapping)."
            ],
            "guid": "v%V3vJb;VA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give examples of algorithms considered to be <b>divide and conquer</b>.",
                "1- An O(N log N) solution to the maximum subsequence sum problem.<br>2-&nbsp;Linear-time tree traversal strategies. <br>3- Mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively.<br><br>There are recursive algorithms that probably do not classify as divide-and-conquer, but merely reduce to a single simpler case. <br>1- A simple routine to print a number. <br>2- Recursion to perform efficient exponentiation. <br>3- Simple search routines for binary search.<br>4- Simple recursion used to merge leftist heaps.<br>5- Selection that takes linear average time.<br>6- The disjoint set find operation was written recursively.<br>7- routines to recover the shortest path in Dijkstra’s algorithm and other procedures to perform depth-first search in graphs.<br>None of these algorithms are really divide-and-conquer algorithms, because only one recursive call is performed.<br><br>A very bad recursive routine to compute the Fibonacci numbers could be called a divide-and-conquer algorithm, but it is terribly inefficient, because the problem really is not divided at all."
            ],
            "guid": "ptc{vrI]zh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>divide and conquer</b>&nbsp;algorithms?",
                "The solution to the equation T(N) = aT(N/b) + Θ(N<sup>k</sup>), where a ≥ 1 and b &gt; 1, is:<br><img src=\"paste-f8903d874b463936c8d15593be0d2481912eddf6.jpg\"><br>As an example, mergesort has a = b = 2 and k = 1. The second case applies, giving the answer O(N log N). If we solve three problems, each of which is half the original size, and combine the solutions with O(N) additional work, then a = 3, b = 2, and k = 1. Case 1 applies here, giving a bound of O(N<sup>log</sup><sub>2</sub><sup> 3</sup>) = O(N<sup>1.59</sup>). An algorithm that solved three half-sized problems, but required O(N<sup>2</sup>) work to merge the solution, would have an O(N<sup>2</sup>) running time, since the third case would apply."
            ],
            "guid": "gK`p~B84UH",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <b>closest-points problem</b>.",
                "The input to our first problem is a list P of points in a plane. If p1 = (x<sub>1</sub>, y<sub>1</sub>) and p2 = (x<sub>2</sub>, y<sub>2</sub>), then the Euclidean distance between p<sub>1</sub> and p<sub>2</sub> is [(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup>+(y<sub>1</sub>-y<sub>2</sub>)<sup>2</sup>]<sup>1/2</sup>. We are required to find the closest pair of points. It is possible that two points have the same position; in that case, that pair is the closest, with distance zero."
            ],
            "guid": "lu$IcYAkJ7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>closest-path solutions</b>?",
                "If there are N points, then there are N(N - 1)/2 pairs of distances. We can check all of these, obtaining a very short program, but at the expense of an O(N<sup>2</sup>) algorithm. Since this approach is just an exhaustive search, we should expect to do better. Let us assume that the points have been sorted by x coordinate. At worst, this adds O(N log N) to the final time bound. Since we will show an O(N log N) bound for the entire algorithm, this sort is essentially free, from a complexity standpoint."
            ],
            "guid": "sCzjYWlNq~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the solution to <b>closest-points problem.&nbsp;</b>Figure shows a small sample point set, P.\n<br><img src=\"paste-21e80bb03883801b91d88d4ba8e02598f2949c76.jpg\"><b><br></b>",
                "Since the points are sorted by x coordinate, we can draw an imaginary vertical line that partitions the point set into two halves,P<sub>L</sub> and P<sub>R</sub>. This is certainly simple to do. Now we have almost exactly the same situation as we saw in the maximum subsequence sum problem.Either the closest<br>points are both in P<sub>L</sub>, or they are both in P<sub>R</sub>, or one is in P<sub>L</sub> and the other is in P<sub>R</sub>. Let us call these distances d<sub>L</sub>, d<sub>R</sub>, and d<sub>C</sub>. The figure below shows the partition of the point set and these three distances.<br><img src=\"paste-25e5f9cf99df03aa3e03e2000f0d7d0d6ba31b74.jpg\"><br>We can compute d<sub>L</sub> and d<sub>R</sub> recursively. The problem, then, is to compute d<sub>C</sub>. Since we would like an O(N log N) solution, we must be able to compute d<sub>C</sub> with only O(N) additional work. We have already seen that if a procedure consists of two half-sized recursive calls and O(N) additional work, then the total time will be O(N log N).<br>Let δ = min(d<sub>L</sub>, d<sub>R</sub>). The first observation is that we only need to compute d<sub>C</sub> if d<sub>C</sub> improves on δ. If d<sub>C</sub> is such a distance, then the two points that define d<sub>C</sub> must be within δ of the dividing line; we will refer to this area as a <b>strip</b>. As shown in the figure below, this observation limits the number of points that need to be considered (in our case, δ = dR).<br><img src=\"paste-64681e1ce28bb6eab65bd029f571e53bb641ff57.jpg\"><br>There are two strategies that can be tried to compute d<sub>C</sub>. For large point sets that are uniformly distributed, the number of points that are expected to be in the strip is very small. Indeed, it is easy to argue that only O(√N) points are in the strip on average. Thus, we could perform a brute-force calculation on these points in O(N) time.<br>In the worst case, all the points could be in the strip, so this strategy does not always work in linear time. We can improve this algorithm with the following observation: The y coordinates of the two points that define d<sub>C</sub> can differ by at most δ. Otherwise, d<sub>C</sub> &gt; δ. Suppose that the points in the strip are sorted by their y coordinates. Therefore, if p<sub>i</sub> and p<sub>j</sub>’s&nbsp;y coordinates differ by more than δ, then we can proceed to p<sub>i+1</sub>. This extra test has a significant effect on the running time, because for each p<sub>i</sub> only a few points pj are examined before p<sub>i</sub>’s and p<sub>j</sub>’s y coordinates differ by more than δ and force an exit from the inner for loop. The figure below shows, for instance, that for point p<sub>3</sub>, only the two points p<sub>4</sub> and p<sub>5</sub> lie in the strip within δ vertical distance.<br><img src=\"paste-e0bd31f0877619d1b0186625ab0457e00b3025c4.jpg\">"
            ],
            "guid": "Ac4U6-u}OC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst case scenario for the <b>closest-path problem</b>&nbsp;with the following points:<br><img src=\"paste-5ad888ff62f21cf2ab6790635bc67e8109c230c3.jpg\">",
                "In the worst case, for any point p<sub>i</sub>, at most 7 points p<sub>j</sub> are considered. This is because these points must lie either in the δ-by-δ square in the left half of the strip or in the δ-by-δ square in the right half of the strip. On the other hand, all the points in each δ-by-δ square are separated by at least δ. In the worst case, each square contains four points, one at each corner. One of these points is p<sub>i</sub>, leaving at most seven points to be considered. This worstcase situation is shown in the figure below.<br><img src=\"paste-5900a33426e7d0b291955e6a8583e98b2b156398.jpg\"><br>&nbsp;Notice that even though p<sub>L2</sub> and p<sub>R1</sub> have the same coordinates, they could be different points. For the actual analysis, it is only important that<br>the number of points in the λ-by-2λ rectangle be O(1), and this much is certainly clear.<br>Because at most seven points are considered for each p<sub>i</sub>, the time to compute a d<sub>C</sub> that is better than δ is O(N). Thus, we appear to have an O(N log N) solution to the closestpoints problem, based on the two half-sized recursive calls plus the linear extra work to combine the two results. However, we do not quite have an O(N log N) solution yet.<br>The problem is that we have assumed that a list of points sorted by y coordinate is available. If we perform this sort for each recursive call, then we have O(N log N) extra work: This gives an O(N log<sup>2</sup> N) algorithm. This is not all that bad, especially when compared to the brute-force O(N<sup>2</sup>). However, it is not hard to reduce the work for each recursive call to O(N), thus ensuring an O(N log N) algorithm.<br>We will maintain two lists. One is the point list sorted by x coordinate, and the other is the point list sorted by y coordinate. We will call these lists P and Q, respectively. These can be obtained by a preprocessing sorting step at cost O(N log N) and thus does not affect the time bound. PL and QL are the lists passed to the left-half recursive call, and P<sub>R</sub> and Q<sub>R</sub> are the lists passed to the right-half recursive call. We have already seen that P is easily split in the middle. Once the dividing line is known, we step through Q sequentially, placing each element in Q<sub>L</sub> or Q<sub>R</sub> as appropriate. It is easy to see that Q<sub>L</sub> and Q<sub>R</sub> will be automatically sorted by y coordinate. When the recursive calls return, we scan through the Q list and discard all the points whose x coordinates are not within the strip. Then Q contains only points in the strip, and these points are guaranteed to be sorted by their y coordinates.<br>This strategy ensures that the entire algorithm is O(N log N), because only O(N) extra work is performed."
            ],
            "guid": "xA8k5GV_7#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define&nbsp;<strong>the selection problem</strong>.",
                "The selection problem requires us to find the kth smallest element in a collection S of N elements. Of particular interest is the special case of finding the median. This occurs when k = [N/2]."
            ],
            "guid": "ypAZa7i@u",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative to finding the median is better for quick select?",
                "To get a good worst case, the key idea is to use one more level of indirection. Instead of finding the median from a sample of random elements, we will find the median from a <strong>sample of medians</strong>.\n"
            ],
            "guid": "kefGaEsorv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <strong>median-of-median-of-five partitioning</strong>.",
                "The basic pivot selection algorithm is as follows:<br>1. Arrange the N elements into [N/5] groups of five elements, ignoring the (at most four) extra elements.<br>2. Find the median of each group. This gives a list M of [N/5] medians.<br>3. Find the median of M. Return this as the pivot, v.\n<br>It guarantees that each recursive subproblem is at most roughly 70 percent as large as the original. Also the pivot can be computed quickly enough to guarantee an O(N) running time for the entire selection algorithm.\n"
            ],
            "guid": "hq540v5I((",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of quickselect using <strong>median-of-median-of-five partitioning?</strong>",
                "O(N)"
            ],
            "guid": "is)=HK+FLf",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average number of comparisons for finding the median using quick select algorithm?",
                "1.5N"
            ],
            "guid": "oMa}>x4(y]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use&nbsp;<strong>divide and conquer</strong>&nbsp;algorithm to multiply two numbers?",
                "If X = 61,438,521 and Y = 94,736,407, XY = 5,820,464,730,934,047. Let us break X and Y into two halves, consisting of the most significant and least significant digits, respectively. Then X<sub>L</sub> = 6,143, X<sub>R</sub> = 8,521, Y<sub>L</sub> = 9,473, and Y<sub>R</sub> = 6,407. We also have X = X<sub>L</sub>10<sup>4</sup> + X<sub>R</sub> and Y = Y<sub>L</sub>10<sup>4</sup> + Y<sub>R</sub>. It follows that: X<sub>Y</sub> = X<sub>L</sub>Y<sub>L</sub>10<sup>8</sup> + (X<sub>L</sub>Y<sub>R</sub> + X<sub>R</sub>Y<sub>L</sub>)10<sup>4</sup> + X<sub>R</sub>Y<sub>R</sub><br>\nTo achieve a subquadratic algorithm, we must use less than four recursive calls. The key observation is that: X<sub>L</sub>Y<sub>R</sub> + X<sub>R</sub>Y<sub>L</sub> = (X<sub>L</sub> - X<sub>R</sub>)(Y<sub>R</sub> - Y<sub>L</sub>) + X<sub>L</sub>Y<sub>L</sub> + X<sub>R</sub>Y<sub>R</sub><br>\nThus, instead of using two multiplications to compute the coefficient of 10<sup>4</sup>, we can use one multiplication, plus the result of two multiplications that have already been performed. It is easy to see that now the recurrence equation satisfies: T(N) = 3T(N/2) + O(N) and so we obtain T(N) = O(N<sup>log</sup><sub>2</sub><sup> 3</sup>) = O(N<sup>1.59</sup>). To complete the algorithm, we must have a base case, which can be solved without recursion."
            ],
            "guid": "lEXth^R*3@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is basic matrix multiplication done?",
                "The figure below gives a simple O(N<sup>3</sup>) algorithm to compute C = AB, where A, B, and C are N×N matrices. The algorithm follows directly from the definition of matrix multiplication. To compute C<sub>i,j</sub>, we compute the dot product of the ith row in A with the jth column in B.<br><img src=\"paste-ed3d09bf876884254d4494da3ec4186111f43cc2.jpg\">"
            ],
            "guid": "e3N^p$WhY%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you optimize matrix multiplication using <strong>divide and conquer algorithm</strong>?",
                "The basic idea of Strassen’s algorithm is to divide each matrix into four quadrants:<br><img src=\"paste-f34ea77f074496495b2233397c0adcb3941acbcd.jpg\"><br>We could then perform eight N/2-by-N/2 matrix multiplications and four N/2-by-N/2 matrix additions. The matrix additions take O(N<sup>2</sup>) time. If the matrix multiplications are done recursively, then the running time satisfies: T(N) = 8T(N/2) + O(N<sup>2</sup>)\n<br>As we saw with integer multiplication, we must reduce the number of subproblems below 8. Strassen used a strategy similar to the integer multiplication divide-and-conquer algorithm and showed how to use only seven recursive calls by carefully arranging the computations. The seven multiplications are:<br><img src=\"paste-5c3bf6b2ff2e50479fcc10d04eb64343d37fd027.jpg\"><br>Once the multiplications are performed, the final answer can be obtained with eight more additions.\n<br><img src=\"paste-d68194029ed87bccd06fbc90cd1dfb827da3f591.jpg\"><br>It is straightforward to verify that this tricky ordering produces the desired values. The running time now satisfies the recurrence T(N) = 7T(N/2) + O(N<sup>2</sup>). The solution of this recurrence is T(N) = O(N<sup>log</sup><sub>2</sub><sup> 7</sup>) = O(N<sup>2.81</sup>)"
            ],
            "guid": "Q|j5,1*aG,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of Strassen’s algorithm for matrix multiplication?",
                "As usual, there are details to consider, such as the case when N is not a power of 2, but these are basically minor nuisances. Strassen’s algorithm is worse than the straightforward algorithm until N is fairly large. It does not generalize for the case where the matrices are sparse (contain many zero entries), and it does not easily parallelize. When run with floating-point entries, it is less stable numerically than the classic algorithm. Thus, until recently it had only limited applicability. Nevertheless, it has always represented an important theoretical milestone and certainly shows that in&nbsp; computer science, as in many other fields, even though a problem seems to have an intrinsic complexity, nothing is certain until proven.\n"
            ],
            "guid": "jJ^GD#t(tn",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does&nbsp;<strong>dynamic programming&nbsp;</strong>relate to recursive algorithms?",
                "Any recursive mathematical formula could be directly translated to a recursive algorithm, but the underlying reality is that often the compiler will not do justice to the recursive algorithm, and an inefficient program results. When we suspect that this is likely to be the case, we must provide a little more help to the compiler, by rewriting the recursive algorithm as a nonrecursive algorithm that systematically records the answers to the subproblems in a table. One technique that makes use of this approach is known as dynamic programming\n"
            ],
            "guid": "r=JO_0!WXl",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you optimize the recursive version of Fibonacci numbers computation?<br><img src=\"paste-bf907aa44342a1c56f589a89d4fe8db13e3d06a2.jpg\">",
                "The natural recursive program to compute the Fibonacci numbers is very inefficient. The recursive program has a running time, T(N), that satisfies T(N) ≥ T(N -1)+T(N -2). Since T(N) satisfies the same recurrence relation as the Fibonacci numbers and has the same initial conditions, T(N) in fact grows at the same rate as the Fibonacci numbers and is thus exponential.<br>On the other hand, since to compute F<sub>N</sub>, all that is needed is F<sub>N-1</sub> and F<sub>N-2</sub>, we only need to record the two most recently computed Fibonacci&nbsp; &nbsp;numbers. This yields the O(N) algorithm in the figure below:<br><img src=\"paste-d04fa2fdfb325f5ee3b7f3be5d5a1e8ef068ab72.jpg\"><br>The reason that the recursive algorithm is so slow is because of the algorithm used to simulate recursion. To compute F<sub>N</sub>, there is one call to F<sub>N-1</sub> and F<sub>N-2</sub>. However, since F<sub>N-1</sub> recursively makes a call to F<sub>N-2</sub> and F<sub>N-3</sub>, there are actually two separate calls to compute F<sub>N-2</sub>. If one traces out the entire algorithm, then we can see that F<sub>N-3</sub> is computed three times, F<sub>N-4</sub> is computed five times, F<sub>N-5</sub> is computed eight times, and so on. As the figure above shows, the growth of redundant calculations is explosive. If the compiler’s recursion simulation algorithm were able to keep a list of all precomputed values and not make a recursive call for an already solved subproblem, then this exponential explosion would be avoided. This is why the program in the recursive version is so much more efficient.\n"
            ],
            "guid": "eSxJT)dpvV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use&nbsp;<strong>dynamic programming </strong>to optimize the solution of&nbsp;<img src=\"paste-54777724a6625f065aac510ced0683e35c345d6b.jpg\">?<br><img src=\"paste-e14ff42f2792c75fac801a7fd0804c92159c0e71.jpg\">",
                "The recursive calls duplicate work. In this case, the running time T(N) satisfies&nbsp;<img src=\"paste-075234ac7ecc6df052195e6f6145051da686eb0c.jpg\">&nbsp;, because, as shown in the figure, there is one (direct) recursive call of each size from 0 to N - 1, plus O(N) additional work:<br><img src=\"paste-79673edf754f59b2659b9d3787eeefd8035c2185.jpg\"><br>Solving for T(N), we find that it grows exponentially. By using a table, we obtain the program:<br><img src=\"paste-6ed4cb4db01f7ab284fe8745140b45a3ca9d89db.jpg\"><br>This program avoids the redundant recursive calls and runs in O(N<sup>2</sup>)."
            ],
            "guid": "DfG<b!<RG)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best way to perform the&nbsp;three matrix multiplications required to compute ABCD?\n",
                "The calculations show that the best ordering uses roughly one-ninth the number of multiplications as the worst ordering. Thus, it might be worthwhile to perform a few calculations to determine the optimal ordering.<br>The solution of this recurrence is the well-known <b>Catalan numbers</b>, which grow exponentially. Thus, for large N, an exhaustive search through all possible orderings is useless. Nevertheless, this counting argument provides a basis for a solution that is substantially better than exponential. Let ci be the number of columns in matrix Ai for 1 ≤ i ≤ N. Then Ai has ci-1 rows, since otherwise the multiplications are not valid. We will define c0 to be the number of rows in the first matrix, A1. Suppose mLeft, Right is the number of multiplications required to multiply A<sub>Left</sub>A<sub>Left+1</sub> · · · A<sub>Right-1</sub>A<sub>Right</sub>. For consistency, mLeft, Left = 0. Suppose the last multiplication is (A<sub>Left</sub> · · · A<sub>i</sub>)(A<sub>i+1</sub> · · · A<sub>Right</sub>), where Left ≤ i &lt; Right. Then the number of multiplications used is m<sub>Left, i</sub> + m<sub>i+1, Right</sub> + c<sub>Left-1</sub>c<sub>i</sub>c<sub>Right</sub>. These three terms represent the multiplications required to compute (A<sub>Left</sub> · · · A<sub>i</sub>), (A<sub>i+1</sub> · · · A<sub>Right</sub>), and their product, respectively. If we define M<sub>Left, Right</sub> to be the number of multiplications required in an optimal ordering, then, if Left &lt; Right,&nbsp;<img src=\"paste-f3695e94fbe25885492143f801beaae654778368.jpg\">&nbsp;This equation implies that if we have an optimal multiplication arrangement of A<sub>Left</sub> · · · A<sub>Right</sub>, the subproblems A<sub>Left</sub> · · · A<sub>i</sub> and A<sub>i+1</sub> · · · A<sub>Right</sub> cannot be performed suboptimally. This should be clear, since otherwise we could improve the entire result by replacing the suboptimal computation by an optimal computation. The formula translates directly to a recursive program, but, as we have seen in the last section, such a program would be blatantly inefficient. However, since there are only approximately N<sup>2</sup>/2 values of M<sub>Left</sub>, Right that ever need to be computed, it is clear that a table can be used to store these values. Further examination shows that if Right - Left = k, then the only values Mx, y that are needed in the computation of MLeft, Right satisfy y-x &lt; k. This tells us the order in which we need to compute the table.\n<br>This program contains a triply nested loop and is easily seen to run in O(N<sup>3</sup>) time. The references describe a faster algorithm, but since the time to perform the actual matrix multiplication is still likely to be much larger than the time to compute the optimal ordering, this algorithm is still quite practical.\n<br><img src=\"paste-2fc56680fd4b4c0aaac3e0bdf145b2f723e2b889.jpg\">"
            ],
            "guid": "BOA9])#cwz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "We are given a list of words, w<sub>1</sub>, w<sub>2</sub>, . . . , w<sub>N</sub>, and fixed probabilities, p<sub>1</sub>, p<sub>2</sub>, . . . , p<sub>N</sub>, of their occurrence. The problem is to arrange these words in a binary search tree in a way that minimizes the expected total access time. Compare the solutions of this problem.<br><img src=\"paste-be8d0286c07aa572989a628333fd23dc6dfad4f5.jpg\">",
                "In a binary search tree, the number of comparisons needed to access an element at depth d is d + 1, so if w<sub>i</sub> is placed at depth d<sub>i</sub>, then we want to<br>minimize&nbsp;<img src=\"paste-a113a9af94fa6327970b06d8d4e7e359d0eecc5a.jpg\">.\n<br><img src=\"paste-c8562ce7a3bf6240c42b9812136af939327316fa.jpg\"><br>The first tree was formed using a greedy strategy. The word with the highest probability of being accessed was placed at the root. The left and right subtrees were then formed recursively. The second tree is the perfectly balanced search tree. Neither of these trees is optimal, as demonstrated by the existence of the third tree. From this we can see that neither of the obvious solutions works.<br>\n<img src=\"paste-ac8112e16455063cbcb82c1ed13860844cb264ff.jpg\"><br>This is initially surprising, since the problem appears to be very similar to the construction of a Huffman encoding tree, which, as we have already seen, can be solved by a greedy algorithm. Construction of an optimal binary search tree is harder, because the data are not constrained to appear only at the leaves, and also because the tree must satisfy the binary search tree property.\n"
            ],
            "guid": "pKq(8-F38",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an optimum solution for arranging these words in a binary search tree in a way that minimizes the expected total access time:<br><img src=\"paste-0d03228e2309a9229e1a9ca6ca7e4d66fcce916f.jpg\">",
                "A dynamic programming solution follows from two observations. Suppose we are trying to place the (sorted) words w<sub>Left</sub>, w<sub>Left+1</sub>, . . . , w<sub>Right-1</sub>, w<sub>Right</sub> into a binary search tree. Suppose the optimal binary search tree has w<sub>i</sub> as the root, where Left ≤ i ≤ Right. Then the left subtree must contain w<sub>Left</sub>, . . . , w<sub>i-1</sub>, and the right subtree must contain w<sub>i+1</sub>, . . . , w<sub>Right</sub> (by the binary search tree property). Further, both of these subtrees must&nbsp;also be optimal, since otherwise they could be replaced by optimal subtrees, which would give a better solution for w<sub>Left</sub>, . . . , w<sub>Right</sub>. Thus, we can write a formula for the cost C<sub>Left,Right</sub> of an optimal binary search tree.<br><img src=\"paste-a2ca1b44fd69a97b85a5bf550c588099fb7b471e.jpg\"><br>If Left &gt; Right, then the cost of the tree is 0; this is the nullptr case, which we always have for binary search trees. Otherwise, the root costs pi. The left subtree has a cost of C<sub>Left, i-1</sub> relative to its root, and the right subtree has a cost of C<sub>i+1</sub>, Right relative to its root. As the figure above shows, each node in these subtrees is one level deeper from wi than from their respective roots, so we must add&nbsp;<img src=\"paste-3b068a67028d2fd37628996e5aee0d591eec4b9a.jpg\">&nbsp;and&nbsp;<img src=\"paste-d3ceb8c583fd9c24410b603929373107f03f7eee.jpg\">. This gives the formula:<br><img src=\"paste-56109689030757a42b52c88f56fa3c94efbaf258.jpg\"><br>From this equation, it is straightforward to write a program to compute the cost of the optimal binary search tree. As usual, the actual search tree can be maintained by saving the value of i that minimizes C<sub>Left, Right</sub>. The standard recursive routine can be used to print the actual tree.\n<br><img src=\"paste-50be55afb0ae18a879c893973878ce4a93b8b659.jpg\"><br>The precise computation for the optimal binary search tree for a particular subrange, namely, am..if, is shown in the figure below. It is obtained by computing the minimum-cost tree obtained by placing am, and, egg, and if at the root. For instance, when and is placed at the root, the left subtree contains am..am (of cost 0.18, via previous calculation), the right subtree contains egg..if (of cost 0.35), and pam + pand + pegg + pif = 0.68, for a total cost of 1.21.<br><img src=\"paste-1f66f0baa48d9f7241b4420f7119fb7fd85c159d.jpg\"><br>The running time of this algorithm is O(N<sup>3</sup>), because when it is implemented, we obtain a triple loop. An O(N<sup>2</sup>) algorithm for the problem is sketched in the exercises.\n"
            ],
            "guid": "OaA`b>FgqV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a&nbsp;<strong>dynamic programming&nbsp;</strong>solution to <strong>all-pairs shortest path</strong> problem.",
                "The running time of the algorithm is O(|V|<sup>3</sup>), which is not an asymptotic improvement over |V| iterations of Dijkstra's algorithm but could be faster on a very dense graph, because its loops are tighter. The algorithm also performs correctly if there are negative edge costs but no negative-cost cycles; Dijkstra's algorithm fails in this case.<br><img src=\"paste-98e69cc8b78d60a0045ba4e224825afcc05fe8ff.jpg\"><br>When k &gt; 0 we can write a simple formula for D<sub>k,i,j</sub>. The shortest path from v<sub>i</sub> to v<sub>j</sub> that uses only v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>k</sub> as intermediates is the shortest path&nbsp;that either does not use v<sub>k</sub> as an intermediate at all, or consists of the merging of the two paths v<sub>i</sub> → v<sub>k</sub> and v<sub>k</sub> → v<sub>j</sub>, each of which uses only the first k-1 vertices as intermediates. This leads to the formula&nbsp;<img src=\"paste-7e9a2f6bffa7b0d82bc79d08d27fbad73e9d08fa.jpg\"><br>Because the kth stage depends only on the (k - 1)th stage, it appears that only two |V| × |V| matrices need to be maintained. However, using k as an intermediate vertex on a path that starts or finishes with k does not improve the result unless there is a negative cycle. Thus, only one matrix is necessary, because D<sub>k-1,i,k</sub> = D<sub>k,i,k</sub> and D<sub>k-1,k,j</sub> = D<sub>k,k,j</sub>, which implies that none of the terms on the right change values and need to be saved. On a complete graph, where every pair of vertices is connected (in both directions), this algorithm is almost certain to be faster than |V| iterations of Dijkstra's algorithm, because the loops are so tight. Lines 17 to 21 can be executed in parallel, as can lines 26 to 33. Thus, this algorithm seems to be well suited for parallel computation.\n"
            ],
            "guid": "t8s64buH*:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe&nbsp;<strong>dynamic programming&nbsp;</strong>in terms of divide-and-conquer.",
                "Dynamic programming is a powerful algorithm design technique, which provides a starting point for a solution. It is essentially the divide-and-conquer paradigm of solving simpler problems first, with the important difference being that the simpler problems are not a clear division of the original.&nbsp; Because subproblems are repeatedly solved, it is important to record their solutions in a table rather than recompute them. In some cases, the solution can be improved (although it is certainly not always obvious and frequently difficult), and in other cases, the dynamic programming technique is the best approach known. In some sense, if you have seen one dynamic programming problem, you have seen them all."
            ],
            "guid": "On2SO;pI<]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define&nbsp;<strong>randomized algorithms</strong>.",
                "At least once during the algorithm, a random number is used to make a decision. The running time of the algorithm depends not only on the particular input, but also on the random numbers that occur.\n"
            ],
            "guid": "vvv9gblZ7z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst-case running time for&nbsp;<strong>randomized algorithms?</strong>",
                "The worst-case running time of a randomized algorithm is often the same as the worstcase running time of the nonrandomized algorithm. The important difference is that a good randomized algorithm has no bad inputs but only bad random numbers (relative to the particular input)."
            ],
            "guid": "CcjU&ZgY$S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <strong>expected running time</strong>.",
                "We average over all possible random numbers instead of over all possible inputs. Using quicksort with a random pivot gives an O(N log N)-expected-time algorithm. This means that for any input, including already-sorted input, the running time is expected to be O(N log N), based on the statistics of random numbers. An expected running-time bound is somewhat stronger than an average-case bound but, of course, is weaker than the corresponding worst-case bound. On the other hand, as we saw in the selection problem, solutions that obtain the worst-case bound are frequently not as practical as their average-case counterparts. Randomized algorithms usually are.\n"
            ],
            "guid": "q!VP|tHLX|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which hashing algorithms use <strong>randomized algorithms</strong>?",
                "Randomized alogrithms are used implicitly in perfect and universal hashing."
            ],
            "guid": "DedB6[B{c4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between random and pseudorandom numbers?",
                "True randomness is virtually impossible to do on a computer, since these numbers will depend on the algorithm, and thus cannot possibly be random. Generally, it suffices to produce pseudorandom numbers, which are numbers that appear to be random. Random numbers have many known statistical properties; pseudorandom numbers satisfy most of these properties.\n"
            ],
            "guid": "GqjgM&>SHd",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the problem of using system clock to generate <strong>random numbers</strong>?",
                "The clock might record time as an integer that counts the number of seconds since some starting time. We could then use the lowest bit. The problem is that this does not work well if a sequence of random numbers is needed. One second is a long time, and the clock might not change at all while the program is running. Even if the time was recorded in units of microseconds, if the program was running by itself, the sequence of numbers that would be generated would be far from random, since the time between calls to the generator would be essentially identical on every program invocation. We see, then, that what is really needed is a sequence of random numbers.&nbsp;These numbers should appear independent.\n"
            ],
            "guid": "pRHk0c[vCA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the simplest method to <strong>generate random numbers</strong>?",
                "The simplest method to generate random numbers is the <strong>linear congruential generator</strong>, which was first described by Lehmer in 1951. Numbers x1, x2, . . . are generated satisfying x<sub>i+1</sub> = A<sub>xi</sub> mod M. To start the sequence, some value of x<sub>0</sub> must be given. This value is known as the <strong>seed</strong>.&nbsp;\n"
            ],
            "guid": "E}!6qG[4Ws",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of&nbsp;<strong>linear congruential generator</strong>?",
                "If x<sub>0</sub> = 0, then the sequence is far from random, but if <em>A</em> and <em>M</em> are correctly chosen, then any other 1 ≤ x<sub>0</sub> &lt; M is equally valid. If <em>M</em> is prime, then x<em><sub>i</sub></em> is never 0. As an example, if M = 11, A = 7, and x<sub>0</sub> = 1, then the numbers generated are 7, 5, 2, 3, 10, 4, 6, 9, 8, 1, 7, 5, 2, . . .\n<br>Notice that after M - 1 = 10 numbers, the sequence repeats. Thus, this sequence has a period of M - 1, which is as large as possible (by the pigeonhole principle). If <em>M</em> is prime, there are always choices of A that give a full period of <em>M</em> - 1. Some choices of <em>A</em> do not; if <em>A</em> = 5 and x<sub>0</sub> = 1, the sequence has a short period of 5.<br>5, 3, 4, 9, 1, 5, 3, 4, . . .<br>If M is chosen to be a large, 31-bit prime, the period should be significantly large for most applications. Lehmer suggested the use of the 31-bit prime M = 2<sup>31</sup>-1 = 2,147,483,647. For this prime, A = 48,271 is one of the many values that gives a full-period generator. Its use has been well studied and is recommended by experts in the field.\n"
            ],
            "guid": "j:S`F<_X[#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "With random-number generators, tinkering usually means breaking, so one is well advised to stick with this formula until told otherwise. Give an example.",
                "It seems that x<sub>i+1</sub> = (48,271x<sub>i</sub> + 1) mod(2<sup>31</sup> - 1) would somehow be even more random. This illustrates how fragile these generators are. [48,271(179,424,105) + 1] mod(2<sup>31</sup> - 1) = 179,424,105 so if the seed is 179,424,105, the generator gets stuck in a cycle of period 1.\n"
            ],
            "guid": "I]VtDy2|T[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What modification can be done to random number generators to return a number in the range (0,1)?",
                "This can be done by dividing by <em>M</em>. From this, a random number in any closed interval [α, β] can be computed by normalizing."
            ],
            "guid": "wG_/g>8YvI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why doesn't the following random number generator does not work correctly? And what is the solution for that?<br><img src=\"paste-5d9dc6a3f3d6ae9d326f73f69bb61b2dbac00634.jpg\"><br><img src=\"paste-7a6482ee5d1a30d1a8621ebe298a236a77ad2c3a.jpg\"><br><img src=\"paste-bc2ade651cf9e2e92140f54a47d1b805cdd8bfdf.jpg\"><br><img src=\"paste-08514b4472eef13f9db712b0a3985ec93d70ca85.jpg\">",
                "The problem with this class is that the multiplication could overflow; although this is not an error, it affects the result and thus the pseudorandomness. Even though we could use 64-bit long longs, this could slow down the computation. <strong>Schrage</strong> gave a procedure in which all the calculations can be done on a 32-bit machine without overflow. We compute the quotient and remainder of M/A and define these as Q and R, respectively. In our case,<br>Q = 44,488, R = 3,399, and R &lt; Q. We have\n<br><img src=\"paste-9005c9e40282f7670c601bcedc5031c76fb422ad.jpg\"><br>Since&nbsp;<img src=\"paste-5de2d2d0a25f144cdb0e496f3bc3bbf9952a7171.jpg\">, we can replace the leading A<sub>xi</sub> and obtain\n<br>&nbsp;&nbsp;<img src=\"paste-b58b4caffc3fe3cd40d6dac572be2c815cc55d80.jpg\"><br>Since M = AQ + R, it follows that AQ - M = -R. Thus, we obtain\n<br><img src=\"paste-9d47ffbabccb221b576555ace0c86e6899173b4f.jpg\"><br>The term&nbsp;<img src=\"paste-317c1546c1203e39c4968ecd3daf859ec3d46ea1.jpg\">&nbsp;is either 0 or 1, because both terms are integers and their difference lies between 0 and 1. Thus, we have:<br><img src=\"paste-2bf528e8bb77d803fe196947ba0326ee11a0c769.jpg\"><br>A quick check shows that because R &lt; Q, all the remaining terms can be calculated without overflow (this is one of the reasons for choosing A = 48,271). Furthermore, δ(x<sub>i</sub>) = 1 only if the remaining terms evaluate to less than zero. Thus δ(x<sub>i</sub>) does not need to be explicitly computed but can be determined by a simple test. This leads to the revision below:<br><img src=\"paste-dac78ea69bca731bc67ef8a789266d91ff4438f2.jpg\">"
            ],
            "guid": "nZ%Wn),g,o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is wrong with <strong>random number generators</strong> using the function&nbsp;<img src=\"paste-db810e660d1e98de030e8cc725d1cc6f7ac482ec.jpg\">?",
                "One might be tempted to assume that all machines have a good random-number generators in their standard library. Sadly, this is not true. Many libraries have generators based on the function above where B is chosen to match the number of bits in the machine’s integer, and A and C<br>are odd. Unfortunately, these generators always produce values of x<sub>i</sub> that alternate between even and odd—hardly a desirable property. Indeed, the lower k bits cycle with period 2k (at best). Many other random-number generators have much smaller cycles than the optimum solution. These are not suitable for the case where long sequences of random numbers are needed. The UNIX drand48 function uses a generator of this form. However, it uses a 48-bit linear congruential generator and returns only the high 32 bits, thus avoiding the cycling problem in the low-order bits. The constants are A = 25,214,903,917; B = 48; and C = 11."
            ],
            "guid": "xi1**Ht#K&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <strong>random number generation</strong> in C++.",
                "C++11 provides a very general framework for the generation of random numbers. In this framework, the generation of random numbers (i.e., the type of random-number generator used) is separated from the actual distribution used (i.e., whether the numbers are uniformly distributed over integers, a range of reals, or follow other distributions, such as the normal distribution or Poisson distribution).<br>The generators that are provided include linear-congruential generators, with the class template linear_congruential_engine, which allows the specification of the parameters A, C, and M.\n<br>template &lt;typename UnsignedType, UnsignedType A, UnsignedType C, UnsignedType M&gt;<br>class linear congruential engine;\n<br>along with this typedef that yields the random-number generator (the “minimal standard”):\n<br>typedef linear congruential engine&lt;unsigned int, 48271, 0, 2147483647&gt; minstd rand0;\n<br>he library also provides a generator based on a newer algorithm, known as the <strong>Mersenne Twister</strong>, along with a typedef mt19937 that uses its most common parameters, and a third type of random-number generator, known as a “subtract-with-carry” generator.<br>The figure below illustrates how a random-number generator engine can be combined with a distribution (which is a function object) to provide an easy-to-use class for the generation of random numbers.<br>\n<img src=\"paste-81a466385593a92e4271349a11793a84270ba0cd.jpg\">"
            ],
            "guid": "r>wD[1[Rq&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bounds of <strong>skip lists?</strong>",
                "It is a data structure that supports both searching and insertion in O(log N) expected time. This means that the running time for each operation on any input sequence has expected value O(log N), where the expectation is based on the random-number generator. It is possible to add deletion and all the operations that involve ordering and obtain expected time bounds that match the average time bounds of binary search trees.\n"
            ],
            "guid": "t:e~$;k~ti",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we make sure that insertion in&nbsp;<strong>skip lists</strong>&nbsp;is flexible?",
                "The key to making this data structure usable is to relax the structure conditions slightly. We define a level k node to be a node that has k links. As The figure below shows, the <em>i</em>th link in any level <em>k</em> node (k ≥ i) links to the next node with at least i levels. This is an easy property to maintain; however, the figure shows a more restrictive property than this. We thus drop the restriction that the ith link links to the node 2i ahead, and we replace it with the less restrictive condition above.\n<br><img src=\"paste-c97b30b7dc85a753e63f2b99bb00d90e2d72eb96.jpg\">"
            ],
            "guid": "K1DBnsS@TA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert in a&nbsp;<strong>skip list</strong>?",
                "When it comes time to insert a new element, we allocate a new node for it. We must at this point decide what level the node should be. Examining the figure, we find that roughly half the nodes are level 1 nodes, roughly a quarter are level 2, and, in general, approximately 1/2<sup>i</sup> nodes are level <em>i</em>. <br><img src=\"paste-5bd49da3b0b94622f058657d8e94b27c1a7bef61.jpg\"><br>We choose the level of the node randomly, in accordance with this probability distribution. The easiest way to do this is to flip a coin until a head occurs and use the total number of flips as the node level. The figure below shows a typical skip list.\n<br><img src=\"paste-85a40e97bc8ca51553bb0a72d4749f9df36c6114.jpg\">"
            ],
            "guid": "hMF1f8%;BX",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe&nbsp;the<strong> skip list algorithm</strong>.",
                "To perform a search, we start at the highest link at the header. We traverse along this level until we find that the next node is larger than the one we are looking for (or nullptr). When this occurs, we go to the next lower level and continue the strategy. When progress is stopped at level 1, either we<br>are in front of the node we are looking for, or it is not in the list. To perform an insert, we proceed as in a search, and keep track of each point where we switch to a lower level. The new node, whose level is determined randomly, is then spliced into the list.<br><img src=\"paste-bfe1d8aaad5f12dc43d008bdeaf7fa5a812266f4.jpg\">"
            ],
            "guid": "B-B9ku7:T[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do&nbsp;<strong>skip lists</strong>&nbsp;compare to hashmaps and binary trees?",
                "Skip lists are similar to hash tables, in that they require an estimate of the number of elements that will be in the list (so that the number of levels can be determined). If an estimate is not available, we can assume a large number or use a technique similar to rehashing. Experiments have shown that skip lists are as efficient as many balanced search tree implementations and are certainly much simpler to implement in many languages. Skip lists also have efficient concurrent implementations, unlike balanced binary search trees. Hence they are provided in the Java library, though not yet in C++.\n"
            ],
            "guid": "qMC(gUGin$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the problem of&nbsp;<strong>primality testing</strong>.",
                "Some cryptography schemes depend on the difficulty of factoring a large, 600-digit number into two 300-digit primes. In order to implement this scheme, we need a method of generating these two primes. If <em>d</em> is the number of digits in <em>N</em>, the obvious method of testing for the divisibility by odd numbers from 3 to √N requires roughly 1/2√N divisions, which is about 10d/2 and is completely impractical for 600-digit numbers.\n"
            ],
            "guid": "EQjxw7)}Gx",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <strong>Fermat’s Lesser Theorem</strong>?",
                "If <em>P</em> is prime, and 0 &lt; A &lt; P, then A<sup>P-1</sup> ≡ 1 (mod P).<br>For instance, since 67 is prime, 266 ≡ 1 (mod 67). This suggests an algorithm to test whether a number <em>N</em> is prime. Merely check whether 2<sup>N-1</sup> ≡ 1 (mod N). If 2<sup>N-1</sup>&nbsp;<span style=\"font-weight: 700;\">≢</span>&nbsp;1 (mod N), then we can be certain that N is not prime. On the other hand, if<br>the equality holds, then <em>N</em> is probably prime. For instance, the smallest N that satisfies 2<sup>N-1</sup> ≡ 1 (mod N) but is not prime is N<em> </em>= 341.\n"
            ],
            "guid": "jrG?0?i;D?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the problem in <strong>Fermat’s Lesser Theorem</strong>?",
                "This algorithm will occasionally make errors, but the problem is that it will always make the same errors. Put another way, there is a fixed set of <em>N</em> for which it does not work. We can attempt to randomize the algorithm as follows: Pick 1 &lt; A &lt; N - 1 at random. If A<sup>N-1</sup> ≡ 1 (mod N), declare that <em>N</em> is probably prime, otherwise declare that <em>N</em> is definitely not prime. If N = 341, and A = 3, we find that 3340 ≡ 56 (mod 341). Thus, if the algorithm happens to choose A = 3, it will get the correct answer for N = 341.<br>Although this seems to work, there are numbers that fool even this algorithm for most choices of A. One such set of numbers is known as <strong>the Carmichael numbers</strong>. These are not prime but satisfy A<sup>N-1</sup> ≡ 1 (mod N) for all 0 &lt; A &lt; N that are relatively prime to N. The smallest such number is 561. Thus, we need an additional test to improve the chances of not making an error\n"
            ],
            "guid": "j_8-gy$G2y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which alternate theorem can deal with <strong>Fermat’s Lesser Theorem</strong>'s errors?",
                "If P is prime and 0 &lt; X &lt; P, the only solutions to X<sup>2</sup> ≡ 1 (mod P) are X = 1, P - 1.<br>Therefore, if at any point in the computation of A<sup>N-1</sup> (mod N) we discover a violation of this theorem, we can conclude that <em>N</em> is definitely not prime. If we use <em>pow</em>, we see that there will be several opportunities to apply this test. We modify this routine to perform operations mod N, and apply the test of theorem. This strategy is implemented in the pseudocode:\n<br><img src=\"paste-32379a0e5a47060e1c31caf229a0020844e3c8d9.jpg\"><br><img src=\"paste-0ad793a5745951ffe77c7d36161b030e8c5636df.jpg\"><br><img src=\"paste-0893a8cae5fb900700addfbb654b3149881c3e74.jpg\">"
            ],
            "guid": "ER+&(an:TN",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of <strong>randomized algorithms</strong> for <strong>primality testing</strong>?",
                "If witness returns anything but 1, it has proven that <em>N</em> cannot be prime. The proof is nonconstructive, because it gives no method of actually finding the factors. It has been shown that for any (sufficiently large) <em>N</em>, at most (N - 9)/4 values of A fool this algorithm. Thus, if <em>A</em> is chosen at random, and the algorithm answers that <em>N</em> is (probably) prime, then the algorithm is correct at least 75 percent of the time. Suppose witness is run 50 times. The probability that the algorithm is fooled once is at most 1/4. Thus, the probability that 50 independent random trials fool the algorithm is never more than 1/4<sup>50</sup> = 2<sup>-100</sup>. This is actually a very conservative estimate, which holds for only a few choices of <em>N</em>. Even so, one is more likely to see a hardware error than an incorrect claim of primality.\n"
            ],
            "guid": "svny+Q[D;A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do&nbsp;<strong>backtracking algorithms&nbsp;</strong>amount to?",
                "In many cases, a backtracking algorithm amounts to a clever implementation of exhaustive search, with generally unfavorable performance. This is not always the case, however, and even so, in some cases, the savings over a brute-force exhaustive search can be significant. Performance is, of course, relative: an O(N<sup>2</sup>) algorithm for sorting is pretty bad, but an O(N<sup>5</sup>) algorithm for the traveling salesman (or any NP-complete) problem would be a landmark result.\n"
            ],
            "guid": "E{ZZ9{()U@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a non-bruteforce solution to the problem of arranging furniture in a new house.",
                "It is a practical example of a <strong>backtracking algorithm</strong>. There are many possibilities to try, but typically only a few are actually considered. Starting with no arrangement, each piece of furniture is placed in some part of the room. If all the furniture is placed and the owner is happy, then the algorithm terminates. If we reach a point where all subsequent placement of furniture is undesirable, we have to undo the last step and try an alternative. Of course, this might force another undo, and so forth. If we find that we undo all possible first steps, then there is no placement of&nbsp; furniture that is satisfactory. Otherwise, we eventually terminate with a satisfactory arrangement. Notice that although this algorithm is essentially brute force, it does not try all possibilities directly. For instance, arrangements that consider placing the sofa in the kitchen are never tried. Many other bad arrangements are discarded early, because an undesirable subset of the arrangement is detected.\n"
            ],
            "guid": "lS&BP5q+Ti",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <strong>pruning</strong>?",
                "The elimination of a large group of possibilities in one step in&nbsp;<strong>backtracking algorithm</strong>."
            ],
            "guid": "FAJFsnbk<?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <strong>turnpike reconstruction problem.</strong>\n",
                "Suppose we are given <em>N</em> points, p<sub>1</sub>, p<sub>2</sub>, . . . , p<sub>N</sub>, located on the x-axis. x<sub>i</sub> is the x coordinate of p<sub>i</sub>. Let us further assume that x<sub>1</sub> = 0 and the points are given from left to right. These <em>N</em> points determine N(N - 1)/2 (not necessarily unique) distances d<sub>1</sub>, d<sub>2</sub>, . . . , d<sub>N</sub> between every pair of points of the form |x<sub>i</sub> - x<sub>j</sub>| (i ≠&nbsp;j). It is clear that if we are given the set of points, it is easy to construct the set of distances in O(N<sup>2</sup>) time. This set will not be sorted, but if we are willing to settle for an O(N<sup>2</sup> log N) time bound, the distances can be sorted, too. The turnpike reconstruction problem is to reconstruct a point set from the distances. This finds applications in physics and molecular biology."
            ],
            "guid": "E+q7?%dak0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bounds of the&nbsp;<strong>turnpike reconstruction problem?</strong>",
                "Just as factoring seems harder than multiplication, the reconstruction problem seems harder than the construction problem. Nobody has been able to give an algorithm that is guaranteed to work in polynomial time. The algorithm that we will present generally runs in O(N<sup>2</sup> log N) but can take&nbsp; exponential time in the worst case\n"
            ],
            "guid": "Kamp{T=kp*",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Let D be the set of distances, and assume that |D| = M = N(N - 1)/2. As an example, suppose that D = {1, 2, 2, 2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 8, 10}, how do you solve this&nbsp;<strong>turnpike problem</strong>?",
                "<img src=\"paste-5d0cf353be53c6dfa54a59481b3b79ef22d843c6.jpg\"><br>The driving routine, turnpike, is shown in the figure below. It receives the point array x (which need not be initialized) and the distance set D and N. If a solution is discovered, then true will be returned, the answer will be placed in x, and D will be empty. Otherwise, false will be returned, x will be undefined, and the distance set D will be untouched. The routine sets x<sub>1</sub>, x<sub>N-1</sub>, and x<sub>N</sub>, as described above, alters D, and calls the backtracking algorithm place to place the other points. We presume that a check has already been made to ensure that |D| = N(N - 1)/2.\n<br><img src=\"paste-30c17064fb9ec75c9fbb193ea85e93fc811b0aff.jpg\"><br>Like most backtracking algorithms, the most convenient implementation is recursive. We pass the same arguments plus the boundaries Left and Right; x<sub>Left</sub>, . . . , x<sub>Right</sub> are the x coordinates of points that we are trying to place. If D is empty (or Left &gt; Right), then a solution has been found, and we can return. Otherwise, we first try to place x<sub>Right</sub> = D<sub>max</sub>. If all the appropriate distances are present (in the correct quantity), then we&nbsp; tentatively place this point, remove these distances, and try to fill from Left to Right - 1. If the distances are not present, or the attempt to fill Left to Right - 1 fails, then we try setting x<sub>Left</sub> = x<sub>N</sub> - d<sub>max</sub>, using a similar strategy. If this does not work, then there is no solution; otherwise a solution has been found, and this information is eventually passed back to turnpike by the return statement and x array.\n<br><img src=\"paste-d87ed0531c040f53daee8e324a03eae8e8291edd.jpg\">"
            ],
            "guid": "j$DpgfYg|-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the performance bounds of the backtracking based solution to the&nbsp;<strong>turnpike problem?</strong>",
                "<img src=\"paste-19c122db7cc3c29766d032b37c3fc98616632903.jpg\"><br>The analysis of the algorithm involves two factors. Suppose lines 9 to 11 and 18 to 20 are never executed. We can maintain D as a balanced binary search (or splay) tree (this would require a code modification, of course). If we never backtrack, there are at most O(N<sup>2</sup>) operations involving D, such as deletion and the contains implied at lines 4, 12, and 13. This claim is obvious for deletions, since D has O(N<sup>2</sup>) elements and no element is ever reinserted. Each call to place uses at most 2N contains, and since place never backtracks in this analysis, there can be at most 2N<sup>2</sup> contains. Thus, if there is no backtracking, the running time is O(N<sup>2</sup> log N).<br>Of course, backtracking happens, and if it happens repeatedly, then the performance of the algorithm is affected. This can be forced to happen by construction of a pathological case. Experiments have shown that if the points have integer coordinates distributed uniformly and randomly from [0, D<sub>max</sub>], where D<sub>max</sub> = Θ(N<sup>2</sup>), then, almost certainly, at most one backtrack is performed during the entire algorithm.\n"
            ],
            "guid": "ums:lf`?Z>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between general and prelearned strategies in playing tic-tac-toe?",
                "Tic-tac-toe is a draw if both sides play optimally. By performing a careful case-by-case analysis, it is not a difficult matter to construct an algorithm that never loses and always wins when presented the opportunity. This can be done, because certain positions are known traps and can be handled by a lookup table. Other strategies, such as taking the center square when it is available, make the analysis simpler. If this is done, then by using&nbsp;a table we can always choose a move based only on the current position. Of course, this strategy requires the programmer, and not the computer, to do most of the thinking.\n"
            ],
            "guid": "yL_=,/;/$Q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is&nbsp;<strong>minimax strategy?</strong>",
                "The more general strategy is to use an evaluation function to quantify the “goodness” of a position. A position that is a win for a computer might get the value of +1; a draw could get 0; and a position that the computer has lost would get a -1. A position for which this assignment can be determined by examining the board is known as a <strong>terminal position</strong>.\n<br>If a position is not terminal, the value of the position is determined by recursively assuming optimal play by both sides. This is known as a <strong>minimax strategy</strong>, because one player (the human) is trying to minimize the value of the position, while the other player (the computer) is trying to maximize it.\n"
            ],
            "guid": "i*Y`LF~e`,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is&nbsp;<strong>successor position</strong>?",
                "A successor position of P is any position, P<sub>s</sub>, that is reachable from P by playing one move. If the computer is to move when in some position, P, it recursively evaluates the value of all the successor positions. The computer chooses the move with the largest value; this is the value of P. To evaluate any successor position, P<sub>s</sub>, all of P<sub>s</sub>’s successors are recursively evaluated, and the smallest value is chosen. This smallest value represents the most favorable reply for the human player.\n"
            ],
            "guid": "fcO+G{Y+H]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implement a function to return best move in a tic-tac-toe.",
                "<img src=\"paste-80c96d8b7480dd1908cffac23478d54ce0bad605.jpg\"><br>Lines 14 to 18 evaluate immediate wins or draws. If neither of these cases apply, then the position is nonterminal. Recalling that value should contain the maximum of all possible successor positions, line 21 initializes it to the smallest possible value, and the loop in lines 22 to 37 searches for improvements. Each successor position is recursively evaluated in turn by lines 26 to 28. This is recursive, because, as we will see, findHumanMove calls findCompMove. If the human’s response to a move leaves the computer with a more favorable position than that obtained with the previously best computer move, then the value and bestMove are updated. The figure below shows the function for the human’s move selection. The logic is virtually identical, except that the human player chooses the move that leads to the lowestvalued position. Indeed, it is not difficult to combine these two procedures into one by passing an extra variable, which indicates whose turn it is to move. This does make the code somewhat less readable, so we have stayed with separate routines.<br>\n<img src=\"paste-a3ee0b560b26281eb3cd623b5cb7addc6eb7f1d0.jpg\">"
            ],
            "guid": "zkxM*/!CYX",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time-bounds of tic-tac-toe's solution?",
                "The most costly computation is the case where the computer is asked to pick the opening move. Since at this stage the game is a forced draw, the computer selects square 1.&nbsp;A total of 97,162 positions were examined, and the calculation took a few seconds. No attempt was made to optimize the code. When the computer moves second, the number of positions examined is 5,185 if the human selects the center square, 9,761 when a corner square is selected, and 13,233 when a noncorner edge square is selected."
            ],
            "guid": "k/<vx&9{J#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How are next moves evaluated in complex games like chess and and checkers?",
                "For more complex games, such as checkers and chess, it is obviously infeasible to search all the way to the terminal nodes. In this case, we have to stop the search after a certain depth of recursion is reached. The nodes where the recursion is stopped become terminal nodes. These terminal nodes are evaluated with a function that estimates the value of the position. For instance, in a chess program, the evaluation function measures such variables as the relative amount and strength of pieces and positional factors. The evaluation function is crucial for success, because the&nbsp; computer’s move selection is based on maximizing this function. The best computer chess programs have surprisingly sophisticated evaluation functions.<br>Nevertheless, for computer chess, the single most important factor seems to be number of moves of look-ahead the program is capable of. This is sometimes known as <strong>ply</strong>; it is equal to the depth of the recursion. To implement this, an extra parameter is given to the search routines.<br>The basic method to increase the look-ahead factor in game programs is to come up with methods that evaluate fewer nodes without losing any information. One method which we have already seen is to use a table to keep track of all positions that have been evaluated. For instance, in the course of searching for the first move, the program will examine the positions and then if the values of the positions are saved, the second occurrence of a position need not be recomputed; it essentially becomes a terminal position. The data structure that records this is known as a transposition table; it is almost always implemented by hashing. In many cases, this can save considerable computation. For instance, in a chess endgame, where there are relatively few pieces, the time savings can allow a search to go several levels deeper\n"
            ],
            "guid": "NjyuSzV;FP",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is α<b>–β pruning</b>?",
                "It is one of the most significant improvements to a tic-tac-toe algorithm.<br><img src=\"paste-e1488de42eba716e09dfefd66fd4e554c7882d3a.jpg\"><br>The figure above shows the trace of the recursive calls used to evaluate some hypothetical position in a hypothetical game. This is commonly referred to as a game tree. The value of the game tree is 44.<br>The figure below shows the evaluation of the same game tree with several (but not all possible) unevaluated nodes. Almost half of the terminal nodes have not been checked. We show that evaluating them would not change the value at the root.<br>\n<img src=\"paste-e46c1e291b6f3f2b6e43be4384aa614df863c6df.jpg\"><br>First, consider node D. The figure below shows the information that has been gathered when it is time to evaluate D. At this point, we are still in findHumanMove and are contemplating a call to findCompMove on D. However, we already know that findHumanMove will return at most 40, since it is a min node. On the other hand, its max node parent has already found a sequence that guarantees 44. Nothing that D does can possibly increase this value. Therefore, D does not need to be evaluated. This pruning of the tree is known as α pruning. An identical situation occurs at node B. To implement α pruning, findCompMove passes its tentative maximum (α) to findHumanMove. If the tentative minimum of findHumanMove falls below this value, then findHumanMove returns immediately.\n<br><img src=\"paste-66c015b7f610336ea1b4aa4bf7d00c4eb89e29a3.jpg\"><br>A similar thing happens at nodes A and C. This time, we are in the middle of a findCompMove and are about to make a call to findHumanMove to evaluate C. The figure below shows the situation that is encountered at node C. However, the findHumanMove, at the min level, which has called findCompMove, has already determined that it can force a value of at most 44 (recall that low values are good for the human side). Since findCompMove has a tentative maximum of 68, nothing that C does will affect the result at the min level. Therefore, C should not be evaluated. This type of pruning is known as β pruning; it is the symmetric version of α pruning. When both techniques are combined, we have α–β pruning.<br>\n<img src=\"paste-c2f4da9e651d147ec2be2e05d76aa28e849ec3e0.jpg\">"
            ],
            "guid": "oTvt:(I7t^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>α–β pruning</b>?",
                "<img src=\"paste-56ef98dd0d05d922142f2b8c483028feb8358e6f.jpg\"><br>Implementing α–β pruning requires surprisingly little code. The figure above shows half of the α–β pruning scheme (minus type declarations).<br>To take full advantage of α–β pruning, game programs usually try to apply the evaluation function to nonterminal nodes in an attempt to place the best moves early in the search. The result is even more pruning than one would expect from a random ordering of the nodes. Other techniques, such as searching deeper in more active lines of play, are also employed.\n"
            ],
            "guid": "Nsa%HokSe%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time-bounds of <b>α–β pruning</b>?",
                "In practice, α–β pruning limits the searching to only O(√N) nodes, where N is the size of the full game tree. This is a huge savings and means that searches using α–β pruning can go twice as deep as compared to an unpruned tree. Our tic-tac-toe example is not ideal, because there are so many identical values, but even so, the initial search of 97,162 nodes is reduced to 4,493 nodes.\n"
            ],
            "guid": "E[y_VU(CQf",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>amortized time bound</b> in terms of trees.",
                "As an example, we have seen that AVL trees support the standard tree operations in O(log N) worst-case time per operation. AVL trees are somewhat complicated to implement, not only because there are a host of cases, but also because height balance information must be maintained and updated correctly. The reason that AVL trees are used is that a sequence of Θ(N) operations on an unbalanced search tree could require Θ(N<sup>2</sup>)&nbsp;time, which would be expensive. For search trees, the O(N) worst-case running time of an operation is not the real problem. The major problem is that this could happen repeatedly. Splay trees offer a pleasant alternative. Although any operation can still require Θ(N) time, this degenerate behavior cannot occur repeatedly, and we can prove that any sequence of M operations takes O(M log N) worst-case time (total). Thus, in the long run this data structure behaves as though each operation takes O(log N). We call this an <b>amortized time bound</b>.\n"
            ],
            "guid": "C{)Et}lLHp",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do <b>amortized bounds&nbsp;</b>compare to worst- and average-case bounds?",
                "Amortized bounds are weaker than the corresponding worst-case bounds, because there is no guarantee for any single operation. Since this is generally not important, we are willing to sacrifice the bound on a single operation, if we can retain the same bound for the sequence of operations and at the same time simplify the data structure. Amortized bounds are stronger than the equivalent average-case bound. For instance, binary search<br>trees have O(log N) average time per operation, but it is still possible for a sequence of M operations to take O(MN) time.\n"
            ],
            "guid": "kK1M%/{p+.",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example demonstrating the&nbsp;<b>potential</b>&nbsp;variable in&nbsp;<b>amortized analysis</b>.",
                "Consider the following puzzle: Two kittens are placed on opposite ends of a football field, 100 yards apart. They walk toward each other at the speed of 10 yards per minute. At the same time, their mother is at one end of the field. She can run at 100 yards per minute. The mother runs from one kitten to the other, making turns with no loss of speed, until the kittens (and thus the mother) meet at midfield. How far does the mother run?<br>It is not hard to solve this puzzle with a brute-force calculation, but one expects that this calculation will involve computing the sum of an infinite geometric series. Although this straightforward calculation will lead to an answer, it turns out that a much simpler solution can be arrived at by introducing an extra variable, namely, time.<br>Because the kittens are 100 yards apart and approach each other at a combined velocity of 20 yards per minute, it takes them five minutes to get to midfield. Since the mother runs 100 yards per minute, her total is 500 yards. This puzzle illustrates the point that sometimes it is easier to solve a problem indirectly than directly.&nbsp;\n"
            ],
            "guid": "lVM.*$32T[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>rank</b> of a node in a <b>binomial tree</b>?",
                "It is equal to the number of children; in particular,&nbsp;the rank of the root of B<sub>k</sub> is k."
            ],
            "guid": "Icw`@iSMY;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Prove that a binomial queue of N elements can be built by N successive insertions in O(N) time.\n",
                "Consider the result of an insertion. If there is no B<sub>0</sub> tree present at the time of the insertion, then the insertion costs a total of one unit, using the same accounting as above. The result of the insertion is that there is now a B<sub>0</sub> tree, and thus we have added one tree to the forest of binomial trees. If there is a B<sub>0</sub> tree but no B<sub>1</sub> tree, then the insertion costs two units. The new forest will have a B<sub>1</sub> tree but will no longer have a B<sub>0</sub> tree, so the number of trees in the forest is unchanged. An insertion that costs three units will create a B<sub>2</sub> tree but destroy a B<sub>0</sub> and B<sub>1</sub> tree, yielding a net loss of one tree in the forest. In fact, it is easy to see that, in general, an insertion that costs c units results in a net increase of 2 - c trees in the forest, because a B<sub>c-1</sub> tree is created but all B<sub>i</sub> trees 0 ≤ i &lt; c - 1 are removed. Thus, expensive insertions remove trees, while cheap insertions create trees. Let C<sub>i</sub> be the cost of the ith insertion. Let T<sub>i</sub> be the number of trees after the <i>i</i>th insertion. T<sub>0</sub> = 0 is the number of trees initially. Then we have the invariant C<sub>i</sub> + (T<sub>i</sub> - T<sub>i-1</sub>) = 2, we then have:<br><img src=\"paste-21b7167266cc571cb88d2ceefa50702f2aaa773b.jpg\"><br>If we add all these equations, most of the Ti terms cancel, leaving&nbsp;<img src=\"paste-aeba2d19d5544744a5cfb431f26f8324684e9d05.jpg\">&nbsp;or equivalently,&nbsp;<img src=\"paste-655e45ce746fe051a9f9ef914863fec0febf43ec.jpg\">.<br>Recall that T<sub>0</sub> = 0, and T<sub>N</sub>, the number of trees after the N insertions, is certainly not negative, so (T<sub>N</sub> - T<sub>0</sub>) is not negative. Thus&nbsp;<img src=\"paste-78370bbc47ca102d68a6de1ebbbaff5ebf6031d2.jpg\">&nbsp;which proves the claim. During the buildBinomialQueue routine, each insertion had a worstcase time of O(log N), but since the entire routine used at most 2N units of time, the insertions behaved as though each used no more than two units each.\n"
            ],
            "guid": "Xg[w^PB41",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define&nbsp;<b>potential</b>&nbsp;in <b>amortized analysis</b>.",
                "The state of the data structure at any time is given by a function known as the potential. The potential function is not maintained by the program, but rather is an accounting device that will help with the analysis. When operations take less time than we have allocated for them, the unused time is “saved” in the form of a higher potential. When operations occur that exceed the allotted time, then the excess time is accounted for by a decrease in potential. One may view the potential as representing a savings account. If an operation uses less than its allotted time, the difference is saved for use later on by more expensive operations. The figure below shows the cumulative running time used by <b>buildBinomialQueue</b> over a sequence of insertions. Observe that the running time never exceeds 2N and that the potential in the binomial queue after any insertion measures the amount of savings.<br>\n<img src=\"paste-fb539a1e26aa7c27a7dd7a676278d3afbe88cb72.jpg\">"
            ],
            "guid": "q[|H<)dK^E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you calculate&nbsp;<b>amortized time</b>&nbsp;in terms of the&nbsp;<b>potential</b>?",
                "Once a potential function is chosen, we write the main equation:\n<br>T<sub>actual</sub> + ΔPotential = T<sub>amortized<br></sub>\nT<sub>actual</sub>, the actual time of an operation, represents the exact (observed) amount of time required to execute a particular operation. In a binary search tree, for example, the actual time to perform a <b>find(x)</b> is 1 plus the depth of the node containing x. If we sum the basic equation over the entire sequence, and if the final potential is at least as large as the initial potential, then the amortized time is an upper bound on the actual time used during the execution of the sequence. Notice that while Tactual varies from operation to operation, T<sub>amortized</sub> is stable.\n"
            ],
            "guid": "jeTF&D*TOI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you pickup a&nbsp;<b>potential function</b>?",
                "Picking a potential function that proves a meaningful bound is a very tricky task; there is no one method that is used. Generally, many potential functions are tried before the one that works is found. Nevertheless, the discussion above suggests a few rules, which tell us&nbsp;the properties that good potential functions have. The potential function should:<br><ul><li> Always assume its minimum at the start of the sequence. A popular method of choosing potential functions is to ensure that the potential function is initially 0 and always nonnegative. All the examples that we will encounter use this strategy.</li><li> Cancel a term in the actual time. In our case, if the actual cost was c, then the potential change was 2 - c. When these are added, an amortized cost of 2 is obtained.</li></ul><img src=\"paste-e967ef1bd67619a982e167da114d0b29479a80be.jpg\"><br>"
            ],
            "guid": "E}5axFpw&%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Prove the following theorem: The amortized running times of <b>insert</b>, <b>deleteMin</b>, and <b>merge</b> are O(1), O(log N), and<br>O(log N), respectively, for binomial queues.\n",
                "The potential function is the number of trees. The initial potential is 0, and the potential is always nonnegative, so the amortized time is an upper bound on the actual time. The analysis for insert follows from the argument above. For merge, assume the two queues have N<sub>1</sub> and N<sub>2</sub> nodes with T<sub>1</sub> and T<sub>2</sub> trees, respectively. Let N = N<sub>1</sub>+N<sub>2</sub>. The actual time to perform the merge is O(log(N<sub>1</sub>) + log(N<sub>2</sub>)) = O(log N). After the merge, there can be at most log N trees, so the potential can increase by at most O(log N). This gives an amortized bound of O(log N). The deleteMin bound follows in a similar manner.\n"
            ],
            "guid": "B+av3i1YtF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why using the number of right nodes as a <b>portential</b> for the&nbsp;<b>amortized analysis</b>&nbsp;of&nbsp;<b>skew heaps</b>&nbsp;cannot be used to prove the bounds?",
                "What is needed is some sort of a potential function that captures the effect of skew heap operations. Recall that the effect of a merge is that every node on the right path is moved to the left path, and its old left child becomes the new right child. One idea might be to classify each node as a right node or left node, depending on whether or not it is a right child, and use the number of right nodes as a potential function. Although the potential is&nbsp;initially 0 and always nonnegative, the problem is that the potential does not decrease after a merge and thus does not adequately reflect the savings in the data structure. The result is that this potential function cannot be used to prove the desired bound.\n"
            ],
            "guid": "QFw3u]OL/U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which&nbsp;<b>potential&nbsp;</b>works for the <b>amortized analysis</b> of&nbsp;<b>skew heaps</b>?",
                "A node, <i>p</i>, is <b>heavy</b> if the number of descendants of <i>p</i>’s right subtree is at least half of the number of descendants of <i>p</i>, and <b>light</b> otherwise. Note that the number of descendants of a node includes the node itself.\n<br><img src=\"paste-ad8e6bbca827fb2bd4570fe30a5a727b1fd0d4f3.jpg\"><br>The nodes with values 15, 3, 6, 12, and 7 are heavy, and all other nodes are light.\n"
            ],
            "guid": "nzrI@p5POT",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Prove that the <b>amortized time</b> to merge two <b>skew heaps</b> is O(log N).",
                "The potential function we will use is the number of heavy nodes in the (collection of) heaps. This seems like a good choice, because a long right path will contain an inordinate number of heavy nodes. Because nodes on this path have their children swapped, these nodes will be converted to light nodes as a result of the merge.<br>Let H<sub>1</sub> and H<sub>2</sub> be the two heaps, with N<sub>1</sub> and N<sub>2</sub> nodes, respectively. Suppose the right path of H<sub>1</sub> has l<sub>1</sub> light nodes and h<sub>1</sub> heavy nodes, for a total of l<sub>1</sub> + h<sub>1</sub>. Likewise, H<sub>2&nbsp;</sub>has l<sub>2</sub> light and h<sub>2</sub> heavy nodes on its right path, for a total of l<sub>2</sub> + h<sub>2</sub> nodes. If we adopt the convention that the cost of merging two skew heaps is the total number of nodes on their right paths, then the actual time to perform the merge is l<sub>1</sub> + l<sub>2</sub> + h<sub>1</sub> + h<sub>2</sub>. Now the only nodes whose heavy/light status can change are nodes that are initially on the right path (and wind up on the left path), since no other nodes<br>have their subtrees altered.<br><img src=\"paste-b32df738d4ff168eec37c25ed0a5d7a4605e15fd.jpg\"><br>If a heavy node is initially on the right path, then after the merge it must become a light node. The other nodes that were on the right path were light and may or may not become heavy, but since we are proving an upper bound, we will have to assume the worst, which is that they become heavy and increase the potential. Then the net change in the number of heavy nodes is at most l<sub>1</sub> + l<sub>2</sub> - h<sub>1</sub> - h<sub>2</sub>. Adding the actual time and the potential change gives an amortized bound of 2(l<sub>1</sub>+l<sub>2</sub>).<br>Now we must show that l<sub>1</sub> +l<sub>2</sub> = O(log N). Since l<sub>1</sub> and l<sub>2</sub> are the number of light nodes on the original right paths, and the right subtree of a light node is less than half the size of the tree rooted at the light node, it follows directly that the number of light nodes on the right path is at most log N<sub>1</sub> + log N<sub>2</sub>, which is O(log N).<br>The proof is completed by noting that the initial potential is 0 and that the potential is always nonnegative. It is important to verify this, since otherwise the amortized time does not bound the actual time and is meaningless.\n"
            ],
            "guid": "t2Nq+OTeL:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the amortized time of&nbsp;<b>insert</b>&nbsp;and&nbsp;<b>deleteMin</b>&nbsp;for&nbsp;<b>skew heaps</b>?",
                "Since the <b>insert</b> and <b>deleteMin</b> operations are basically just merges, they also have O(log N) amortized bounds.\n"
            ],
            "guid": "BiO@g)@^F/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you lower the time bound of a binary heap based implementation of <b>Daijkstra's shortest-path algorithm</b>?",
                "In order to lower this time bound, the time required to perform the decreaseKey operation must be improved. d-heaps, give an O(logd |V|) time bound for the decreaseKey operation as well as for insert, but an O(d logd |V|) bound for deleteMin. By choosing d to balance the costs of |E| decreaseKey operations with |V| deleteMin operations, and remembering that d must always be at least 2, we see that a good choice for d is:<br><img src=\"paste-0f98456e3b4da2e5d865c036782de30cfc1d2211.jpg\"><br>This improves the time bound for Dijkstra’s algorithm to:<br><img src=\"paste-6dd28699613d365694c01a25077499eae9831bc4.jpg\">"
            ],
            "guid": "e!b^&01V8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bounds of&nbsp;<b>Fibonacci heap</b>?",
                "The Fibonacci heap is a data structure that supports all the basic heap operations in O(1) amortized time, with the exception of deleteMin and remove, which take O(log N) amortized time. It immediately follows that the heap operations in Dijkstra’s algorithm will require a total of O(|E| + |V| log |V|) time."
            ],
            "guid": "M-ozX_a8:w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a&nbsp;<b>Fibonacci heap</b>&nbsp;generalize&nbsp;<b>binomial queues</b>?",
                "Fibonacci heaps generalize binomial queues by adding two new concepts:<br><ul><li>A different implementation of decreaseKey: The method we have seen before is to percolate the element up toward the root. It does not seem&nbsp; reasonable to expect an O(1) amortized bound for this strategy, so a new method is needed.</li><li>Lazy merging: Two heaps are merged only when it is required to do so. This is similar to lazy deletion. For lazy merging, merges are cheap, but because lazy merging does not actually combine trees, the deleteMin operation could encounter lots of trees, making that operation expensive. Any one deleteMin could take linear time, but it is always possible to charge the time to previous merge operations. In particular, an expensive deleteMin must have been preceded by a large number of unduly cheap merges, which were able to store up extra potential.</li></ul>"
            ],
            "guid": "f*i2:<&{?n",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why doesn't a normal&nbsp;<b>decreaseKey</b>&nbsp;work with a&nbsp;<b>leftist heap</b>?",
                "In binary heaps, the decreaseKey operation is implemented by lowering the value at a node and then percolating it up toward the root until heap order is established. In the worst case, this can take O(log N) time, which is the length of the longest path toward the root in a balanced tree.<br>This strategy does not work if the tree that represents the priority queue does not have O(log N) depth. As an example, if this strategy is applied to leftist heaps, then the decreaseKey operation could take Θ(N) time.<br><img src=\"paste-c983775819086b1543ac0c03c90f9d6cf16cfb63.jpg\"><br>We see that for leftist heaps, another strategy is needed for the decreaseKey operation. Our example will be the leftist heap in:<br><img src=\"paste-00f99c08238fdb001bddb1f822755c538c24605c.jpg\"><br>Suppose we want to decrease the key with value 9 down to 0. If we make the change, we find that we have created a violation of heap order, which is indicated by a dashed line in:\n<br><img src=\"paste-e0466af0340b5990d79f8ac048e9cae07b7b6909.jpg\">"
            ],
            "guid": "iz@)$3M]ne",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What an alternative implementation&nbsp;of&nbsp;<b>decreaseKey</b>&nbsp;works with a&nbsp;<b>leftist heap</b>?",
                "We do not want to percolate the 0 to the root, because, as we have seen, there are cases where this could be expensive. The solution is to cut the heap along the dashed line, thus creating two trees, and then merge the two trees back into one. Let X be the node to which the decreaseKey operation is being applied, and let P be its parent. After the cut, we have&nbsp;two trees, namely, H<sub>1</sub> with root X, and T<sub>2</sub>, which is the original tree with H<sub>1</sub> removed.<br>\n<img src=\"paste-6d705b5698d78a41552119766a0937f33aa1d4c8.jpg\"><br>If these two trees were both leftist heaps, then they could be merged in O(log N) time, and we would be done. It is easy to see that H<sub>1</sub> is a leftist heap, since none of its nodes have had any changes in their descendants. Thus, since all of its nodes originally satisfied the leftist property, they still must. Nevertheless, it seems that this scheme will not work, because T<sub>2</sub> is not necessarily leftist. However, it is easy to reinstate the leftist heap property by using two observations:<br><ul><li> Only nodes on the path from P to the root of T<sub>2</sub> can be in violation of the leftist heap property; these can be fixed by swapping children.</li><li> Since the maximum right path length has at most Θlog(N + 1) nodes, we only need to check the first Θlog(N + 1) nodes on the path from P to the root of T<sub>2</sub>. This figure shows H<sub>1</sub> and T<sub>2</sub> after T<sub>2</sub> is converted to a leftist heap.</li></ul><img src=\"paste-9f3340fbfb2a6eacb14f9572c9b4cbfcdfaebef0.jpg\"><br>Because we can convert T<sub>2</sub> to the leftist heap H<sub>2</sub> in O(log N) steps, and then merge H<sub>1</sub> and H<sub>2</sub>, we have an O(log N) algorithm for performing the decreaseKey operation in leftist heaps.<br><img src=\"paste-03f6be8cf7c68d74588d43e738f0ca0ac16041a6.jpg\"><br>"
            ],
            "guid": "G5D&Xz8Db1",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is&nbsp;<b>lazy merging</b>?",
                "<img src=\"paste-45c3a19d6442fba912749666c8206a8acb5447f3.jpg\"><br>The idea is as follows: To merge two binomial queues, merely concatenate the two lists of binomial trees, creating a new binomial queue. This new queue may have several trees of the same size, so it violates the binomial queue property. We will call this a <b>lazy binomial queue</b> in order to maintain consistency. This is a fast operation that always takes constant (worst-case) time.&nbsp;The deleteMin operation is much more painful, because it is where we finally convert the lazy binomial queue back into a standard binomial queue, but, as we will show, it is still O(log N) amortized time—but not O(log N) worst-case time, as before. To perform a <b>deleteMin</b>, we find (and eventually return) the minimum element. We delete it from the queue, making each of its children new trees. We then merge all the trees into a binomial queue by merging two equal-sized trees until it is no longer possible.\n<br><img src=\"paste-60934bee6a350ebe264dc0bbe7cad6bab5723dd0.jpg\"><br>We now have to merge all the trees and obtain a standard binomial queue. A standard binomial queue has at most one tree of each rank. In order to do this efficiently, we must be able to perform the merge in time proportional to the number of trees present (T) (or log N, whichever is larger). To do this, we form an array of lists, L<sub>0</sub>, L<sub>1</sub>, . . . , L<sub>Rmax+1</sub>, where R<sub>max</sub> is the rank of the largest tree. Each list, L<sub>R</sub>, contains all of the trees of rank R. \n<br><img src=\"paste-997553b5ac4186b4d6e8ed82a61c9d597ed83893.jpg\"><br>Each time through the loop, at lines 4 to 6, the total number of trees is reduced by 1. This means that this part of the code, which takes constant time per execution, can only be performed T - 1 times, where T is the number of trees. The for loop counters and tests at the end of the while loop take O(log N) time, so the running time is O(T + log N), as required.\n<br><img src=\"paste-0a489379cc9aaf77f5dc25d77880269283b36e12.jpg\">"
            ],
            "guid": "w>7$ng/q)W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Prove that:<br>The amortized running times of merge and insert are both O(1) for lazy binomial queues. The amortized running time of deleteMin is O(log N).\n",
                "The potential function is the number of trees in the collection of binomial queues. The initial potential is 0, and the potential is always nonnegative. Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time.<br>For the merge operation, the actual time is constant, and the number of trees in the collection of binomial queues is unchanged, so, the amortized&nbsp;time is O(1).<br>For the insert operation, the actual time is constant, and the number of trees can increase by at most 1, so the amortized time is O(1).<br>The deleteMin operation is more complicated. Let R be the rank of the tree that contains the minimum element, and let T be the number of trees. Thus, the potential at the start of the deleteMin operation is T. To perform a deleteMin, the children of the smallest node are split off into separate trees. This creates T + R trees, which must be merged into a standard binomial queue. The actual time to perform this is T+R+log N, if we ignore the constant in the Big-Oh notation, by the argument above. Once this is done, there can be at most log N trees remaining, so the potential function can increase by at most (log N) - T. Adding the actual time and the change in potential gives an amortized bound of 2 log N + R. Since all the trees are binomial trees, we know that R ≤ log N. Thus we arrive at an O(log N) amortized time bound for the deleteMin operation.\n"
            ],
            "guid": "oT;{LBs;a#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why leftist heap decreaseKey operation with the lazy binomial queue merge operation cannot be used in Fibonacci heaps without a modification?<br>Ex: when 39 is decreased to 12 in the following figure:<br><img src=\"paste-4faa76c082caf71ba5f6aa5b1fee6e5eb8c34dd1.jpg\">",
                "The problem is that if arbitrary cuts are made in the binomial trees, the resulting forest will no longer be a collection of binomial trees. Because of this, it will no longer be true that the rank of every tree is at most [log N] . Since the amortized bound for deleteMin in lazy binomial queues was shown to be 2 log N + R, we need R = O(log N) for the deleteMin bound to hold.\n<br>In order to ensure that R = O(log N), we apply the following rules to all non-root nodes:<br><ul><li>\nMark a (non-root) node the first time that it loses a child (because of a cut).</li><li> If a marked node loses another child, then cut it from its parent. This node now becomes the root of a separate tree and is no longer marked. This is called a <b>cascading cut</b>, because several of these could occur in one decreaseKey operation.</li></ul><div><img src=\"paste-0d39d9998006353a94e90dfb2ea010e2d6383aca.jpg\"><br></div>"
            ],
            "guid": "H&1097a?cX",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the amortized time bounds for Fibonacci heaps?",
                "O(1) for insert, merge, and decreaseKey and O(log N) for deleteMin.\n"
            ],
            "guid": "DXtX:Q%84;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the amortized time to splay a tree with root T at node X?",
                "At most 3(R(T)-R(X))+1 = O(log N)\n"
            ],
            "guid": "L1t1ERffQ|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the sources of overhead in <b>splay trees</b>?",
                "A direct implementation of this strategy requires a traversal from the root down the tree, and then a bottom-up traversal to implement the splaying step. This can be done either by maintaining parent links, or by storing the access path on a stack. Unfortunately, both methods require a substantial amount of overhead, and both must handle many special cases.\n"
            ],
            "guid": "nO+N9:=wkf",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the rotations of&nbsp;<b>top-down</b>&nbsp;<b>splay trees?</b>",
                "The figure below shows the rotations for the zig, zig-zig, and zig-zag cases. (As is customary, three symmetric rotations are omitted.) At any point in the access, we have a current node, X, that is the root of its subtree; this is represented in our diagrams as the “middle” tree. Tree L stores nodes in the tree T that are less than X, but not in X’s subtree; similarly tree R stores nodes in the tree T that are larger than X, but not in X’s subtree. Initially, X is the root of T, and L and R are empty.<br><img src=\"paste-f8e30978e0904378086524d07374d530e021b55a.jpg\"><br>If the rotation should be a <b>zig</b>, then the tree rooted at Y becomes the new root of the middle tree. X and subtree B are attached as a left child of the smallest item in R; X’s left child is logically made nullptr. As a result, X is the new smallest item in R. Note carefully that Y does not have to be a leaf for the zig case to apply. If we are searching for an item that is smaller than Y, and Y has no left child (but does have a right child), then the zig<br>case will apply.<br>For the zig-zig case, we have a similar dissection. The crucial point is that a rotation between X and Y is performed. The zig-zag case brings the bottom node Z to the top in the middle tree, and attaches subtrees X and Y to R and L, respectively. Note that Y is attached to, and then becomes, the largest item in L.\n<br>The zig-zag step can be simplified somewhat because no rotations are performed. Instead of making Z the root of the middle tree, we make Y the root.<br><img src=\"paste-d1b329cddf0808ea15f70e106019645f46300275.jpg\"><br>This simplifies the coding because the action for the zig-zag case becomes identical to the zig case. This would seem advantageous because testing for a host of cases is time-consuming. The disadvantage is that by descending only one level, we have more iterations in the splaying procedure.\n<br>Once we have performed the final splaying step, the figure below shows how L, R, and the middle tree are arranged to form a single tree. Note carefully that the result is different from bottom-up splaying. The crucial fact is that the O(log N) amortized bound is preserved.<br><img src=\"paste-469b9dfc71b07040d09b8d9bef357e46781915a0.jpg\">"
            ],
            "guid": "EN)RVJb^SC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you do&nbsp;<b>top-down</b>&nbsp;<b>splay</b>&nbsp;to the following tree?<br><img src=\"paste-f9eff7714585e49ba4a418d7d136638a3f47bebc.jpg\">",
                "<img src=\"paste-6066f0488c507364d11b1b100ba8d635c1d087e1.jpg\">"
            ],
            "guid": "xC/Gd6Ye.j",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an historically popular alternative to AVL trees?",
                "A historically popular alternative to the AVL tree is the <b>red-black tree</b>. Operations on red-black trees take O(log N) time in the worst case, and, as we will see, a careful nonrecursive implementation (for insertion) can be done relatively effortlessly (compared with AVL trees).\n"
            ],
            "guid": "L~m-dVP>^H",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the coloring properties of a&nbsp;<b>red-black tree</b>?",
                "<ol><li>Every node is colored either red or black.</li><li>The root is black.</li><li>If a node is red, its children must be black.</li><li>Every path from a node to a null pointer must contain the same number of black nodes.</li></ol>"
            ],
            "guid": "v5MjoV<*?<",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of <b>red-black trees</b>?",
                "A consequence of the coloring rules is that the height of a red-black tree is at most 2 log(N + 1). Consequently, searching is guaranteed to be a logarithmic operation.<br>Red nodes are shown with double circles. The difficulty, as usual, is inserting a new item into the tree. The new item, as usual, is placed as a leaf in the tree. If we color this item black, then we are certain to violate condition 4, because we will create a longer path of black nodes. Thus, the item must be colored red. If the parent is black, we are done. If the parent is already red, then we will violate condition 3 by having consecutive red nodes. In this case, we have to adjust the tree to ensure that condition 3 is enforced (without introducing a violation of condition 4). The basic operations that are used to do this are color changes and tree rotations.\n<br><img src=\"paste-81d78cc1e008a3af42970e15b2982dd734bab181.jpg\">"
            ],
            "guid": "PYVsn+bWDS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the considerations of bottom-up insertion in&nbsp;<b>red-black trees</b>?",
                "<img src=\"paste-4ed2168d3b2b001c9c8a69a8499773d2e8206d77.jpg\"><br>If the parent of the newly inserted item is black, we are done. Thus insertion of 25 into the tree is trivial.\n<br>There are several cases (each with a mirror image symmetry) to consider if the parent is red. First, suppose that the sibling of the parent is black (we adopt the convention that null nodes are black). This would apply for an insertion of 3 or 8, but not for the insertion of 99. Let X be the newly added leaf, P be its parent, S be the sibling of the parent (if it exists), and G be the grandparent. Only X and P are red in this case; G is black, because<br>otherwise there would be two consecutive red nodes prior to the insertion, in violation of red-black rules. Adopting the splay tree terminology, X, P, and G can form either a zig-zig chain or a zig-zag chain (in either of two directions). The figure below shows how we can rotate the tree for the case where P is a left child (note there is a symmetric case). Even though X is a leaf, we have drawn a more general case that allows X to be in the middle of the tree. We will use this more general rotation later.<br>\n<img src=\"paste-1aa377763dce9a19c0e5bcba54aec33338a3a307.jpg\"><br>The first case corresponds to a single rotation between P and G, and the second case corresponds to a double rotation, first between X and P and then between X and G. When we write the code, we have to keep track of the parent, the grandparent, and, for reattachment purposes, the great-grandparent.<br>In both cases, the subtree’s new root is colored black, and so even if the original great-grandparent was red, we removed the possibility of two consecutive red nodes. Equally important, the number of black nodes on the paths into A, B, and C has remained unchanged as a result of the rotations.<br>So far so good. But what happens if S is red, as is the case when we attempt to insert 79 in the tree in the first figure? In that case, initially there is one black node on the path from the subtree’s root to C. After the rotation, there must still be only one black node. But in both cases, there are three nodes (the new root, G, and S) on the path to C. Since only one may be black, and since we cannot have consecutive red nodes, it follows that<br>we’d have to color both S and the subtree’s new root red, and G (and our fourth node) black. That’s great, but what happens if the great-grandparent is also red? In that case, we could percolate this procedure up toward the root as is done for B-trees and binary heaps, until we no longer have two consecutive red nodes, or we reach the root (which will be recolored black).\n"
            ],
            "guid": "Hk[Kn|#&,C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do bottom-up&nbsp;<b>red-black trees&nbsp;</b>compare to top-down ones?",
                "Implementing the percolation would require maintaining the path using a stack or parent links. Since splay trees are more efficient if we use a top-down procedure, and it turns out that we can apply a top-down procedure to red-black trees that guarantees that S won’t be red.\n"
            ],
            "guid": "kq|t6M1(aS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is top-down insertion is done in&nbsp;<b>red-black trees</b>?",
                "On the way down, when we see a node X that has two red children, we make X red and the two children black. (If X is the root, after the color<br>flip it will be red but can be made black immediately to restore property 2.) The figure below shows this color flip.<br><img src=\"paste-6d9e0f24ef3ff7ae362fb583a12d6efda733a7b7.jpg\"><br>This will induce a red-black violation only if X’s parent P is also red. But in that case, we can apply the appropriate rotations in the figure below.<br><img src=\"paste-2af56cafd950e0cbb567702bd08afa9401b7937a.jpg\"><br>What if X’s parent’s sibling is red? This possibility has been removed by our actions on the way down, and so X’s parent’s sibling can’t be red! Specifically, if on the way down the tree we see a node Y that has two red children, we know that Y’s grandchildren must be black, and that since Y’s children are made black too, even after the rotation that may occur, we won’t see another red node for two levels. Thus when we see X, if X’s parent is red, it is not possible for X’s parent’s sibling to be red also.\n"
            ],
            "guid": "Bl-|gp|13B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we do top-down insertion for the value 45 in the following <b>red-black tree</b>?<br><img src=\"paste-332b5e5cfd47084ef40b008299f0fedcf38ad03b.jpg\">",
                "On the way down the tree, we see node 50, which has two red children. Thus, we perform a color flip, making 50 red, and 40 and 55 black. Now 50 and 60 are both red. We perform the single rotation between 60 and 70, making 60 the black root of 30’s right subtree, and 70 and 50 both red. We then continue, performing an identical action if we see other nodes on the path that contain two red children. When we get to the leaf, we insert 45 as a red node, and since the parent is black, we are done.<br><img src=\"paste-6981b1bfdd56b0530ac3423294299fe101bbf63e.jpg\">"
            ],
            "guid": "NfT(O!3Q@m",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of the&nbsp;<b>red-black trees&nbsp;</b>resulting from top-down inserts?",
                "The red-black tree that results is frequently very well balanced. Experiments suggest that the average red-black tree is about as deep as an average AVL tree and that, consequently, the searching times are typically near optimal. The advantage of red-black trees is the relatively low overhead required to perform insertion, and the fact that, in practice, rotations occur relatively infrequently.\n"
            ],
            "guid": "K]R@AvMX/T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the complications of implementing top-down insertion in&nbsp;<b>red-black trees</b>?<br><img src=\"paste-2c721c419c4aedb03939cf508937104821c78f35.jpg\">",
                "An actual implementation is complicated not only by the host of possible rotations but also by the possibility that some subtrees (such as 10’s right subtree) might be empty, and the special case of dealing with the root (which among other things, has no parent). Thus, we use two sentinel nodes: one for the root, and nullNode, which indicates a nullptr pointer as it did for splay trees. The root sentinel will store the key -∞ and a right link to the real root. Because of this, the searching and printing procedures need to be adjusted. The recursive routines are trickiest. The figure below shows how the inorder traversal is rewritten.<br><img src=\"paste-ef45a4211df55ae5025cffce4e32391813ac5cee.jpg\"><br>The printTree routines are straightforward. The test t!=t-&gt;left could be written as t!=nullNode. However, there is a trap in a similar routine that performs the deep copy. This is also shown in the figure above. The copy constructor calls clone after other initialization is complete. But in clone, the test t==nullNode does not work, because nullNode is the target’s nullNode, not the source’s (that is, not rhs’s). Thus we use a trickier test.<br>The figure below shows the RedBlackTree skeleton, along with the constructor.<br><img src=\"paste-e0b4c82202836e8480bacae0f55544b14709feac.jpg\"><br><img src=\"paste-f8e8236894b0893eb9c92a554f6346add0873989.jpg\"><br>Next, the figure below shows the routine to perform a single rotation. Because the resultant tree must be attached to a parent, rotate takes the&nbsp; parent node as a parameter. Rather than keeping track of the type of rotation as we descend the tree, we pass item as a parameter. Since we expect very few rotations during the insertion procedure, it turns out that it is not only simpler, but actually faster, to do it this way. rotate simply returns the result of performing an appropriate single rotation.<br><img src=\"paste-a0ebfee0516dac2d560a467f68385ba4c6820273.jpg\"><br>Finally, we provide the insertion procedure in the figure below. The routine handleReorient is called when we encounter a node with two red children, and also when we insert a leaf. The trickiest part is the observation that a double rotation is really two single rotations, and is done only when branching to X (represented in the insert method by current) takes opposite directions. As we mentioned in the earlier discussion, insert must keep track of the parent, grandparent, and great-grandparent as the tree is descended. Since these are shared with handleReorient, we make these class members.<br><img src=\"paste-8c68e82d8a545c4d40144f3b7d6e305075f947b6.jpg\"><br>Note that after a rotation, the values stored in the grandparent and great-grandparent are no longer correct. However, we are assured that they will be restored by the time they are next needed.\n"
            ],
            "guid": "k%acWp`y(A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we perform top-down deletion in a&nbsp;<b>red-black tree</b>?",
                "Deletion in red-black trees can also be performed top-down. Everything boils down to being able to delete a leaf. This is because to delete a node that has two children, we replace it with the smallest node in the right subtree; that node, which must have at most one child, is then deleted. Nodes with only a right child can be deleted in the same manner, while nodes with only a left child can be deleted by replacement with the largest node in<br>the left subtree, and subsequent deletion of that node. Note that for red-black trees, we don’t want to use the strategy of bypassing for the case of a node with one child because that may connect two red nodes in the middle of the tree, making enforcement of the red-black condition difficult.<br>\nDeletion of a red leaf is, of course, trivial. If a leaf is black, however, the deletion is more complicated because removal of a black node will violate condition 4. The solution is to ensure during the top-down pass that the leaf is red.\n"
            ],
            "guid": "iE/[[zvpvg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we delete a black leaf in a&nbsp;<b>red-black tree</b>?",
                "Throughout this discussion, let X be the current node, T be its sibling, and P be their parent. We begin by coloring the root sentinel red. As we traverse down the tree, we attempt to ensure that X is red. When we arrive at a new node, we are certain that P is red (inductively, by the invariant we are trying to maintain), and that X and T are black (because we can’t have two consecutive red nodes). There are two main cases.\n<br>First, suppose X has two black children. Then there are three subcases, which are shown in the figure below.<br><img src=\"paste-9fed57e45185ee7f381431f70ceb6c287e410718.jpg\"><br>If T also has two black children, we can flip the colors of X, T, and P to maintain the invariant. Otherwise, one of T’s children is red. Depending on which one it is, we can apply the rotation shown in the second and third cases of the figure above. Note carefully that this case will apply for the leaf, because nullNode is considered to be black.\n<br>Otherwise one of X’s children is red. In this case, we fall through to the next level, obtaining new X, T, and P. If we’re lucky, X will land on the red child, and we can continue onward. If not, we know that T will be red, and X and P will be black. We can rotate T and P, making X’s new parent red; X and its grandparent will, of course, be black. At this point, we can go back to the first main case.\n"
            ],
            "guid": "q7_qJMV0$v",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time-bounds of a&nbsp;<b>treap</b>?",
                "Like the skip list, it uses random numbers and gives O(log N) expected time behavior for any input. Searching time is identical to an unbalanced binary search tree (and thus slower than balanced search trees), while insertion time is only slightly slower than a recursive unbalanced binary search tree implementation. Although deletion is much slower, it is still O(log N) expected time.\n"
            ],
            "guid": "b|1`>~&>SS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the structure of a&nbsp;<b>treap</b>.",
                "Each node in the tree stores an item, a left and right pointer, and a priority that is randomly assigned when the node is created. A treap is a binary search tree with the property that the node priorities satisfy heap order: Any node’s priority must be at least as large as its parent’s.\n"
            ],
            "guid": "LDsI|yu1x3",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we implement a&nbsp;<b>treap</b>?",
                "A collection of distinct items each of which has a distinct priority can only be represented by one treap. This is easily deduced by induction, since the node with the lowest priority must be the root. Consequently, the tree is formed on the basis of the N! possible arrangements of priority instead of the N! item orderings. The node declarations are straightforward, requiring only the addition of the priority data member. The sentinel nullNode will have priority of ∞, as shown below.<br><img src=\"paste-ad9fa24795dde3f49ea22eeb5d21963b5062e40f.jpg\"><br>Insertion into the treap is simple: After an item is added as a leaf, we rotate it up the treap until its priority satisfies heap order. It can be shown that the expected number of rotations is less than 2. After the item to be deleted has been found, it can be deleted by increasing its priority to ∞ and rotating it down through the path of low-priority children. Once it is a leaf, it can be removed. The routines in the figures below implement these strategies using recursion. <br><img src=\"paste-887ce7f8c9c42ef7bf5a05cf1afb1444ab360d48.jpg\"><br><img src=\"paste-283fc2bedc9a8515cd604bd1dfc8f8e11a53b3c7.jpg\"><br>For deletion, note that when the node is logically a leaf, it still has nullNode as both its left and right children. Consequently, it is rotated with the right child. After the rotation, t is nullNode, and the left child, which now stores the item to be deleted, can be freed. Note also that our implementation assumes that there are no duplicates; if this is not true, then the remove could fail.<br>The treap implementation never has to worry about adjusting the priority data member. One of the difficulties of the balanced tree approaches is that it is difficult to track down errors that result from failing to update balance information in the course of an operation. In terms of total lines for a reasonable insertion and deletion package, the treap, especially a nonrecursive implementation, seems like the hands-down winner.\n"
            ],
            "guid": "Ee;u)om){H",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>suffix array</b>?",
                "A suffix array for a text, T, is simply an array of all suffixes of T arranged in sorted order. For instance, suppose our text string is banana. Then the suffix array for banana is shown in the figure.<br><img src=\"paste-6e3a316db5bdb5059f44ae76b2c676141c16e621.jpg\">"
            ],
            "guid": "mtptie/9XX",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative do we have to storing the suffixes explicitly in&nbsp;<b>suffix arrays</b>?",
                "A suffix array that stores the suffixes explicitly would seem to require quadratic space, since it stores one string of each length 1 to N (where N is the length of T). In C++, this is not exactly true, since in C++ we can use the primitive null-terminated array of character representation of strings, and in that case, a suffix is specified by a char * that points at the first character of the substring. Thus, the same array of characters is shared, and the additional memory requirement is only the char * pointer for the new substring. Nonetheless, using a char * is highly C or C++ dependent; thus it is common for a practical implementation to store only the starting indices of the suffixes in the suffix array, which is much more language independent.<br>\n<img src=\"paste-1e890c3d036844c1e460d3b6fb45f011e0840da4.jpg\">"
            ],
            "guid": "z`:,}(v&fE",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use&nbsp;<b>suffix arrays&nbsp;</b>to find patterns in a text?",
                "For instance, if a pattern, P, occurs in the text, then it must be a prefix of some suffix. A binary search of the suffix array would be enough to determine if the pattern P is in the text: The binary search either lands on P, or P would be between two values, one smaller than P and one larger than P. If P is a prefix of some substring, it is a prefix of the larger value found at the end of the binary search. Immediately, this reduces the query time to O( | P | log | T | ), where the log | T | is the binary search, and the | P | is the cost of the comparison at each step.\n"
            ],
            "guid": "r#IH<$f~ky",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use <b>suffix arrays</b> to find the number of occurences of a pattern in a text?",
                "We can use the suffix array to find the number of occurrences of P: They will be stored sequentially in the suffix array, thus two binary searches suffice to find a range of suffixes that will be guaranteed to begin with P. One way to speed this search is to compute the longest common prefix (LCP) for each consecutive pair of substrings; if this computation is done as the suffix array is built, then each query to find the number of&nbsp; occurrences of P can be sped up to O( | P | + log | T | ) although this is not obvious. The figure shows the LCP computed for each substring, relative to the preceding substring.<br>\n<img src=\"paste-70079154169e6cf8fa504990ec76229c99f57de5.jpg\"><br>The longest common prefix also provides information about the longest pattern that occurs twice in the text: Look for the largest LCP value, and take that many characters of the corresponding substring. In the figure above, this is 3, and the longest repeated pattern is ana.\n"
            ],
            "guid": ",$jLJ0[2E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bounds of the following simple algorithm for&nbsp;<b>suffix arrays</b>&nbsp;computation?<br><img src=\"paste-c28635903e4cacf58e2cd4c7bcd93818cfe5f9fd.jpg\">",
                "The running time of the suffix array computation is dominated by the sorting step, which uses O( N log N ) comparisons. In many circumstances this can be reasonably acceptable performance. For instance, a suffix array for a 3,000,000-character English-language novel can be built in just a few seconds. However, the O( N log N ) cost, based on the number of comparisons, hides the fact that a String comparison between s1 and s2 takes time that depends on LCP(s1, s2), So while it is true that almost all these comparisons end quickly when run on the suffixes found in natural&nbsp; language processing, the comparisons will be expensive in applications where there are many long common substrings. One such example occurs in pattern searching of DNA, whose alphabet consists of four characters (A, C, G, T) and whose strings can be huge. For instance, the DNA string for human chromosome 22 has roughly 35 million characters, with a maximum LCP of approximately 200,000 and an average LCP of nearly 2,000. And even the HTML/Java distribution for JDK 1.3 (much smaller than the current distribution) is nearly 70 million characters, with a maximum LCP of roughly 37,000 and an average LCP of roughly 14,000. In the degenerate case of a String that contains only one character, repeated N times, it is<br>easy to see that each comparison takes O(N) time, and the total cost is O( N<sup>2</sup> log N ).\n"
            ],
            "guid": "Qar(ktrHfb",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative do we have for&nbsp;<b>suffix arrays</b>?",
                "Suffix arrays are easily searchable by binary search, but the binary search itself automatically implies log T cost. What we would like to do is find a matching suffix even more efficiently. One idea is to store the suffixes in a <b>trie</b>.\n"
            ],
            "guid": "p#1.G3Qsx8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the basic idea behind a&nbsp;<b>trie</b>?",
                "The basic idea of the trie is to store the suffixes in a tree. At the root, instead of having two branches, we would have one branch for each possible first character. Then at the next level, we would have one branch for the next character, and so on. At each level we are doing multiway branching, much like radix sort, and thus we can find a match in time that would depend only on the length of the match.<br>\n<img src=\"paste-bed2fa4a0d92b759bdcd18fce24c1bdee72aca3a.jpg\">"
            ],
            "guid": "K=R}eF~HqW",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we optimized the space of a&nbsp;<b>trie</b>?<br><img src=\"paste-bed2fa4a0d92b759bdcd18fce24c1bdee72aca3a.jpg\">",
                "In the figure above, we see on the left a basic trie to store the suffixes of the string deed. These suffixes are d, deed, ed, and eed. In this trie, internal branching nodes are drawn in circles, and the suffixes that are reached are drawn in rectangles. Each branch is labeled with the character that is chosen, but the branch prior to a completed suffix has no label.<br>This representation could waste significant space if there are many nodes that have only one child. Thus in the figure below, we see an equivalent representation on the right, known as a compressed trie. Here, single-branch nodes are collapsed into a single node. Notice that although the branches now have multicharacter labels, all the labels for the branches of any given node must have unique first characters. Thus, it is still just as easy as before to choose which branch to take. Thus we can see that a search for a pattern, P, depends only on the length of the pattern P, as desired. (We assume that the letters of the alphabet are represented by numbers 1, 2, . . . . Then each node stores an array representing each<br>possible branch and we can locate the appropriate branch in constant time. The empty edge label can be represented by 0.)<br><img src=\"paste-a0640992b26f65f3804ae02db77b1a5def55d21b.jpg\"><br>If the original string has length N, the total number of branches is less than 2N. However, this by itself does not mean that the compressed trie uses linear space: The labels on the edges take up space. The total length of all the labels on the compressed trie in the figure above is exactly one less than the number of internal branching nodes in the original trie in the figure above. And of course writing all the suffixes in the leaves could take quadratic space. So if the original used quadratic space, so does the compressed trie. Fortunately, we can get by with linear space as follows:\n<br><ol><li>In the leaves, we use the index where the suffix begins (as in the suffix array).</li><li>In the internal nodes, we store the number of common characters matched from the root until the internal node; this number represents the letter depth.</li></ol>\nThe figure below shows how the compressed trie is stored for the suffixes of banana. The leaves are simply the indices of the starting points for each suffix. The internal node with a letter<br>depth of 1 is representing the common string “a” in all nodes that are below it. The internal node with a letter depth of 3 is representing the common string “ana” in all nodes that are below it. And the internal node with a letter depth of 2 is representing the common string “na” in all nodes that are below it. In fact, this analysis makes clear that a suffix tree is equivalent to a suffix array plus an LCP array.<br><img src=\"paste-1f32a7e5542a3981d552ec0927b197e69f96ff1e.jpg\"><br>"
            ],
            "guid": "o]35RBR>%g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How we can compute the suffix array and the LCP array if we have a <b>suffix tree</b>?<br><img src=\"paste-c6b67a50c3807984c8d2e3c1b9c15413c50007ce.jpg\">",
                "If we have a suffix tree, we can compute the suffix array and the LCP array by performing an inorder traversal of the tree. At that time we can compute the LCP as follows: If the suffix node value PLUS the letter depth of the parent is equal to N, then use the letter depth of the grandparent as the LCP; otherwise use the parent’s letter depth as the LCP. In the figure above, if we proceed inorder, we obtain for our suffixes and LCP values&nbsp;<br>Suffix = 5, with LCP = 0 (the grandparent) because 5 + 1 equals 6<br>Suffix = 3, with LCP = 1 (the grandparent) because 3 + 3 equals 6<br>Suffix = 1, with LCP = 3 (the parent) because 1 + 3 does not equal 6<br>Suffix = 0, with LCP = 0 (the parent) because 0 + 0 does not equal 6<br>Suffix = 4, with LCP = 0 (the grandparent) because 4 + 2 equals 6<br>Suffix = 2, with LCP = 2 (the parent) because 2 + 2 does not equal 6\n<br>This transformation can clearly be done in linear time.\n"
            ],
            "guid": "m&jgUw)}+?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Do <b>suffix arrays</b> uniquely define <b>suffix trees</b>?",
                "<img src=\"paste-c31c53f1eb1eed3670242a6db8307e7c4f3369ee.jpg\"><br>The suffix array and LCP array also uniquely define the suffix tree. First, create a root with letter depth 0. Then search the LCP array (ignoring position 0, for which LCP is not really defined) for all occurrences of the minimum (which at this phase will be the zeros). Once these minimums are found, they will partition the array (view the LCP as residing between adjacent elements). For instance, in our example, there are two zeros in the LCP array, which partitions the suffix array into three portions: one portion containing the suffixes {5, 3, 1}, another portion containing the suffix {0}, and the third portion containing the suffixes {4, 2}. The internal nodes for these portions can be built recursively, and then the suffix leaves can be attached with an inorder traversal. Although it is not obvious, with care the suffix tree can be generated in linear time from the suffix array and LCP array.\n"
            ],
            "guid": "QYw=M{QC%0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What problems can be solved with&nbsp;<b>suffix trees</b>?",
                "The suffix tree solves many problems efficiently, especially if we augment each internal node to also maintain the number of suffixes stored below it. A small sampling of suffix tree applications includes the following:<br><ol><li>Find the longest repeated substring in T: Traverse the tree, finding the internal node with the largest number letter depth; this represents the maximum LCP. The running time is O( | T | ). This generalizes to the longest substring repeated at least k times.</li><li>Find the longest common substring in two strings T<sub>1</sub> and T<sub>2</sub>: Form a string T<sub>1</sub>#T<sub>2</sub> where # is a character that is not in either string. Then build a suffix tree for the resulting string and find the deepest internal node that has at least one suffix that starts prior to the #, and one that starts after the #. This can be done in time proportional to the total size of the strings and generalizes to an O( k N ) algorithm for k strings of total length N.</li><li>Find the number of occurrences of the pattern P: Assuming that the suffix tree is augmented so that each node keeps track of the number of suffixes below it, simply follow the path down the tree; the first internal node that is a prefix of P provides the answer; if there is no such node, the answer is either zero or one and is found by checking the suffix at which the search terminates. This takes time, proportional to the length of the pattern P, and is independent of the size of |T|.</li><li>Find the most common substring of a specified length L &gt; 1: Return the internal node with largest size amongst those with letter depth at least L. This takes time O( | T | ).</li></ol>"
            ],
            "guid": "y%ckhn4qLD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we compute a&nbsp;<b>suffix array&nbsp;</b>in linear time?",
                "This algorithm makes use of divide and conquer. The basic idea is as follows:<br>1. Choose a sample, A, of suffixes.<br>2. Sort the sample A by recursion.<br>3. Sort the remaining suffixes, B, by using the now-sorted sample of suffixes A.<br>4. Merge A and B.\n"
            ],
            "guid": "J&guccQ@uz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does this step work for constructing&nbsp;<b>suffix arrays</b>?<br>\"Sort the remaining suffixes, B, by using the now-sorted sample of suffixes A.\"",
                "Suppose the sample A of suffixes are all suffixes that start at an odd index. Then the remaining suffixes, B, are those suffixes that start at an even index. So suppose we have computed the sorted set of suffixes A. To compute the sorted set of suffixes B, we would in effect need to sort all the suffixes that start at even indices. But these suffixes each consist of a single first character in an even position, followed by a string that starts with the second character, which must be in an odd position. Thus the string that starts in the second character is exactly a string that is in A. So to sort all the suffixes B, we can do something similar to a radix sort: First sort the strings in B starting from the second character. This should take linear time, since the sorted order of A is already known. Then stably sort on the first character of the strings in B. Thus B could be sorted in linear time, after A is sorted recursively. If A and B could then be merged in linear time, we would have a linear-time algorithm. The algorithm we present uses a different sampling step, that admits a simple linear-time merging step.\n"
            ],
            "guid": "Blq!8N?H*B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the first step in computing the <b>suffix array</b> for the string ABRACADABRA?",
                "We adopt the following conventions:<br><table><tbody><tr><td>S[i] <br>S[i →] <br>&lt;&gt; </td><td>represents the ith character of string S<br>represents the suffix of S starting at index i<br>represents an array</td></tr></tbody></table>\nStep 1: Sort the characters in the string, assigning them numbers sequentially starting at 1. Then use those numbers for the remainder of the algorithm. Note that the numbers that are assigned depend on the text. So, if the text contains DNA characters A, C, G, and T only, then there will be only four numbers. Then pad the array with three 0s to avoid boundary cases. If we assume that the alphabet is a fixed size, then the sort takes some<br>constant amount of time.<br>\nIn our example, the mapping is A = 1, B = 2, C = 3, D = 4, and R = 5.<br><img src=\"paste-52e3e688dede4ff5d49a42630f38adf21f1a8721.jpg\"><br>"
            ],
            "guid": "uuuRdy#g+>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we do after sorting the characters in a string to calculate a&nbsp;<b>suffix array</b>?<br><img src=\"paste-97c8eebfbc98e3d967e85e6d5ac4d546f3fb4e4d.jpg\">",
                "Divide the text into three groups:<br>S<sub>0</sub> = &lt; S[3i]S[3i + 1]S[3i + 2] for i = 0, 1, 2, . . . &gt;<br>S<sub>1</sub> = &lt; S[3i + 1]S[3i + 2]S[3i + 3] for i = 0, 1, 2, . . . &gt;<br>S<sub>2</sub> = &lt; S[3i + 2]S[3i + 3]S[3i + 4] for i = 0, 1, 2, . . . &gt;<br>The idea is that each of S<sub>0</sub>, S<sub>1</sub>, S<sub>2</sub> consists of roughly N/3 symbols, but the symbols are no longer the original alphabet, but instead each new symbol is some group of three symbols from the original alphabet. We will call these tri-characters. Most importantly, the suffixes of S<sub>0</sub>, S<sub>1</sub>, and S<sub>2</sub> combine to form the suffixes of S. Thus one idea would be to recursively compute the suffixes of S<sub>0</sub>, S<sub>1</sub>, and S<sub>2</sub> (which by definition implicitly represent sorted strings) and then merge the results in linear time. However, since this would be three recursive calls on problems 1/3 the original size, that would result in an O( N log N ) algorithm. So the idea is going to be to avoid one of the three recursive calls, by computing two of the suffix groups recursively and using that information to compute the third suffix group.\n<br>In our example, if we look at the original character set and use $ to represent the padded character, we get:<br>S<sub>0</sub> = [ABR], [ACA], [DAB], [RA$]<br>S<sub>1</sub> = [BRA], [CAD], [ABR], [A$$]<br>S<sub>2</sub> = [RAC], [ADA], [BRA]<br>\nWe can see that in S<sub>0</sub>, S<sub>1</sub>, and S<sub>2</sub>, each tri-character is now a trio of characters from the original alphabet. Using that alphabet, S<sub>0</sub> and S<sub>1</sub> are arrays of length four and S<sub>2</sub> is an array of length three. S<sub>0</sub>, S<sub>1</sub>, and S<sub>2</sub> thus have four, four, and three suffixes, respectively. S<sub>0</sub>’s suffixes are [ABR][ACA][DAB][RA$], [ACA][DAB][RA$], [DAB][RA$], [RA$], which clearly correspond to the suffixes ABRACADABRA, ACADABRA, DABRA, and RA in the original string S. In the original string S, these suffixes are located at indices 0, 3, 6, and 9, respectively, so looking at all three of S<sub>0</sub>, S<sub>1</sub>, and S<sub>2</sub>, we can see that each S<sub>i</sub> represents the suffixes that are located at indices i mod 3 in S.\n"
            ],
            "guid": "Ex#r4(*@S_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we do after generating the <b>tri-characters&nbsp;</b>for the&nbsp;<b>suffix arrays</b>&nbsp;of the string ABRACADABRA?",
                "Step 3: Concatenate S<sub>1</sub> and S<sub>2</sub> and recursively compute the suffix array. In order to compute this suffix array, we will need to sort the new alphabet of tri-characters. This can be done in linear time by three passes of radix sort, since the old characters were already sorted in step 1. If in fact all the tri-characters in the new alphabet are unique, then we do not even need to bother with a recursive call. Making three passes of radix sort takes linear time. If T(N) is the running time of the suffix array construction algorithm, then the recursive call takes T(2N/3) time.<br>Example:<br>In our example<br>S<sub>1</sub>S<sub>2</sub> = [BRA], [CAD], [ABR], [A$$], [RAC], [ADA], [BRA]<br>The sorted suffixes that will be computed recursively will represent tri-character strings.<br><img src=\"paste-f8800ab83180fbb26c810466f256d7bc2c620dcc.jpg\"><br>Notice that these are not exactly the same as the corresponding suffixes in S; however, if we strip out characters starting at the first $, we do have a match of suffixes. Also note that the indices returned by the recursive call do not correspond directly to the indices in S, though it is a simple matter to map them back. So to see how the algorithm actually forms the recursive call, observe that three passes of radix sort will assign the following alphabet: [A$$] = 1, [ABR] = 2, [ADA] = 3, [BRA] = 4, [CAD] = 5, [RAC] = 6. The figure below shows the mapping of tri-characters, the resulting array that is formed for S<sub>1</sub>, S<sub>2</sub>, and the resulting suffix array that is computed recursively.<br><img src=\"paste-ebca703d814df5026017db3210024c2a7bc267d4.jpg\">"
            ],
            "guid": "Qd+lhDe/4D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we do after mapping <b>tri-characters</b> when calculating <b>suffix arrays</b>&nbsp;for the string ABRACADABRA?<br><img src=\"paste-55e11d4b799768028208abc2214294eead9019dc.jpg\">",
                "Step 4: Compute the suffix array for S0. This is easy to do because:<br><img src=\"paste-26a80d1874f6083de2a6bbb343b4180b2cf8c63d.jpg\"><br>Since our recursive call has already sorted all S<sub>1</sub>[i →], we can do step 4 with a simple two-pass radix sort: The first pass is on S<sub>1</sub>[i →], and the second pass is on S<sub>0</sub>[i]. In our example S0 = [ABR], [ACA], [DAB], [RA$]<br>From the recursive call in step 3, we can rank the suffixes in S<sub>1</sub> and S<sub>2</sub>. The figure below shows how the indices in the original string can be referenced from the recursively computed suffix array and shows how the suffix array from the figure above leads to a ranking of suffixes among S<sub>1</sub> + S<sub>2</sub>. Entries in the next-to-last row are easily obtained from the prior two rows. In the last row, the ith entry is given by the location of i in the row labelled SA[S<sub>1</sub>, S<sub>2</sub>].\n<br><img src=\"paste-f137e8911897863666a51272a6299679e571e0bd.jpg\"><br>The ranking established in S<sub>1</sub> can be used directly for the first radix sort pass on S<sub>0</sub>. Then we do a second pass on the single characters from S, using the prior radix sort to break ties. Notice that it is convenient if S<sub>1</sub> has exactly as many elements as S<sub>0</sub>. The figure below shows how we can compute the suffix array for S<sub>0</sub>. At this point, we now have the suffix array for S<sub>0</sub> and for the combined group S<sub>1</sub> and S<sub>2</sub>. Since this is a two-pass radix sort, this step takes O( N ).\n<br><img src=\"paste-548e2866cb60fdd14bb1911f6be06810faea5840.jpg\">"
            ],
            "guid": ",Um5%)qky",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we do to the two suffix arrays generated as a part of calculating the&nbsp;<b>suffix array&nbsp;</b>for the string ABRACADABRA?",
                "Step 5: Merge the two suffix arrays using the standard algorithm to merge two sorted lists. The only issue is that we must be able to compare each suffix pair in constant time. There are two cases.<br>Case 1: Comparing an S<sub>0</sub> element with an S<sub>1</sub> element: Compare the first letter; if they do not match, we are done; otherwise, compare the remainder of S<sub>0</sub> (which is an S<sub>1&nbsp;</sub>suffix) with the remainder of S<sub>1</sub> (which is an S<sub>2</sub> suffix); those are already ordered, so we are done.<br>Case 2: Comparing an S<sub>0</sub> element with an S<sub>2</sub> element: Compare at most the first two letters; if we still have a match, then at that point compare the remainder of S<sub>0</sub> (which after skipping the two letters becomes an S<sub>2</sub> suffix) with the remainder of S<sub>2</sub> (which after skipping two letters becomes an S<sub>1 </sub>suffix); as in case 1, those suffixes are already ordered by SA12 so we are done.\n<br>In our example, we have to merge:<br><img src=\"paste-16cfbd27da8e3bbd928c57dbd60be34a11affb09.jpg\"><br>with<br><img src=\"paste-0586bd7d02607541187b8ac9e534790c3cee089e.jpg\"><br>The first comparison is between index 0 (an A), which is an S<sub>0</sub> element and index 10 (also an A) which is an S<sub>1</sub> element. Since that is a tie, we now have to compare index 1 with index 11. Normally this would have already been computed, since index 1 is S<sub>1</sub>, while index 11 is in S<sub>2</sub>. However, this is special because index 11 is past the end of the string; consequently it always represents the earlier suffix lexicographically, and the first element<br>in the final suffix array is 10. We advance in the second group and now we have.\n<br><img src=\"paste-2342e8d8fdb61f7085309799af12c8d088fd9c00.jpg\"><br>Again the first characters match, so we compare indices 1 and 8, and this is already computed, with index 8 having the smaller string. So that means that now 7 goes into the final suffix array, and we advance the second group, obtaining<br>\n<img src=\"paste-cab7beb9772de88b0a0bd5fe4396eb156cdfbd7a.jpg\"><br>Once again, the first characters match, so now we have to compare indices 1 and 6. Since this is a comparison between an S<sub>1</sub> element and an S<sub>0</sub> element, we cannot look up the result. Thus we have to compare characters directly. Index 1 contains a B and index 6 contains a D, so index 1 wins. Thus 0 goes into the final suffix array and we advance the first group.<br>\n<img src=\"paste-418e305a854e291d8a0408735c60ab3b9e6df56f.jpg\"><br>The same situation occurs on the next comparison between a pair of A’s; the second comparison is between index 4 (a C) and index 6 (a D), so the element from the first group advances.<br>\n<img src=\"paste-189b424d973e013c1eff29d109341c2d3adeb537.jpg\"><br>At this point, there are no ties for a while, so we quickly advance to the last characters of each group:<br>\n<img src=\"paste-a510c39f639951e7326af12ca62c31167ec826d0.jpg\"><br>Finally, we get to the end. The comparison between two R’s requires that we compare the next characters, which are at indices 10 and 3. Since this comparison is between an S<sub>1&nbsp;</sub>element and an S<sub>0</sub> element, as we saw before, we cannot look up the result and must compare directly. But those are also the same, so now we have to compare indices 11 and 4, which is an automatic winner for index 11 (since it is past the end of the string). Thus<br>the R in index 9 advances, and then we can finish the merge. Notice that had we not been at the end of the string, we could have used the fact that the comparison is between an S<sub>2&nbsp;</sub>element and an S<sub>1</sub> element, which means the ordering would have been obtainable from the suffix array for S<sub>1</sub> + S<sub>2</sub>.<br>\n<img src=\"paste-8bc818db865ebfee26473515a6fea4d404e5f818.jpg\">"
            ],
            "guid": "rxFVMc~)Iw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the following problem known as?<br>\"Suppose that an advertising company maintains a database and needs to generate mailing labels for certain constituencies. A typical request might require sending out a mailing to people who are between the ages of 34 and 49 and whose annual income is between $100,000 and $150,000.\"&nbsp;\n",
                "This problem is known as a <b>two-dimensional range query</b>.\n"
            ],
            "guid": "cF#DDQ4;ip",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bounds for a&nbsp;<b>one-dimensional range query</b>?",
                "The problem can be solved by a simple recursive algorithm in O(M + log N) average time, by traversing a preconstructed binary search tree. Here M is the number of matches reported by the query.&nbsp;\n"
            ],
            "guid": "PsjKp>cjq#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of <b>two-dimensional search trees</b>?",
                "The two-dimensional search tree has the simple property that branching on odd levels is done with respect to the first key, and branching on even levels is done with respect to the second key. The root is arbitrarily chosen to be an odd level. <br><img src=\"paste-9cc0449def2ad66080fe40811c474509f7a16049.jpg\"><br>Insertion into a 2-d tree is a trivial extension of insertion into a binary search tree: As we go down the tree, we need to maintain the current level. To keep our code simple, we assume that a basic item is an array of two elements. We then need to toggle the level between 0 and 1. <br><img src=\"paste-e548184e3400ee1010f08e3be4e6b9daa0585f9a.jpg\"><br>We use recursion in this section; a nonrecursive implementation that would be used in practice is straightforward. One difficulty is duplicates, particularly since several items can agree in one key. Our code allows duplicates, and always places them in right branches; clearly this can be a problem if there are too many duplicates.<br>A moment’s thought will convince you that a randomly constructed 2-d tree has the same structural properties as a random binary search tree: The height is O(log N) on average, but O(N) in the worst case.\n<br>Unlike binary search trees, for which clever O(log N) worst-case variants exist, there are no schemes that are known to guarantee a balanced 2-d tree. The problem is that such a scheme would likely be based on tree rotations, and tree rotations don’t work in 2-d trees. The best one can do is to periodically rebalance the tree by reconstructing a subtree, as described in the exercises. Similarly, there are no deletion algorithms beyond the obvious lazy deletion strategy. If all the items arrive before we need to process queries, then we can construct a perfectly balanced 2-d tree in O(N log N) time."
            ],
            "guid": "um+L_<y2W)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What kinds of queries are possible on a <b>2-d tree</b>?",
                "We can ask for an exact match or a match based on one of the two keys; the latter type of request is a <b>partial match query</b>.<br>Both of these are special cases of an (orthogonal) <b>range query</b>."
            ],
            "guid": "K|EW-96,c|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>orthogonal range query</b>?",
                "It gives all items whose first key is between a specified set of values and whose second key is between another specified set of values. This is exactly the problem that was described in the introduction to this section. A range query is easily solved by a recursive tree traversal. By testing before making a recursive call, we can avoid unnecessarily visiting all nodes.<br>\n<img src=\"paste-4024b838fe787867c329d557229fef38d3ca19de.jpg\"><br>To find a specific item, we can set low equal to high equal to the item we are searching for. To perform a partial match query, we set the range for the key not involved in the match to -∞ to ∞. The other range is set with the low and high point equal to the value of the key involved in the match.\n"
            ],
            "guid": "A58%qls$vY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the time bouds of an insertion or exact match search in a <b>2-d tree</b>?",
                "An insertion or exact match search in a 2-d tree takes time that is proportional to the depth of the tree, namely, O(log N) on average and O(N) in the worst case. The running time of a range search depends on how balanced the tree is, whether or not a partial match is requested, and how many items are actually found. We mention three results that have been shown.<br>For a perfectly balanced tree, a range query could take O(M + √N) time in the worst case to report M matches. At any node, we may have to visit two of the four grandchildren, leading to the equation T(N) = 2T(N/4) + O(1). In practice, however, these searches tend to be very efficient, and even the worst case is not poor because for typical N, the difference between √N and log N is compensated by the smaller constant that is hidden in the Big-Oh notation.<br>For a randomly constructed tree, the average running time of a partial match query is O(M + Nα), where α = (-3 + √17)/2 (see below). A recent, and somewhat surprising, result is that this essentially describes the average running time of a range search of a random 2-d tree.\n"
            ],
            "guid": "g*4rg;zEtr",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the considerations of a k-dimensional tree when compared to a 2-dimensional one?",
                "For k dimensions, the same algorithm works; we just cycle through the keys at each level. However, in practice, the balance starts getting worse because typically the effect of duplicates and nonrandom inputs becomes more pronounced. For a perfectly balanced tree, the worst-case running time of a range query is O(M + kN<sup>1-1/k</sup>). In a randomly constructed k-d tree, a partial match query that involves p of the k keys takes O(M + N<sup>α</sup>), where α is the (only) positive root of (2 + α)<sup>p</sup>(1 + α)<sup>k-p</sup> = 2<sup>k<br></sup>\nComputation of α for various p and k is left as an exercise; the value for k = 2 and p = 1 is reflected in the result stated above for partial matching in random 2-d trees. Although there are several exotic structures that support range searching, the k-d tree is probably the simplest such structure that achieves respectable running times.\n"
            ],
            "guid": "m:E3y>j]6k",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a&nbsp;<b>pairing heap</b>&nbsp;compare to other heap structures?",
                "The analysis of the pairing heap is still open, but when decreaseKey operations are needed, it seems to outperform other heap structures. The most likely reason for its efficiency is its simplicity. The pairing heap is represented as a heap-ordered tree.<br>\n<img src=\"paste-a17b89881dd1d1256a09f35ea7c50c077ad1774c.jpg\">"
            ],
            "guid": "z.EAT8J{La",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is the following heap represented as a&nbsp;<b>pairing heap</b>?<br><img src=\"paste-e6422bb0d09f13b99ec89f7b5ca54c1b21f8f85d.jpg\">",
                "The actual pairing heap implementation uses a left child, right sibling representation. The decreaseKey operation, as we will see, requires that each node contain an additional link. A node that is a leftmost child contains a link to its parent; otherwise the node is a right sibling and contains a link to its left sibling. We’ll refer to this data member as prev. The class skeleton and pairing heap node declaration are omitted for brevity; they are completely straightforward. The figure below shows the actual representation of the pairing heap above.<br>\n<img src=\"paste-9edcb5a74ff9c8c568285eda8dd0ca3a6710657b.jpg\">"
            ],
            "guid": "eM-{4HpcVU",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we merge two&nbsp;<b>pairing heaps</b>?",
                "To merge two pairing heaps, we make the heap with the larger root a left child of the heap with the smaller root. Insertion is, of course, a special case of merging. To perform a decreaseKey, we lower the value in the requested node. Because we are not maintaining parent pointers for all nodes, we don’t know if this violates the heap order. Thus we cut the adjusted node from its parent and complete the decreaseKey by merging the two heaps that result. To perform a deleteMin, we remove the root, creating a collection of heaps. If there are c children of the root, then c - 1 calls to the merge procedure will reassemble the heap. The most important detail is the method used to perform the merge and how the c - 1 merges are applied.<br>\n<img src=\"paste-91fb6ac845ac09ba3d94afaa85ef64d529b04889.jpg\"><br>The procedure is generalized to allow the second subheap to have siblings. As we mentioned earlier, the subheap with the larger root is made a leftmost child of the other subheap.\n"
            ],
            "guid": "s)AYr0kr9R",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>two-pass merging</b>&nbsp;for&nbsp;<b>pairing heaps</b>.",
                "It is the simplest and most practical of the many variants that have been suggested for combining siblings. We first scan left to right, merging pairs of children.&nbsp;After the first scan, we have half as many trees to merge. A second scan is then performed, right to left. At each step we merge the rightmost tree remaining from the first scan with the current merged result. As an example, if we have eight children, c<sub>1</sub> through c8, the first scan performs the merges c<sub>1</sub> and c<sub>2</sub>, c<sub>3</sub> and c<sub>4</sub>, c<sub>5</sub> and c<sub>6</sub>, and c<sub>7</sub> and c<sub>8</sub>. As a result, we obtain d<sub>1</sub>, d<sub>2</sub>, d<sub>3</sub>, and d<sub>4</sub>. We perform the second pass by merging d<sub>3</sub> and d<sub>4</sub>; d<sub>2</sub> is then merged with that result, and then d<sub>1</sub> is merged with the result of the previous merge.<br>Our implementation requires an array to store the subtrees. In the worst case, N - 1 items could be children of the root, but declaring a (non-static) array of size N inside of combineSiblings would give an O(N) algorithm. So we use a single expanding array instead. Because it is static, it is reused in each call, without the overhead of reinitialization.\n"
            ],
            "guid": "i2Id[&O&D7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        }
    ]
}