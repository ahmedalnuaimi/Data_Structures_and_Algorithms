{
    "__type__": "Deck",
    "children": [],
    "crowdanki_uuid": "2ac8a044-6e46-11ec-9eba-18c04df1fa07",
    "deck_config_uuid": "2ac8a045-6e46-11ec-adf0-18c04df1fa07",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoplay": true,
            "crowdanki_uuid": "2ac8a045-6e46-11ec-adf0-18c04df1fa07",
            "dyn": false,
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 0,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "5 per day",
            "new": {
                "bury": true,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 5,
                "separate": true
            },
            "newGatherPriority": 0,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "replayq": true,
            "rev": {
                "bury": true,
                "ease4": 1.3,
                "fuzz": 0.05,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "minSpace": 1,
                "perDay": 100
            },
            "reviewOrder": 0,
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [
        "paste-02088ec5250f2c0c58a5a71ea2db2de66c4a419f.jpg",
        "paste-0361f4901f8e7233ddc5f94beefada26779e08e4.jpg",
        "paste-0689df9309716c497c8d2c47888d56626a9f111e.jpg",
        "paste-07e070a38dc5b94908d78e90970a2c85f148f1a1.jpg",
        "paste-07f33a00b349b29d8793850932732e3b650a8f31.jpg",
        "paste-085e451dba547532239c93eb14e90dd6e17b7bcb.jpg",
        "paste-091b2dc77c1ea227303a50ab376e0c6c88af5112.jpg",
        "paste-0a589a7abadbdaebcd06f45344dbbf3e8de900d5.jpg",
        "paste-0b92770d84c2c2ca44a7d58cdd0ab83695b346d1.jpg",
        "paste-0c7f90d442aa367deca741aeca767f438140f62b.jpg",
        "paste-0f8d9ec3dbd035273ebc6c4d7d998110aacaff01.jpg",
        "paste-1019cffdc21d039085adc66be5b24a928060db22.jpg",
        "paste-12f3a7e6cc65fa617ab9239a27b31c0a0bb167e8.jpg",
        "paste-138eeba44a90e7cb5959060753c99daa3f8df68a.jpg",
        "paste-14f64e311620cf3515281c2b84a6b9fca61eb254.jpg",
        "paste-152e70520fb2a034923e9d517afcdb5983babe79.jpg",
        "paste-15c1861be7d9e204ebda7c33baf4c216925e9126.jpg",
        "paste-16fe74b80b60d6ad24d282cdccfcc6f34223e0cf.jpg",
        "paste-1773f101a1f0ea9c1655db9709db9f5b68d0fc4e.jpg",
        "paste-17866d9c05d567f5b8aab12cef9e74a9604c2d4e.jpg",
        "paste-181f50b65b3affa7adc52bb6aff32958c3dd24a2.jpg",
        "paste-1a3a3e924c2529e0d3ae6665efac1ed9e27c9040.jpg",
        "paste-1a59eee151ca60f3537025c1eec3f001ce7b1b16.jpg",
        "paste-1b2677abb3718cee7d30cd7f918b9f0dc2058e7a.jpg",
        "paste-1d1112407221c3dcc2a0fac54938d5d1e40963ea.jpg",
        "paste-1d8b032be31297c586ad5c1164bd0dfddbade2ec.jpg",
        "paste-217ebe08300a935500b838ef8a1437350902853b.jpg",
        "paste-21ea0184d8b75a195a2743d8e6aada2dc0271859.jpg",
        "paste-237767969ba2af0d632b596cf4807653c7968c4a.jpg",
        "paste-239d9916b6aba59b35f1f5bb4dcdd8afdd6ca6df.jpg",
        "paste-252f1f33951d320e152e8fc4f9722865ee6e09e0.jpg",
        "paste-25950947b1193801a715541288690e4ea087d281.jpg",
        "paste-25b4262438225c1a4904f9c40f8e610451902b88.jpg",
        "paste-294e9e4c42d34e7e10d1e612f669fd6a65f7ddfe.jpg",
        "paste-297dd7624892ca12ddf629143366e2f8d1c4d8ac.jpg",
        "paste-2b624bc74bcabf69354245109a1ee8118c3d4cdb.jpg",
        "paste-2b6d43694ad888ce4151e2888746d9f2f9952e72.jpg",
        "paste-2f4f0fa840ba16db307f5e7698a7b0fab753580d.jpg",
        "paste-2f7652a98b89ce1f1e8b4f09806ef411108d5590.jpg",
        "paste-304edf77b40cd69e88519ef4305ec8031f846847.jpg",
        "paste-30963476b82950f7b2013dd41ecb653605a76af4.jpg",
        "paste-317cc996cd21a24e3569047e5b49a1b0e711b2fa.jpg",
        "paste-3247dea264178f5d22e247946aa1bfca23fc5bfa.jpg",
        "paste-3328f29f8fbae67b793800c5aaf275c9f6b419f8.jpg",
        "paste-33c4a58f14918968a81a7d1a077a91c3746a3c23.jpg",
        "paste-3425bbeea1747147eafd63ef514039faf2ba9f19.jpg",
        "paste-3555a7924e0d98470a5742f4d5329ee08cea8d3a.jpg",
        "paste-3769af4173894b3f6570ee5d6055100c042181d1.jpg",
        "paste-3ab4c907cd64f3c98d7e8486a01b40b24490ae40.jpg",
        "paste-3acaddd2a21ffbf8d1e25b6029c2a82d765b1590.jpg",
        "paste-3b34a29d8f42307be988c1ae4f9d00b888ac8aec.jpg",
        "paste-3c7b264556f08af3bdb44584095b2fd85c2ca0db.jpg",
        "paste-3d31de6c9e08eba3e99436ef3a4670c1695aa9d2.jpg",
        "paste-3d570dea05c02022deb071443e38b9efd805201a.jpg",
        "paste-403714400f63227de204855151ee2448cb59f6b7.jpg",
        "paste-42ca6ae3f94b1aab2f918c407b03d1eb87a22d67.jpg",
        "paste-43d9956fcd57c2e2459153d6306d0a554ba06031.jpg",
        "paste-4417a05617c84f5ec6d53b092b3ee788add37f83.jpg",
        "paste-44e816d412d434c74406f217b38f8056e78f4f1f.jpg",
        "paste-451428472900b303aadce2d023f2e69744d73d1a.jpg",
        "paste-457b62ee8e12e5c8aed91c0f1ee159930cc655c3.jpg",
        "paste-45ccbfdf0ff9a07d832050b595e3cabd4a9e3103.jpg",
        "paste-47afc056d248665879e4b7ebd51d16ba73463b97.jpg",
        "paste-482a887dd52a089d02bbac5cee63a8ef89aaa8b5.jpg",
        "paste-48c72e2bd7fcaa1e3ca1123c2f664a2f98e163be.jpg",
        "paste-4a5cd15f5efd5d6e9747aed1bfdfa65c1ba9fc97.jpg",
        "paste-4b526a7803daa2d53b8d65b68011d4e7a4e1868d.jpg",
        "paste-4d7d8a7f32e406a237944f3d2441c8209362db22.jpg",
        "paste-4d93a61a959ebebddcacddf9c400e114160c7004.jpg",
        "paste-4df8a6fac3ee8ff605b2587946a6a1fc2eacc76d.jpg",
        "paste-4ff51f5cbde13fe4ada98cb70b204f67f7312e7d.jpg",
        "paste-506cb1143e1ca5a2f34884d4d79cb9ae27c29f77.jpg",
        "paste-52605f4aa09c0442e5a62a1732730ad3cda84c58.jpg",
        "paste-54045d30f572c478b1e0ca22fe7174d914d20dad.jpg",
        "paste-54b9f2aa2a98dcd6f5a6c931853fd147a9e34b8b.jpg",
        "paste-58b21ec371ffde421dc7aaad4f8c3f13e309c60a.jpg",
        "paste-58c77b2aea46543e4080a8b4b0a83959d25a4ad7.jpg",
        "paste-58c7f2867f1e1ad7f827df5bc8422c9ace7ab89b.jpg",
        "paste-599e081c9312f6717ea86e336f828e1e0a8b0658.jpg",
        "paste-5baf2e4075678e9bcee1aac407046b85f241d7cb.jpg",
        "paste-5e624e0405e2d43b055ff6b7555d216fd1473d81.jpg",
        "paste-61808f2da8a62723216eb4bb649e3c6a7eec98ca.jpg",
        "paste-63996316413844fd2e89e376fb4f25f9ce1cf015.jpg",
        "paste-657956ac05a39f3da757204cdae689ab6aa220be.jpg",
        "paste-6618833ff5f21027e6a65c66038288137dad1e7e.jpg",
        "paste-664f58cf5b203e5c3c013f1b81974a550a554747.jpg",
        "paste-67067edfbf3988d40e6df9067564e6b098cb2ef6.jpg",
        "paste-6869b08b3c8739dd047ae93cdb095a962e991382.jpg",
        "paste-6bd4b2dd58a6ddbb0e3340d175173adaf0a4ebb5.jpg",
        "paste-72fcfd247d80544e3dfa7a5f5eaecd86313786cd.jpg",
        "paste-737ea761d23452b9f711d8f6cfb4b5590f0c863f.jpg",
        "paste-738c435a7a53bcb9399576d5c709440d3c2a26bc.jpg",
        "paste-738d4a0f2e0f403309fb844cba67dce125eee98d.jpg",
        "paste-73c77fb390f3082d2b4a28bd834ef1cc47b924db.jpg",
        "paste-755c925a62bfc10b6ed08fa50443777f99fa551d.jpg",
        "paste-76f6c98beee0811b370ab4201ee6b1b6088ad884.jpg",
        "paste-7780261789fd30bc60d1aa75bd7935bbe972ac36.jpg",
        "paste-782a3499ad72473942795f70ea31f726f7a7f6f0.jpg",
        "paste-7b42642087ca77b8ce7a1a0796b5e802ba1fd873.jpg",
        "paste-7d2051b8ac2b10122dc64d304e395c3a17ed7dd9.jpg",
        "paste-8185a09237df01bfa309c5670ae016a5c89da1e7.jpg",
        "paste-82a2d8672aa3c143217af2b898eeacbcfe7905ee.jpg",
        "paste-82aada586bf6d3142f9365acc8f03f13b852596b.jpg",
        "paste-830fcd81093a7670a3140a1de9bea91b34b571d5.jpg",
        "paste-838febabac9024fa3e6436eb3936e233a7540fa4.jpg",
        "paste-83e52836349ced9e73bcc3a2886148228c1ee7cd.jpg",
        "paste-85df9bb180cfc08df6e8d8ab3bcedb63a460dc6b.jpg",
        "paste-87776b5f92b11e71cc42e802fe70751b1fa7886d.jpg",
        "paste-87fcc822dc9ddd16c1dd90cabf81c7b2cd9bbf90.jpg",
        "paste-8944a416c2aff3101e8de7ba91df5eb68e35932a.jpg",
        "paste-8c3366c4c273a57f0546432ae3c0ce7b47fe1b32.jpg",
        "paste-8ce8ae8a5c56ba3a06f0ef5f70b9f99f9774dbc4.jpg",
        "paste-8e804226c87e81ef0ae579d897618d88159d2fd8.jpg",
        "paste-8ef2fbc44c81120e0c6247cc1ea09721ab957da7.jpg",
        "paste-8ffb06ee34372d426d7cf273de7fdd1e58f5b9d5.jpg",
        "paste-903a902b305b6352c36549a4f9e3c471e8bc54d8.jpg",
        "paste-91103479777465e4fdeb255e7b0eebaeec1a30de.jpg",
        "paste-91a2711d28eed5b2e0808c5a38ad33fa21df5ff8.jpg",
        "paste-931e71a5385ce9c9266c2573c9037f48e51822f7.jpg",
        "paste-973c3547647fe0bcde1206b14fcafab34de4e4b0.jpg",
        "paste-9b37081e9fd399acab9728744b6c7f4689c5efb4.jpg",
        "paste-9bb862de47a6f79008059b54abc38c38cbfcb6ba.jpg",
        "paste-9e186cf04192205626e56fee41b87802d390944e.jpg",
        "paste-9f97deafddb34a4d4a7b617ff14735b8dc724dfb.jpg",
        "paste-a11a6986fac3d7e1674504516a6c8f7ae1a03d9f.jpg",
        "paste-a2a51799c5613b8b570d6ecbad2c8cb1eab3fe29.jpg",
        "paste-a5d102b03a54da8e83d66ea65e5d696f73c78bb9.jpg",
        "paste-a6916e3524fa8b221406150c1bfe4ba991b05208.jpg",
        "paste-a7e62a120818080a897d70243488c01b3c60159b.jpg",
        "paste-a8472babb1400070a6e3fc88c4e23e93b0f6f256.jpg",
        "paste-ab4f56d3d662b0d8adaf1725fc24d46e908fbda4.jpg",
        "paste-ad2b64415d5ed9a3da90cacf77ed1e7d8baf2c42.jpg",
        "paste-b02ddc574bcd3b6c9c3729e1bc5d8dddd418af91.jpg",
        "paste-b52bfdd9f0120194a607f158e4f12b9b7456ac10.jpg",
        "paste-b787a56dae16ba09df0eb0a52952d2fbc1df94d3.jpg",
        "paste-b8cc3746e416849bba85b54c74221dfa8a28038d.jpg",
        "paste-b9f6668a2b1d50faac7e4d76ad781d4065689060.jpg",
        "paste-ba5aa4733fa77deb9fc04a339f78c672d5ada772.jpg",
        "paste-bb7204bb9d0e4afb17de88c08de0e0d33468ee01.jpg",
        "paste-bd40421692b51039e97b050744017ef77be4b7c4.jpg",
        "paste-be4e723174991ade35931612a711a714d14ca119.jpg",
        "paste-bec8bb8f3d76d5e94512ff2af9266de35b9bf0f1.jpg",
        "paste-bf409e64da466adc427ef0c6bbc48625f0fea0bd.jpg",
        "paste-c24a64ed279d59ca0371c0b707e98ea0fe010904.jpg",
        "paste-c3ece164e96901bb96c7095fb58bfad0ad9721d9.jpg",
        "paste-c67e6489b051b882d1321ed022ed27f8fb02099a.jpg",
        "paste-caf8b352f892a58b68a9e7b4ad46bd79b4601262.jpg",
        "paste-cb4c91bbb05e86bff147aad9a00eba584049bbeb.jpg",
        "paste-cd90fe01f6286f6090f9c822ae1bffc81b3217f5.jpg",
        "paste-cef2cc7a7f38f1000eb7ad0e9dc539c79d4d4f5b.jpg",
        "paste-d020223182cf852dfaf0421f11d742d1a6ace915.jpg",
        "paste-d1fed0acf4019f8220b18c977e226cb8bc0da754.jpg",
        "paste-d8032a3f82abcab07b4450da7cc0aa5bb34bb74a.jpg",
        "paste-d806069343b9645943b424d12f6de09a5780e059.jpg",
        "paste-dad1f31867490f93a1aed8be5274a5a74e92b0ee.jpg",
        "paste-dd5b8762ffb3881b64c53fb766ecc4464a913f70.jpg",
        "paste-e0a4be7b9804895b5ef5f1c3f6b7a615b3c2fe09.jpg",
        "paste-e1d7f5dd1983461c2c5b91de785fe0d78dc7bba9.jpg",
        "paste-e47f2bd70eed027f29a4e1a7b9380fabd4c0008f.jpg",
        "paste-e57d18e37aba7192630e5696bed02dddaf6bd61e.jpg",
        "paste-e5b9d5be1e4aa03baaaab156b48896cb6d36aaa3.jpg",
        "paste-e5e3b9da0df9c8bf4a2cc600b2530c17aa65f2ae.jpg",
        "paste-e5e4b96f5b0eede1f0f37e5f8990c46e7555e879.jpg",
        "paste-ea8f40d52f5be8c12c22f202563ce9d1f9845b21.jpg",
        "paste-ecb98386498b5511cdf857a7bab1e676112fbd5c.jpg",
        "paste-eddb3c7a16cb90abc3bca2d3c86eb42a17a95330.jpg",
        "paste-eeca0f36d8f4f60ea0a701e1ae371587d4a47451.jpg",
        "paste-f14bc5e16577f912a4939e6d87d4d1751328634b.jpg",
        "paste-f365a5f98e8d9e4dffd0def3b7cdc75697fd27fc.jpg",
        "paste-f3b0d5def21168abf7f0ddefa23018cd8e4bcba7.jpg",
        "paste-f4629e6ecef9fd0f4a560eda513f742a7e898788.jpg",
        "paste-f5dc75ba051a99e6cda769613ce5dcbd62bd33f6.jpg",
        "paste-f8ed8289dd5f5db2ddd3e85afc10c84ad6e0bc7c.jpg",
        "paste-fab6fadd9d2ea5b29bb71abf162638fd40021857.jpg",
        "paste-fba65d001b8e1a9d997ba993bd4509122620b436.jpg",
        "paste-fc20d98b336d1c6124d0f423d7f44ec2defc2908.jpg",
        "paste-fcd4ea04e9fb20e6fec8937d1ce6ab1e6fcab292.jpg",
        "paste-fcfc8f72cc2daf428023c2718a96fa9e8101440a.jpg",
        "paste-fd02b48b8683d7e584dfc6f659f9ec46c48ffaf7.jpg",
        "paste-fdbe8b903808c2cd9cb1ecf5f65c08658ac92fc4.jpg",
        "paste-fed19aea15e6b3acbb499478e1d6c7c0016b53d1.jpg"
    ],
    "name": "Data Structures and Algorithms",
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n",
            "flds": [
                {
                    "description": "",
                    "font": "Arial",
                    "media": [],
                    "name": "Frente",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "description": "",
                    "font": "Arial",
                    "media": [],
                    "name": "Verso",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Básico",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tags": [],
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Verso}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "name": "Cartão 1",
                    "ord": 0,
                    "qfmt": "{{Frente}}"
                }
            ],
            "type": 0,
            "vers": []
        }
    ],
    "notes": [
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-e0a4be7b9804895b5ef5f1c3f6b7a615b3c2fe09.jpg\">",
                "<img src=\"paste-43d9956fcd57c2e2459153d6306d0a554ba06031.jpg\">"
            ],
            "guid": "r5lAd@DP1Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-b9f6668a2b1d50faac7e4d76ad781d4065689060.jpg\">",
                "<img src=\"paste-f8ed8289dd5f5db2ddd3e85afc10c84ad6e0bc7c.jpg\">"
            ],
            "guid": "dazGr+$epu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-1019cffdc21d039085adc66be5b24a928060db22.jpg\">",
                "<img src=\"paste-3247dea264178f5d22e247946aa1bfca23fc5bfa.jpg\">"
            ],
            "guid": "i/=?Dj,]n2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-54045d30f572c478b1e0ca22fe7174d914d20dad.jpg\">",
                "<img src=\"paste-8185a09237df01bfa309c5670ae016a5c89da1e7.jpg\">"
            ],
            "guid": "fwJlek[Ul:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-506cb1143e1ca5a2f34884d4d79cb9ae27c29f77.jpg\">",
                "<img src=\"paste-830fcd81093a7670a3140a1de9bea91b34b571d5.jpg\">"
            ],
            "guid": "GCK7-N~|~Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-1b2677abb3718cee7d30cd7f918b9f0dc2058e7a.jpg\">",
                "<img src=\"paste-c67e6489b051b882d1321ed022ed27f8fb02099a.jpg\">"
            ],
            "guid": ">=vy6hb2o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-07e070a38dc5b94908d78e90970a2c85f148f1a1.jpg\">",
                "<img src=\"paste-1d1112407221c3dcc2a0fac54938d5d1e40963ea.jpg\">"
            ],
            "guid": "B2gTE3|{AD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-3ab4c907cd64f3c98d7e8486a01b40b24490ae40.jpg\">",
                "<img src=\"paste-e57d18e37aba7192630e5696bed02dddaf6bd61e.jpg\">"
            ],
            "guid": "k}>UB_K*Xv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<img src=\"paste-58b21ec371ffde421dc7aaad4f8c3f13e309c60a.jpg\">",
                "<img src=\"paste-0a589a7abadbdaebcd06f45344dbbf3e8de900d5.jpg\">"
            ],
            "guid": "c%IE`9~GE7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Recursive routine to print an integer&nbsp;",
                "<img src=\"paste-755c925a62bfc10b6ed08fa50443777f99fa551d.jpg\">"
            ],
            "guid": "ILY;3Njnbv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the time complexity of binary search?",
                "O(log N)"
            ],
            "guid": "M<p=D4]vf}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of array as list",
                "Insert: O(N) at the beginning and O(1) at the end<br>Delete: O(N) at the beginning and O(1) at the end<br>Access: O(1)"
            ],
            "guid": "D_{.dlq$=x",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of simple linked list as list",
                "Insert: O(1) for head, tail, and known location. Otherwise O(i)<br>Delete: O(1) for head, tail, and known location. Otherwise O(i)<br>Find i: O(i)"
            ],
            "guid": "A)]3ChV{Si",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Properties of stack",
                "Usually implemented using array<br>Insert (push): O(1)<br>Delete (pop): O(1)<br>Access (peek): O(1)"
            ],
            "guid": "s]b%IX8+hW",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you check for balancing symbols?<br>Ex: The sequence [()] is legal, but [(]) is wrong&nbsp;",
                "The simple algorithm uses a stack and is as follows:<br>Make an empty stack. Read characters until end of file. If the character is an opening<br>symbol, push it onto the stack. If it is a closing symbol and the stack is empty, report<br>an error. Otherwise, pop the stack. If the symbol popped is not the corresponding<br>opening symbol, then report an error. At end of file, if the stack is not empty, report an<br>error&nbsp;"
            ],
            "guid": "A8U|k~9qGB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you evaluate postfix expressions?<br>Ex: 4.99 1.06 ∗ 5.99 + 6.99 1.06 ∗ +&nbsp; is the postfix expression of 4.99 ∗ 1.06 + 5.99 + 6.99 ∗ 1.06&nbsp;",
                "Keep on pushing numbers to stack and when a mathematical symbol is encountered pop two values, evalute the result and push it to the stack.<br>Continue reading from the expression until you have one number in the stack."
            ],
            "guid": "JSMSLu9qpx",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you convert infix to postfix expression?<br>Ex: a + b * c + ( d * e + f ) * g&nbsp; &nbsp; to&nbsp; &nbsp; a b c * + d e * f + g * +&nbsp;",
                "Create a stack for storing mathematical symbols and an array for numbers<br>Read numbers from infix and add them to the array. When a symbol is encountered push it to the stack.<br>If the read symbol has a lower precedence than the symbol on the top of the stack, pop all the symbols and add them to the output, and then push the read symbol to the stack.<br>Treat parantheses as symbols of highest precedence.<br>Parantheses are not output until the closing one is encountered. When that happens, output all the symbols till the opening parantheses.<br>When the end is reached pop all the remaining symbols and add them to output."
            ],
            "guid": "Gr+%+P-P2[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implementation of queues",
                "Circular buffer using arrays<br>Insert (enqueue): O(1)<br>Delete (dequeue): O(1)<br>Access (peek): O(1)"
            ],
            "guid": "Hh*WD5Afa{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average running time of most operations in binary search trees?",
                "O(log N)"
            ],
            "guid": "t3+h,Gd9C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call A?<br>What is the relationship between A and E?<br>What is the relationship between J and E?<br>What do you call the connection between A and E?<br><img src=\"paste-76f6c98beee0811b370ab4201ee6b1b6088ad884.jpg\">",
                "A is the tree's root<br>A is E's parent node<br>J is E's child note<br>The connection between them is an edge"
            ],
            "guid": "Q$1P1@lD<8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call P &amp; Q?<br>What is the relationship between I &amp; J?<br>What is the relationship between E &amp; Q?<br>What is the relationship between Q &amp; E?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "P &amp; Q are leaves because they have no children<br>I &amp; J are siblings<br>E is Q's grandparent<br>Q is E's grandchild"
            ],
            "guid": "Jp_sM%;(Ne",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call all the nodes between A and Q?<br>How do you define its length?<br>What is a node's depth?<br>What is a node's height?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "The nodes between A &amp; Q are called the path between them.<br>The length is the number of edges on the path.<br>A node's depth is the length of a unique path to the root.<br>A node's height is the length of the longest path between the node and a leaf."
            ],
            "guid": "BQ#9fkXV[,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between A &amp; Q?<br>What is the relationship between Q &amp; A?<br>Why is the relationship between them is considered to be proper?<br><img src=\"paste-a7e62a120818080a897d70243488c01b3c60159b.jpg\">",
                "A is an ancestor of Q.<br>Q is a descendant of A.<br>A is a proper ancestor of Q because A!=Q since each node can be its own ancestor and descendant."
            ],
            "guid": "x81oXD`|]t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which tree implementation reduces wasted space?",
                "<img src=\"paste-8ce8ae8a5c56ba3a06f0ef5f70b9f99f9774dbc4.jpg\"><br>Using linked-lists for siblings and connecting the parent to the first child only, avoiding the need for an array in the parent that point to each child."
            ],
            "guid": "szRjEjYz_o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is preorder traversal?",
                "In a preorder traversal, work at a node is performed before (pre) its children are processed. Ex: directory names are printed before the files inside them."
            ],
            "guid": "sloEhJ8_eQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is postorder traversal?",
                "In a postorder traversal, the work at a node is performed after (post) its children are evaluated.&nbsp;Ex: print file sizes before directory size."
            ],
            "guid": "AMMtnBcNHL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average depth of a binary tree?",
                "O(√N)&nbsp;"
            ],
            "guid": "M(}Skt2*S0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average depth of a binary search tree?",
                "O(log N)&nbsp;"
            ],
            "guid": "saN%I+T#9P",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum depth of a binary tree?",
                "N-1"
            ],
            "guid": "Q$c6}xE0A/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an expression tree?",
                "The leaves of an expression tree are operands, such as constants or variable names, and the other nodes contain operators. This particular tree happens to be binary, because all the operators are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator.&nbsp;We can evaluate an expression tree, T, by applying the operator at the root to the values&nbsp;obtained by recursively evaluating the left and right subtrees.\n<br><img src=\"paste-82aada586bf6d3142f9365acc8f03f13b852596b.jpg\">"
            ],
            "guid": "n~(/V5q:zK",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which kind of traversal is used to convert an expression tree to infix expression?",
                "We can produce an (overly parenthesized) infix expression by recursively producing a parenthesized left expression, then printing out the operator at the root, and finally recursively producing a parenthesized right expression. This general strategy (left, node, right) is known as an inorder traversal; it is easy to remember because of the type of expression it produces."
            ],
            "guid": "h=`,D0m;wh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you construct a tree out of the postfix expression?",
                "We read our expression one symbol at a time. If the symbol is an operand, we create a one-node tree and push a pointer to it onto a stack. If the symbol is an operator, we pop (pointers) to two trees T1 and T2 from the stack (T1 is popped first) and form a new tree whose root is the operator and whose left and right children point to T2 and T1, respectively. A pointer to this new tree is then pushed onto the stack&nbsp;"
            ],
            "guid": "u%Yp)]]kd!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When a binary tree is considered to be a binary search tree?",
                "The property that makes a binary tree into a binary search tree is that for every node, X, in the tree, the values of all the items in its left subtree are smaller than the item in X, and the values of all the items in its right subtree are larger than the item in X.&nbsp;"
            ],
            "guid": "cwHxS[]Yk4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What kind of recursion can be replaced easily with a while loop?",
                "Tail recursion"
            ],
            "guid": "oL^3uFjx}I",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an alternative for inserting duplicates in a binary search tree?",
                "Including the frequency of occurence in the node instead of adding new child node which tend to make the tree very deep.<br>If counting the frequency is not enough we can use an auxiliary data structure to store all the duplicate instances in the node, such as a list or another tree."
            ],
            "guid": "p5fOSi@?Cv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find the minimum and maximum in a binary search tree?",
                "The minimum is found by recursively traversing the left item tell the left item of the current node is null.<br>The maximum is found the same way traversing to the right instead."
            ],
            "guid": "rjPBi_LYf/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you remove a node in a BST?",
                "1- If the node is a leaf we can just remove it<br>2- If the node has one child we bypass it by making the nod's parent point to to the node's child<br>3- If the node has two children we need to replace it with the smallest (left-most) child in the right sub-tree.<br><img src=\"paste-091b2dc77c1ea227303a50ab376e0c6c88af5112.jpg\">"
            ],
            "guid": "o*qQ[`#Z)l",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we call the sum of the depths of all nodes in a tree?",
                "It is called <b>internal path length</b>."
            ],
            "guid": "nj7G%Qd4NY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>self-adjusting tree</b>?",
                "Is a tree in which after every operation, a restructuring rule is applied that tends to make future operations efficient.<br>This does not mean though that the tree will be necessarily balanced after the self-adjustment."
            ],
            "guid": "FCfB>!B=*W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of self-adjusting trees?",
                "Although they cannot guarantee O(log N)&nbsp;for each single operation, any sequence of M operations on the tree takes total time O(M log N)&nbsp;in the worst case."
            ],
            "guid": "qe]u4x`>&s",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an AVL tree?",
                "An AVL (Adelson-Velskii and Landis) tree is a binary search tree with a balance condition. The balance condition must be easy to maintain, and it ensures that the depth of the tree is O(log N)&nbsp;.<br>An AVL tree is identical to a binary search tree, except that for every node in the tree,<br>the height of the left and right subtrees can differ by at most 1.&nbsp;"
            ],
            "guid": "xS7ZTX@g+=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the height of an AVL tree in practice?",
                "It is only slightly more than log N&nbsp;"
            ],
            "guid": "i=6dfTP$V=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is time complexity of operations in AVL trees?",
                "All the tree operations can be performed in O(log N) time, except possibly<br>insertion and deletion.&nbsp;"
            ],
            "guid": "F1~qM~jh81",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the name of the operation that restores the balance of AVL trees?",
                "The balance property has to be restored before the insertion step is considered over. It turns out that this can always be done with a simple modification to the tree, known as a <b>rotation</b>."
            ],
            "guid": "QS.C!X~mz$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you balance an <b>outside</b> insertion in AVL trees?",
                "The case in which the insertion occurs on the “outside” (i.e., left–left or right–right), is fixed by a single rotation of the tree."
            ],
            "guid": "br6zdGd1>7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you balance an <b>inside&nbsp;</b>insertion in AVL trees?",
                "The case in which the insertion occurs on the “inside” (i.e., left–right or right–left) is handled by&nbsp;<b>double rotation</b>.&nbsp;\n"
            ],
            "guid": "Aq@_*r:#nn",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform a <b>single rotation</b>?",
                "<img src=\"paste-252f1f33951d320e152e8fc4f9722865ee6e09e0.jpg\"><br><img src=\"paste-4d7d8a7f32e406a237944f3d2441c8209362db22.jpg\">"
            ],
            "guid": "Q89@7ALgdy",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform a <b>double-rotation</b>?",
                "<img src=\"paste-e5e4b96f5b0eede1f0f37e5f8990c46e7555e879.jpg\"><br><img src=\"paste-664f58cf5b203e5c3c013f1b81974a550a554747.jpg\">"
            ],
            "guid": "d4b$}M}*1T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does a <b>splay-tree</b>&nbsp;guarantee?",
                "a splay tree that guarantees that any M consecutive tree operations starting from an empty tree take at most O(M log N) time.<br>The problem with binary search trees is that it is possible, and not uncommon, for a whole sequence of bad accesses to take place. The cumulative running time then becomes noticeable. A search tree data structure with O(N) worst-case time, but a guarantee of at most O(M log N) for any M consecutive operations, is certainly satisfactory, because there are no bad sequences&nbsp;"
            ],
            "guid": "fpRFmJQr5.",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why do splay-trees have a simpler implementation and use less space?",
                "Because they do not require the maintenance of height or balance information."
            ],
            "guid": "BdfuC<7zs$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a splay-tree guarantee an O(M log N) amortized time for operations?",
                "The basic idea of the splay tree is that after a node is accessed, it is pushed to the root by a series of AVL tree rotations. Notice that if a node is deep, there are many nodes on the path that are also relatively deep, and by restructuring we can make future accesses cheaper on all these nodes. Thus, if the node is unduly deep, then we want this restructuring to have the side effect of balancing the tree (to some extent)."
            ],
            "guid": "ih~$PLR(0?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Descripe how splaying is done",
                "Let X be a (non-root) node on the access path at which we are rotating.<br>1- If the parent of X is the root of the tree, we merely rotate X and the root. This is the last rotation along the access path.<br>2- If X has both a parent (P) and a grandparent (G), and there are two cases, plus symmetries, to consider. The first case is the zig-zag case. Here X is a right child and P is a left child (or vice versa). If this is the case, we perform a double rotation, exactly like an AVL double rotation.<br><img src=\"paste-3555a7924e0d98470a5742f4d5329ee08cea8d3a.jpg\"><br>3- Otherwise, we have a zig-zig case: X and P are both left children (or, in the symmetric case, both right children). In that case, we transform the tree on the left of Figure 4.49 to the tree on the right&nbsp;<br><img src=\"paste-a8472babb1400070a6e3fc88c4e23e93b0f6f256.jpg\">"
            ],
            "guid": "Hsesqc&v!w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What effect does splaying has on the depth of the tree?",
                "Splaying not only moves the accessed node to the root but also has the effect of roughly halving the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels)&nbsp;"
            ],
            "guid": "Kj0-{K}NVw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which kind of traversal is used to list all the items in a BST in sorted order?",
                "In-order traversal"
            ],
            "guid": "g~Y,r0cOGY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the total time for listing all the items of a BST in sorted order?",
                "O(N)"
            ],
            "guid": "gcO=>Sp;fY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which order of traversal is to calculate the height(s) of the node(s)?",
                "Post-order traversal"
            ],
            "guid": "F:N?~r,HQz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which order of traversal is used to calculate the depth(s) of node(s)?",
                "Preorder traversal"
            ],
            "guid": "C>V#;(b.@{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call traversal that lists the nodes of depth d before depth d+1?",
                "Level-order traversal"
            ],
            "guid": "E-@2|zeTFc",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is used for level-order traversal?",
                "A queue."
            ],
            "guid": "i(1GvS8]Ot",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What trade-off are we making when considering B-trees instead of binary trees?",
                "We want to reduce the number of disk accesses to a very small constant, such as three or four. We are willing to write complicated code to do this, because machine instructions are essentially free, as long as we are not ridiculously unreasonable. It should probably be clear that a binary search tree will not work, since the typical AVL tree is close to optimal height. We cannot go below log N using a binary search tree. The solution is intuitively simple: If we have more branching, we have less height.&nbsp;"
            ],
            "guid": "pt5h)2nSxz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the height of an M-ary tree?",
                "A complete M-ary tree has height that is roughly:<br>&nbsp;<img src=\"paste-0b92770d84c2c2ca44a7d58cdd0ab83695b346d1.jpg\">"
            ],
            "guid": "H6Gk_kCkR!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a B-tree of order M?",
                "A B-tree of order M is an M-ary tree with the following properties:<br>1. The data items are stored at leaves.<br>2. The nonleaf nodes store up to M - 1 keys to guide the searching; key i represents the smallest key in subtree i + 1.<br>3. The root is either a leaf or has between two and M children.<br>4. All nonleaf nodes (except the root) have between [M/2]&nbsp;and M children.<br>5. All leaves are at the same depth and have between [L/2] and L data items, for some L<br><br>Rules 3 and 5 must be relaxed for the first L insertions&nbsp;<br><img src=\"paste-085e451dba547532239c93eb14e90dd6e17b7bcb.jpg\">"
            ],
            "guid": "pvTep/$0GZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you choose M and L parameters for a B-tree?",
                "M (number of branches) and L (number of data items in a leaf) can be chosen according to this rule:<br>Each node represents a disk block, so we choose M and L on the basis of the size of the items that are being stored.<br>We try to choose an M so that all the parent nodes would fit into a single disk block.<br>We choose L so that the whole leaf fits in a single disk block."
            ],
            "guid": "yz64*oy9`>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the approximate worst-case number of disk accesses required to retrieve an item in a B-tree?",
                "log<sub>M/2</sub>N"
            ],
            "guid": "gr;:S,7>RW",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What part of a B-tree can be cached to reduce the number of disk reads for all access operations?",
                "The root and the next level could be cached in main memory, so that over the long run, disk accesses would be needed only for level 3 and deeper.&nbsp;"
            ],
            "guid": "t/vq2wRsvB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is insertion and deletion done in a B-tree?",
                "1- If the leaf is not full, insert the item in the leaf and reorganize its data.<br>2- If the leaf is full, split it by creating new keys in its parent that point to a new leaf, and split the data equally between the new and the old leaf.<br>3- If after splitting a parent node is full, it can be split also.<br>4- If the root is also full, we split it and create another root, effectively increasing the height of the tree.<br>An alternative to splitting would be to offer the new data item for adoption in another leave which requires reordering the parent nodes.<br><br>Deletion is done the same way but in reverse (splitting is nodes is replaced by combining them)."
            ],
            "guid": "O{3zB6dy$?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of a <b>set</b>?",
                "An ordered container that does not allow duplicates."
            ],
            "guid": "etFj=%cVm{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between a list's/vector's and set's return type when inserting?",
                "Since sets do not allow duplicates, the insertion might fail and that is reflected in the returned value."
            ],
            "guid": "cT!E>##d]7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you speed up the insertion operation of a set?",
                "By passing a hint for the exact location an insert should be done. This would allow a time-complexity of O(1)"
            ],
            "guid": "oPJfYO=|,L",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call a collection of ordered entries that consists of keys and their values?",
                "A map"
            ],
            "guid": "j5~^wf/|!h",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>map</b>'s key?",
                "Keys must be unique, but several keys can map to the same values. Thus values need not be unique. The keys in the map are maintained in logically sorted order.&nbsp;"
            ],
            "guid": "s)KaTK}8`z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you make a <b>map</b> out of a set?",
                "The map behaves like a set instantiated with a pair, whose comparison function refers only to the key. Thus it supports begin, end, size, and empty, but the underlying iterator is a key-value pair.&nbsp;"
            ],
            "guid": "Cdug=hmN,S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How are sets and maps implemented (at least in C++)?",
                "C++ requires that set and map support the basic insert, erase, and find operations in logarithmic worst-case time. Consequently, the underlying implementation is a balanced&nbsp;binary search tree. Typically, an AVL tree is not used; instead, top-down red-black trees&nbsp;are often used.&nbsp;"
            ],
            "guid": "Qg^3Z6RiNT",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the set/map iteration problem?",
                "Maintain the extra links, for the next smaller and larger nodes, only for nodes that have nullptr left or right links by using extra Boolean variables to allow the routines to tell if a left link is being used as a standard binary search tree left link or a link to the next smaller node, and similarly for the right link. This&nbsp;idea is called a <b>threaded tree</b> and is used in many of the STL implementations.&nbsp;"
            ],
            "guid": "OWB$y<&?xv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "A solution for finding all the words with 1 letter difference",
                "We group the words by word length, and then apply the following algorithm on each group separately:<br><img src=\"paste-973c3547647fe0bcde1206b14fcafab34de4e4b0.jpg\"><br>"
            ],
            "guid": "q=*@A]T!BP",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a a search tree to order elements?",
                "By inserting elements into a search tree and then performing an inorder traversal, we obtain the elements in sorted order. This gives an O(N log N) algorithm to sort, which is a worst-case bound if any sophisticated search tree is used&nbsp;"
            ],
            "guid": "k6@/27?{VR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does a 2-3 tree refer to?",
                "A special case of the B-tree is the 2–3 tree (M = 3), which is another way to implement balanced search trees&nbsp;"
            ],
            "guid": "ufhp@tNB[R",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of hashing/hash tables?",
                "Hashing is a technique used for performing insertions, deletions, and finds in constant average time. Tree operations that require any ordering information among the elements are not supported efficiently. Thus, operations such as findMin, findMax, and the printing of the entire table in sorted order in linear time are not supported.&nbsp;"
            ],
            "guid": "hj<~)A.S?%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the role of a <b>hash function</b>?",
                "It maps into some number in the range 0 to TableSize - 1 and placed in the appropriate cell.&nbsp;It ideally should be simple to compute and should ensure that any two distinct keys get different cells. Since there are a finite number of cells and a virtually inexhaustible supply of keys, this is clearly<br>impossible, and thus we seek a hash function that distributes the keys evenly among the cells.&nbsp;"
            ],
            "guid": "C=B/;Ks{;i",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a simple hashing function for integer keys?",
                "Simply returning Key mod TableSize is generally a reasonable strategy, unless Key happens to have some undesirable properties."
            ],
            "guid": "L0rAl!ijxZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of the mod10 hashing function being a bad choice for integer keys.",
                "If the table size is 10 and the keys all end in zero, then the standard hash function is a bad choice.&nbsp;"
            ],
            "guid": "N*T~4e:%oZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the prefered property of the table size in a hashed table?",
                "It is a good idea to choose a table size that is a prime number."
            ],
            "guid": "H6_8x!BSp@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "A simple and well distributed hashing function for English words/strings.",
                "<img src=\"paste-fed19aea15e6b3acbb499478e1d6c7c0016b53d1.jpg\"><br><img src=\"paste-45ccbfdf0ff9a07d832050b595e3cabd4a9e3103.jpg\"><br>The code computes a polynomial function (of 37) by use of Horner’s rule."
            ],
            "guid": "N?fafI@#8C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Some strategies for hashing long strings.",
                "If the keys are very long, the hash function will take too long to compute. A common practice in this case is not to use all the characters. The length and properties of the keys would then influence the choice. For instance, the keys could be a complete street address. The hash function might include a couple of characters from the street address and perhaps a couple of characters from the city name and ZIP code. Some programmers implement their hash function by using only the characters in the odd spaces, with the idea that the time saved computing the hash function will make up for a slightly less evenly distributed function&nbsp;"
            ],
            "guid": "A8EcCrKL>j",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>hashing collision</b>?",
                "If, when an element is inserted, it hashes to the same value as an already inserted element, then we have a collision and need to resolve it. There are several methods for dealing with this,&nbsp;the simplest are: separate chaining and open addressing."
            ],
            "guid": "C?*bb]X>hc",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>separate chaining</b>?",
                "Separate chaining is to keep a list of all elements that hash to the same value. We can use the Standard Library list implementation. If space&nbsp;is tight, it might be preferable to avoid their use (since these lists are doubly linked and waste space).<br>Any scheme could be used besides linked lists to resolve the collisions; a binary search tree or even another hash table would work, but we expect that if the table is large and the hash function is good, all the lists should be short, so basic separate chaining makes no attempt to try anything complicated.\n<br><img src=\"paste-0361f4901f8e7233ddc5f94beefada26779e08e4.jpg\">"
            ],
            "guid": "oICc+8s_C=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the load factor, λ, of a hash table?",
                "It is the load factor, λ, of a hash table is the ratio of the number of elements&nbsp;<span style=\"color: var(--field-fg); background: var(--field-bg);\">in the hash table to the table size.</span>"
            ],
            "guid": "Q2S8l#A5uR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the number of links to be traversed in a map?",
                "1 + (λ/2)&nbsp;"
            ],
            "guid": "NB;ioA)I&p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the general rule for separate chaining hashing's table size?",
                "To make the table size about as large as the number of elements expected (in other words, let λ ≈ 1)&nbsp;"
            ],
            "guid": "zFucryM9df",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do&nbsp;<b>probing hash tables</b> do collision resolution<b>?</b>",
                "We try alternative cells until an empty cell is found. More formally, cells h0(x), h1(x), h2(x), . . . are tried in succession, where hi(x) = (hash(x) + f(i)) mod TableSize, with f(0) = 0. The function, f, is the collision resolution strategy. Because all the data go inside the table, a bigger table is needed<br>in such a scheme than for separate chaining hashing."
            ],
            "guid": "iwQDO[$]tz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the target load factor for probing hash tables?",
                "λ = 0.5 for a hash table that doesn’t use separate chaining."
            ],
            "guid": "eTpFtmY:q]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is linear probing done?",
                "In linear probing, f is a linear function of i, typically f(i) = i. This amounts to trying cells sequentially (with wraparound) in search of an empty cell."
            ],
            "guid": "EmL_*}*HQ%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>primary clustering</b>?",
                "In linear probing, as long as the table is big enough, a free cell can always be found, but the time to do so can get quite large. Worse, even if the table is relatively empty, blocks of occupied cells start forming. This effect, means that any key that hashes into the cluster will require several attempts to resolve the collision, and then it will add to the cluster."
            ],
            "guid": "N2Rvd>>=~S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected number of probes using linear probing?",
                "It is roughly 1/2 (1 + 1/(1 - λ)<sup>2</sup>) for insertions and&nbsp;unsuccessful searches, and 1/2(1 + 1/(1 - λ)) for successful searches."
            ],
            "guid": "r@4$yZ^|]C",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the unsuccessful search time in chaining hash tables?",
                "λ"
            ],
            "guid": "LsSE8,+S36",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative probing method eliminates the primary clustering of linear probing?",
                "Quadratic probing."
            ],
            "guid": "I,#:?VsBd7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which quadratic probing strategy function is popular?",
                "f(i) = i<sup>2</sup>"
            ],
            "guid": "Gtm~OEmaTg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What problem does quadratic probing suffer from?",
                "There is no guarantee of finding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime. This is because at most half of the table can be used as alternative locations to resolve collisions."
            ],
            "guid": "vH~&LXGV*;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you guarantee that an element can be always inserted when quadratic probing is used?",
                "By using a prime table size and making sure that the table is never more than half-full."
            ],
            "guid": "i2ybdxpG7&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can you delete an object from a probing hash table?",
                "Standard deletion cannot be performed in a probing hash table, because the cell might have caused a collision to go past it. Thus, probing hash tables require lazy deletion, although in this case there really is no laziness implied.&nbsp;"
            ],
            "guid": "EYbC7DgyRa",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>secondary clustering</b>?",
                "Although quadratic probing eliminates primary clustering, elements that hash to the same position will probe the same alternative cells. This is known as secondary clustering. Secondary clustering is a slight theoretical blemish. Simulation results suggest that it generally causes less than an extra half probe per search.&nbsp;"
            ],
            "guid": "w7%{MHF;GC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>double hashing</b>?",
                "It's a collision resolution method. For double hashing, one popular choice is f(i) = i·hash<sub>2</sub>(x). This formula says that we apply a second hash&nbsp;function to x and probe at a distance hash<sub>2</sub>(x), 2hash<sub>2</sub>(x), . . . , and so on."
            ],
            "guid": "Gp7@8Dl%k6",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the required properties of a function used for double hashing?",
                "It should never evaluate to zero and it should be able to probe all the cells."
            ],
            "guid": "g/FBwSTZcl",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Example of a good function used for double hashing.",
                "hash2(x) = R - (x mod R), with R a prime smaller than TableSize."
            ],
            "guid": "QZZ-+NY<gI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is rehashing done and why?",
                "If the table gets too full, the running time for the operations will start taking too long, and insertions might fail for open addressing hashing with quadratic resolution. This can happen if there are too many removals intermixed with insertions. A solution, then, is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table."
            ],
            "guid": "ChzH#|IyGu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of rehashing?",
                "This entire operation is called rehashing. This is obviously a very expensive operation; the running time is O(N), since there are N elements to rehash and the table size is roughly 2N."
            ],
            "guid": "w6C@m`aK4*",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When should rehashing be done?",
                "One alternative is to rehash as soon as the table is half full. The other extreme is to rehash only when an insertion fails. A third, middle-of-the-road strategy is to rehash when the table reaches a certain load factor. Since performance does degrade as the load factor increases, the third strategy, implemented with a good cutoff, could be best."
            ],
            "guid": "f=E_3gYeLs",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is used to implement unordered sets and maps?",
                "Hash tables."
            ],
            "guid": "gyRfh.1/I(",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use unordered maps to solve word-changing problem?",
                "1. A map in which the key is a word length, and the value is a collection of all words of that word length.<br>2. A map in which the key is a representative, and the value is a collection of all words with that representative.<br>3. A map in which the key is a word, and the value is a collection of all words that differ in only one character from that word.<br>Because the order in which word lengths are processed does not matter, the first map can be an unordered_map. Because the representatives are not even needed after the second map is built, the second map can be an unordered_map. The third map can also be an unordered_map, unless we want printHighChangeables to alphabetically list the subset of words that can be changed into a large number of other words.<br>The performance of an unordered_map can often be superior to a map, but it is hard to know for sure without writing the code both ways."
            ],
            "guid": "I^ffec8fCR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Given N balls placed randomly (uniformly) in N bins, what is the expected number of balls in the most occupied bin?",
                "For separate chaining, assuming a load factor of 1, this is one version of the classic balls and bins problem. The answer is well known to&nbsp;be Θ(log N/ log log N), meaning that on average, we expect some queries to take nearly logarithmic time."
            ],
            "guid": "ji^/^H8z]}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the probability that no bin has more than one ball if N balls are placed into M = N<sup>2</sup> bins?",
                "Less than 1/2"
            ],
            "guid": "ux7Ve;g/}Q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>perfect hashing</b>?",
                "It is a hashing method that uses a secondary hash table for collision resolution. Each secondary hash table will be constructed using a different hash function until it is collision free. The primary hash table can also be constructed several times if the number of collisions that are produced is higher than required."
            ],
            "guid": "+SYrg^2HF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the total size of the secondary hash tables if N items are placed in a primary hash table containing N bins?",
                "The total size of the secondary hash tables has expected value at most 2N.&nbsp;"
            ],
            "guid": "u0S{WGPXQL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition for perfect hashing to work?",
                "Perfect hashing works if the items are all known in advance."
            ],
            "guid": "K]fH_B`JDt",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do you call a perfect hashing algorithm that allows insertions and deletions?",
                "Dynamic&nbsp;schemes that allow insertions and deletions are called <b>dynamic perfect hashing</b>."
            ],
            "guid": "rv6{gvPo+2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the <b>power of two choices</b>.",
                "In the balls and bins problem, if at each toss&nbsp;two bins were randomly chosen and the item was tossed into the more empty bin (at the time), then the size of the largest bin would only be Θ(log log N), a significantly lower number than the expected Θ(log N/ log log N)."
            ],
            "guid": "h#E&mhKuz~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How <b>cuckoo hashing</b>&nbsp;works?",
                "Suppose we have N items. We maintain two tables, each more than half empty, and we have two independent hash functions that can assign each item to a position in each table. Cuckoo hashing maintains the invariant that an item is always stored in one of these two locations.<br><img src=\"paste-738d4a0f2e0f403309fb844cba67dce125eee98d.jpg\">"
            ],
            "guid": "IYP{H:W4P0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the implication of having an item in one of the two cuckoo tables?",
                "1- Search requires at most two table accesses.<br>2- No lazy deleting is required."
            ],
            "guid": "yw0h?}3-gZ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert an item in a cuckoo hashing table?",
                "1- We use a hashing algorithm to determine the position in table 1, and if it's empty, insert the item.<br>2- If the cell is already occupied we preemptively displace the existing item to table 2 which uses a different hashing algorithm and insert the new item to table 1.<br>3- If the displaced item still causes a collision we continue displacement until an empty cell is found.<br>4- After a certain number of collisions the tables can be rehashed."
            ],
            "guid": "vGlrx;<0QC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between the load-factor and the number of displacements in a cuckoo hashing table?",
                "If the table’s load factor is below 0.5, an analysis shows that the probability of a cycle is very low, that the expected number of displacements is a small constant, and that it is extremely unlikely that a successful insertion would require more than O(log N) displacements.<br>However, if the table’s load factor is at 0.5 or higher, then the probability of a cycle becomes drastically higher.&nbsp;"
            ],
            "guid": "s%X5e!|~F,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the probability that a single insertion would require a new set of hash functions?",
                "O(1/N<sup>2</sup>)&nbsp;"
            ],
            "guid": "I$EE}opb)q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What extensions are proposed for cuckoo tables?",
                "1- Instead of two tables, we can use a higher number of tables, such as 3 or 4. While this increases the cost of a lookup, it also drastically increases the theoretical space utilization.<br>2- In some applications the lookups through separate hash functions can be done in parallel and thus cost little to no additional time.<br>3- Another extension is to allow each table to store multiple keys; again, this can increase space utilization and make it easier to do insertions and can be more cache-friendly.<br><img src=\"paste-fd02b48b8683d7e584dfc6f659f9ec46c48ffaf7.jpg\"><br>4- Often cuckoo hash tables are implemented as one giant table with two (or more) hash functions that probe the entire table, and some variations attempt to place an item in the second hash table immediately if there is an available spot, rather&nbsp;than starting a sequence of displacements.&nbsp;"
            ],
            "guid": "p`}zA}mWv!",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of cuckoo hashing?",
                "The benefits of cuckoo hashing include the worst-case constant lookup and deletion times, the avoidance of lazy deletion and extra data, and the potential for parallelism.<br>However, cuckoo hashing is extremely sensitive to the choice of hash functions; the inventors of the cuckoo hash table reported that many of the standard hash functions that they attempted performed poorly in tests.&nbsp;Furthermore, although the insertion time is expected to be constant time as long as the load factor is below 1/2, deteriorates rapidly as the load factor gets close to 1/2. Using lower load factors or more than two hash functions seems like a reasonable alternative."
            ],
            "guid": "ffq;Cy@6tR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What's the main idea behind <b>Hopscotch hashing</b>?",
                "The idea of hopscotch hashing is to bound the maximal length of the probe sequence by a predetermined constant that is optimized to the underlying computer’s architecture. Doing so would give constant-time lookups in the worst case, and like cuckoo hashing, the lookup could be parallelized to simultaneously check the bounded set of possible locations.<br>Let MAX_DIST be the chosen bound on the maximum probe sequence. This means that item x must be found somewhere in the MAX_DIST positions listed in hash(x), hash(x) + 1, . . . , hash(x) + (MAX_DIST - 1). In order to efficiently process evictions, we maintain information that tells for each position x, whether the item in the alternate position is occupied by an element that hashes to position x.<br><img src=\"paste-91a2711d28eed5b2e0808c5a38ad33fa21df5ff8.jpg\">"
            ],
            "guid": "yI6{`NB<<0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of 0.5 and 0.9 load factors on Hopscotch hashing?",
                "The algorithm is deterministic in that given a hash function, either the items can be evicted or they can't. The latter case implies that the table is likely&nbsp;too crowded, and a rehash is in order; but this would happen only at extremely high load&nbsp;factors, exceeding 0.9. For a table with a load factor of 1/2, the failure probability is almost zero."
            ],
            "guid": "A;ZM7.n0RS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "For which applications Hopscotch hashing is most useful?",
                "The algorithm is especially promising for applications that make use of multiple processors and require significant parallelism and concurrency&nbsp;"
            ],
            "guid": "Bt/3l92W+_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the factors that guarantee a constant average cost per operation for hash tables?",
                "1. The hash function must be computable in constant time (i.e., independent of the number of items in the hash table).<br>2. The hash function must distribute its items uniformly among the array slots."
            ],
            "guid": "QhA&w*},Nm",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of <b>universal hash functions</b>?",
                "A family H of hash functions is universal, if for any x != y, the number of hash functions h in H for which h(x) = h(y) is at most |H|/M, where M represents TableSize.<br>Notice that this definition holds for each pair of items, rather than being averaged over all pairs of items. The definition above means that if we choose a hash function randomly from a universal family H, then the probability of a collision between any two distinct items is at most 1/M, and when adding into a table with N items, the probability of a collision at the initial point is at most N/M, or the load factor."
            ],
            "guid": "i`aB$[9zf2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which types of applications require universal hash functions?",
                "Although a strong motivation for the use of universal hash functions is to provide theoretical justification for the assumptions used in the classic hash table analyses, these functions can also be used in applications that require a high level of robustness, in which worst-case (or even substantially degraded) performance, perhaps based on inputs generated by a saboteur or hacker, simply cannot be tolerated."
            ],
            "guid": "kHB5]76CWF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of a k-universal family of hash functions?",
                "<img src=\"paste-ea8f40d52f5be8c12c22f202563ce9d1f9845b21.jpg\">"
            ],
            "guid": "dU1t%W=`_g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of a universal hash family.",
                "<img src=\"paste-2f4f0fa840ba16db307f5e7698a7b0fab753580d.jpg\">"
            ],
            "guid": "Cna!nkzGx=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the <b>Carter-Wegman trick</b>.",
                "<img src=\"paste-52605f4aa09c0442e5a62a1732730ad3cda84c58.jpg\"><br><img src=\"paste-44e816d412d434c74406f217b38f8056e78f4f1f.jpg\">"
            ],
            "guid": "H~Nolm)hT$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many disk accesses are required in <b>extendible hashing</b>?",
                "Extendible hashing allows a search to be performed in two disk accesses. Insertions also require few disk accesses."
            ],
            "guid": "uJH8a1O=$b",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which hashing algorithm reduces the number of disk accesses?",
                "A clever alternative, known as <b>extendible hashing</b>, allows a search to be performed in two disk accesses. Insertions also require few disk accesses."
            ],
            "guid": "tIv_3&]/P?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does extendible hashing relate to B-trees?",
                "A B-tree has depth O(log<sub>M/2</sub> N). As M increases, the depth of a B-tree decreases. We could in theory choose M to be so large that the depth of the B-tree would be 1. Then any search after the first would take one disk access, since, presumably, the root node could be stored in main memory. The problem with this strategy is that the branching factor is so high that it would take considerable processing to determine which leaf the data was in. If the time to perform this step could be reduced, then we would have a practical scheme. This is exactly the strategy used by extendible hashing.<br><img src=\"paste-838febabac9024fa3e6436eb3936e233a7540fa4.jpg\">"
            ],
            "guid": "d|sHz55oS+",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of the root and leaves in an extendible hashing tree?",
                "D will represent the number of bits used by the root, which is sometimes known as the directory. The number of entries in the directory is thus 2<sup>D</sup>.&nbsp;\nd<sub>L</sub>&nbsp;is the number of leading bits that all the elements of some leaf L have in common. d<sub>L</sub> will depend on the particular leaf, and d<sub>L</sub> ≤ D."
            ],
            "guid": "JBSJXE-8YY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a split done in extendible hashing?",
                "All the leaves not involved in the split are now pointed to by two adjacent directory entries. Thus, although an entire directory is rewritten, none of the other leaves is actually accessed."
            ],
            "guid": "y.ayY3tJC4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What edge cases we can have splitting an extendible hashing table's directories?",
                "First, it is possible that several directory splits will be required if the elements in a leaf agree in more than D + 1 leading bits. This is an easy detail to take care of, but must not be forgotten. Second, there is the possibility of duplicate keys; if there are more than M duplicates, then this algorithm does not work at all. In this case, some other arrangements need to be made.<br>These possibilities suggest that it is important for the bits to be fairly random. This can be accomplished by hashing the keys into a reasonably long integer—hence the name."
            ],
            "guid": "q*!d0r@ymF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected number of leaves in extendible hashing?",
                "The expected number of leaves is (N/M) log<sub>2</sub>e. Thus the average leaf is ln 2 = 0.69 full. This is the same as for B-trees, which is not entirely surprising, since for both data structures new nodes are created when the (M + 1)th entry is added."
            ],
            "guid": "tcCSq*#,w)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the expected size of the directory in extendible hashing?",
                "the expected size of the directory (in other words, 2D) is O(N<sup>1+1/M</sup>/M). If M is very small, then the directory can get unduly large. In this case, we can have the leaves contain pointers to the records instead of the actual records, thus increasing the value of M. This adds a second disk access to each search operation in order to maintain a smaller directory. If the directory is too large to fit in main memory, the second disk access would be needed anyway."
            ],
            "guid": "BMveuq:x~E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structures do compilers use to keep track of declared variables?",
                "Compilers use hash tables to keep track of declared variables in source code. The data structure is known as a <b>symbol table</b>. Hash tables are the ideal application for this problem. Identifiers are typically short, so the hash function can be computed quickly, and alphabetizing the variables is often unnecessary."
            ],
            "guid": "opZ+?[sY:]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does the performance of a binary search tree and a hash table compare?",
                "Although the resulting average time bounds of insert and contains operations&nbsp;are O(log N), binary search trees also support routines that require order and are thus more powerful. Using a hash table, it is not possible to find the minimum element. It is not possible to search efficiently for a string unless the exact string is known. A binary search tree could quickly find all items in a certain range; this is not supported by hash tables. Furthermore, the O(log N) bound is not necessarily that much more than O(1), especially since no multiplications or divisions are required by search trees.<br>On the other hand, the worst case for hashing generally results from an implementation error, whereas sorted input can make binary trees perform poorly. Balanced search trees are quite expensive to implement, so if no ordering information is required and there is any suspicion that the input might be sorted, then hashing is the data structure of choice."
            ],
            "guid": "o%[XJ)l|D|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe using hash tables in graph theory problems.",
                "A hash table is useful for any graph theory problem where the nodes have real names instead of numbers. Here, as the input is read, vertices are assigned integers from 1 onward by order of appearance. Again, the input is likely to have large groups of alphabetized entries. For example, the vertices could be computers. Then if one particular installation lists its computers as ibm1, ibm2, ibm3, . . . , there could be a dramatic effect on efficiency if a search tree is used."
            ],
            "guid": "k35b3y6z3}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a <b>transposition table </b>used in games?",
                "A common use of hash tables is in programs that play games. As the program searches through different lines of play, it keeps track of positions it has seen by computing a hash function based on the position (and storing its move for that position). If the same position recurs, usually by a simple transposition of moves, the program can avoid expensive recomputation."
            ],
            "guid": "HQX%zJ?ByY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structure we can use for an online spelling checker?",
                "If misspelling detection (as opposed to correction) is important, an entire dictionary can be prehashed and words can be checked in constant time. Hash tables are well suited for this, because it is not important to alphabetize words; printing out misspellings in the order they occurred in the document is certainly acceptable."
            ],
            "guid": "zTo6gvE6yv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data-structure is frequently used for caching?",
                "Hash tables are often used to implement caches, both in software (for instance, the cache in your Internet browser) and in hardware (for instance, the memory caches in modern computers). They are also used in hardware implementations of routers."
            ],
            "guid": "Nd,6qFBlze",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the minimum operations required by a <b>priority queue</b>?",
                "Insert and deleteMin, which finds, returns, and removes the minimum element in the priority queue."
            ],
            "guid": "mO[{h@:75~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which data structure is commonly used in greedy algorithms?",
                "Priority queue (heap)."
            ],
            "guid": "F(z3J^Y+S_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the structure of <b>heap</b>.",
                "A heap is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled from left to right. Such a tree is known as a complete binary tree.<br><img src=\"paste-54b9f2aa2a98dcd6f5a6c931853fd147a9e34b8b.jpg\">"
            ],
            "guid": "A}^P_|g!!G",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between a <b>complete binary tree</b>'s height and number of nodes?",
                "A complete binary tree of height h has between 2<sup>h</sup> and 2<sup>h+1</sup> - 1 nodes. This implies that the height of a complete binary tree is log N , which is clearly O(log N)."
            ],
            "guid": "J?z5ZMY&~A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What simple data-structure can be used to represent a complete binary tree? Why?",
                "Because a complete binary tree is so regular, it can be represented in an array and no links are necessary.<br><img src=\"paste-58c7f2867f1e1ad7f827df5bc8422c9ace7ab89b.jpg\">"
            ],
            "guid": "O[-7ZyO&|g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between the array index and heap traversal when implemented using an array?",
                "For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child (2i + 1), and the parent is in position i/2 . Thus, not only are links not required, but the operations required to traverse the tree are extremely simple and likely to be very fast on most computers.&nbsp;"
            ],
            "guid": "jilXnd4/0D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which property allows operations to be performed quickly in a <b>heap</b>?",
                "Since we want to be able to find the minimum quickly, it makes sense that the smallest element should be at the root. If we consider that any subtree should also be a heap, then any node should be smaller than all of its descendants.&nbsp;This is called <b>heap-order property</b>."
            ],
            "guid": "Ai-JUa0hHo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert a new item in a <b>heap</b>?",
                "To insert an element X into the heap, we create a hole in the next available location, since otherwise, the tree will not be complete. If X can be placed in the hole without violating heap order, then we do so and are done. Otherwise, we slide the element that is in the hole’s parent node into the hole, thus bubbling the hole up toward the root. We continue this process until X can be placed in the hole.&nbsp;This general strategy is known as a <b>percolate up</b>.<br><img src=\"paste-eeca0f36d8f4f60ea0a701e1ae371587d4a47451.jpg\">"
            ],
            "guid": "hq-14$laE$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average time for inserting into a <b>heap</b>?",
                "The time to do the insertion could be as much as O(log N), if the element to be inserted is the new minimum and is percolated all the way to the root. On average, the percolation terminates early; it has been shown that 2.607 comparisons are required on average to perform an insert, so the average insert moves an element up 1.607 levels."
            ],
            "guid": "rNxkov!Vh]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is deleteMin done in heaps?",
                "When the minimum is removed, a hole is created at the root. Since the heap now becomes one smaller, it follows that the last element X in the heap<br>must move somewhere in the heap. If X can be placed in the hole, then we are done. This is unlikely, so we slide the smaller of the hole’s children into the hole, thus pushing the hole down one level. We repeat this step until X can be placed in the hole. Thus, our action is to place X in its correct spot along a path from the root containing minimum children.<br><img src=\"paste-1d8b032be31297c586ad5c1164bd0dfddbade2ec.jpg\">"
            ],
            "guid": "fV!h!dgw4B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a common mistake when percolating down in <b>heap</b>s? And what are the possible solutions?",
                "A frequent implementation error in heaps occurs when there are an even number of elements in the heap, and the one node that has only one child is encountered. You must&nbsp;make sure not to assume that there are always two children, so this usually involves an extra test. In the code depicted in&nbsp; One extremely tricky solution is always to ensure that your algorithm thinks every node has two children. Do this by placing a sentinel, of value higher than any in the heap, at the spot after the heap ends, at the start of each percolate down when the heap size is even. You should think very carefully before attempting this, and you must put in a prominent comment if you do use this technique. Although this eliminates the need to test for the presence of a right child, you cannot eliminate the requirement that you test when you reach the bottom, because this would require a sentinel for every leaf."
            ],
            "guid": "x`GD#@*PmI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times for <b>deleteMin</b> in <b>heap</b>s?",
                "The worst-case running time for this operation is O(log N). On average, the element that is placed at the root is percolated almost to the bottom of the heap (which is the level it came from), so the average running time is O(log N)."
            ],
            "guid": "ia:#5EU)tL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find an item other than the minimum in a <b>heap</b>?",
                "A heap has very little ordering information, so there is no way to find any particular element without a linear scan through the entire heap.&nbsp;If it is important to know where elements are, some other data structure, such as a hash table, must be used in addition to the heap."
            ],
            "guid": "Iwr;Vp|OFh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>decreaseKey</b> in a <b>heap</b>?",
                "The decreaseKey(p, delta) operation lowers the value of the item at position p by a positive amount delta. Since this might violate the heap order, it must be fixed by a percolate up. This operation could be useful to system administrators: They can make their programs run with highest priority,"
            ],
            "guid": "dd^m*5d[dE",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>increaseKey</b>&nbsp;<b>in a heap</b>?",
                "The increaseKey(p, delta) operation increases the value of the item at position p by a positive amount delta. This is done with a percolate down. Many schedulers automatically drop the priority of a process that is consuming excessive CPU time."
            ],
            "guid": "h9u3JhuM4W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you remove an arbitrary item from a <b>heap</b>?",
                "The remove(p) operation removes the node at position p from the heap. This is done by first performing decreaseKey(p,∞) and then performing deleteMin(). When a process&nbsp;is terminated by a user (instead of finishing normally), it must be removed from the priority queue."
            ],
            "guid": "ob8MhCor:D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you implement <b>buildHeap</b>&nbsp;operation?",
                "This constructor takes as input N items and places them into a heap. Obviously, this can be done with N successive inserts. Since each insert will take O(1) average and O(log N) worst-case time, the total running time of this algorithm would be O(N) average but O(N log N) worst-case."
            ],
            "guid": "udJgv#t_:A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the sum of heights in a <b>perfect binary tree</b>?",
                "For the perfect binary tree of height h containing 2<sup>h+1</sup>-1 nodes, the sum of the heights of the nodes is 2<sup>h+1</sup> - 1 - (h + 1)."
            ],
            "guid": "e<Kz%=c&Ya",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the time required to find the kth largest/smallest number when arrays + sorting are used?",
                "1- Sorting the whole array: O(N<sup>2</sup>)<br>2- Sorting an array of k elements and then inserting into the array the other items if they are within the range, or simply ignoring them: The running time is O(N·k). If k = [N/2], then both algorithms are O(N<sup>2</sup>). Notice that for any k, we can solve the symmetric problem of finding the (N - k + 1)th smallest element, so k = [N/2] is really the hardest case for these algorithms. This also happens to be the most interesting case, since this value of k is known as the median&nbsp;"
            ],
            "guid": "EX:n:fo&S)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a <b>heap</b>&nbsp;to get the kth smallest element?",
                "1- We read the N elements into an array. We then apply the buildHeap algorithm to this array. Finally, we perform k deleteMin operations. The last element extracted from the heap is our answer.&nbsp;If k = [N/2], then the running time is&nbsp;Θ(N log N).<br>2- At any point in time we will maintain a set S of the k largest elements. After the first k elements are read, when a new element is read it is compared with the kth largest element, which we denote by S<sub>k</sub>. Notice that S<sub>k</sub> is the smallest element in S. If the new element is larger, then it replaces S<sub>k</sub> in S. S will then have a new smallest element, which may or may not be the newly added element. At the end of the input, we find the smallest element in S and return it as the answer.&nbsp;The total time is O(N log k). This algorithm also gives a bound of Θ(N log N) for finding the median."
            ],
            "guid": "G.$AN&Wj}|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we simulate a teller queue in a bank?",
                "A simulation consists of processing events. The two events here are (a) a customer arriving and (b) a customer departing, thus freeing up a teller.&nbsp;The key is to advance the clock to the next event time at each stage. At any point, the next event that can occur is either (a) the next customer in the input file arrives or (b) one of the customers at a teller leaves. Since all the times when the events will happen are available, we just need to find the event that happens nearest in the future and process that event. If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to see whether there is another customer waiting. If so, we add that customer, process whatever statistics are required, compute the time when that customer will leave, and add that departure to the set of events waiting to happen. If the event is an arrival, we check for an available teller. If there is none, we place the arrival on the line (queue); otherwise we give the customer a teller, compute the customer’s departure time, and add the departure to the set of events waiting to happen.<br>The waiting line for customers can be implemented as a queue. Since we need to find the event nearest in the future, it is appropriate that the set of departures waiting to happen be organized in a priority queue. The next event is thus the next arrival or next departure (whichever is sooner); both are easily available."
            ],
            "guid": "xuo,Y|OyN=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the bank teller queueing problem when a heap is used?",
                "If there are&nbsp;C&nbsp;customers (and thus 2C&nbsp;events) and&nbsp;k&nbsp;tellers, then the running time&nbsp;of the simulation&nbsp;would be&nbsp;O(C&nbsp;log(k&nbsp;+&nbsp;1)) because computing and processing each event&nbsp;takes&nbsp;O(log&nbsp;H), where&nbsp;H&nbsp;=&nbsp;k&nbsp;+&nbsp;1 is the size of the heap."
            ],
            "guid": "bM[619)HdK",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>d-heap</b>?",
                "It's a simple generalization&nbsp;which is exactly like a binary heap except that all nodes have d children (thus, a binary heap is a 2-heap)."
            ],
            "guid": "dqd;|nd,xo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>d-heap</b>?",
                "A d-heap is much shallower than a binary heap, improving the running time of inserts to O(log<sub>d</sub> N). However, for large d, the deleteMin operation is more expensive, because even though the tree is shallower, the minimum of d children must be found, which takes d - 1 comparisons using a&nbsp; standard algorithm. This raises the time for this operation to O(d log<sub>d</sub> N). If d is a constant, both running times are, of course, O(log N). Although an array can still be used, the multiplications and divisions to find children and parents are now by d, which, unless d is a power of 2, seriously increases the running time, because we can no longer implement division by a bit shift.<br><img src=\"paste-42ca6ae3f94b1aab2f918c407b03d1eb87a22d67.jpg\">"
            ],
            "guid": "NZva&){^;H",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which applications is a d-heap useful?",
                "d-heaps are interesting in theory, because there are many algorithms where the number of insertions is much greater than the number of deleteMins (and thus a theoretical speedup is possible). They are also of interest when the priority queue is too large to fit entirely in main memory. In this case, a d-heap can be advantageous in much the same way as B-trees. Finally, there is evidence suggesting that 4-heaps may outperform binary heaps in practice."
            ],
            "guid": "mE:?g}lO)$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of heaps?",
                "The most glaring weakness of the heap implementation, aside from the inability to perform finds, is that combining two heaps into one is a hard operation. This extra operation is known as a merge."
            ],
            "guid": "fqSo,2c9I=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What change in the heap's data-structure we can make for merging to be faster? And what is the impact of this change on other heap operations?",
                "It seems difficult to design a data structure that efficiently supports merging (that is, processes a merge in o(N) time) and uses only an array, as in a binary heap. The reason for this is that merging would seem to require copying one array into another, which would take Θ(N) time for equal-sized heaps. For this reason, all the advanced data structures that support efficient merging require the use of a linked data structure. In practice, we can expect that this will make all the other operations slower."
            ],
            "guid": "PObZz|oRt>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the similarities and differences between a binary heap and a <b>leftist heap</b>?",
                "A leftist heap has both a structural property and an ordering property. Indeed, a leftist heap, like virtually all heaps used, has the same heap-order property we have already seen. Furthermore, a leftist heap is also a binary tree. The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced, but actually attempt to be very unbalanced."
            ],
            "guid": "ocTeOr;}OJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>null path length</b>.",
                "The null path length, npl(X), of any node X to be the length of the shortest path from X to a node without two children. Thus, the npl of a node with zero or one child is 0, while npl(nullptr) = -1."
            ],
            "guid": "IE[.51s5El",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the <b>leftist heap property</b>.",
                "The leftist heap property is that for every node X in the heap, the <b>null path length</b> of the left child is at least as large as that of the right child. This property is satisfied by only the tree on the left. This property actually goes out of its way to ensure that the tree is unbalanced, because it clearly biases the tree to get deep toward the left.<br><img src=\"paste-3d31de6c9e08eba3e99436ef3a4670c1695aa9d2.jpg\">"
            ],
            "guid": "r6jNH?<WH1",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum number of nodes on the right path in a <b>leftist tree</b>?",
                "[log(N + 1)] nodes."
            ],
            "guid": "jCHP:5;4{z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge two <b>leftist heap</b>s?",
                "If either of the two heaps is empty, then we can return the other heap. Otherwise, to merge the two heaps, we compare their roots.<br><img src=\"paste-657956ac05a39f3da757204cdae689ab6aa220be.jpg\"><br>First, we recursively merge the heap with the larger root with the right subheap of the heap with the smaller root.<br><img src=\"paste-782a3499ad72473942795f70ea31f726f7a7f6f0.jpg\"><br>Although the resulting heap satisfies the heap-order property, it is not leftist because the left subtree of the root has a null path length of 1 whereas the right subtree has a null path length of 2. Thus, the leftist property is violated at the root.<br><img src=\"paste-30963476b82950f7b2013dd41ecb653605a76af4.jpg\"><br>We can make the entire tree leftist by merely swapping the root’s left and right children and updating the null path length.<br><img src=\"paste-737ea761d23452b9f711d8f6cfb4b5590f0c863f.jpg\">"
            ],
            "guid": "QjF@LWazp2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the merging time of two <b>leftist heaps</b>?",
                "The time to perform the merge is proportional to the sum of the length of the right paths, because constant work is performed at each node visited during the recursive calls. Thus we obtain an O(log N) time bound to merge two leftist heaps."
            ],
            "guid": "Em+P8uSAzA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is deleteMin implemented in <b>leftist heap</b>s?",
                "We merely destroy the root, creating two heaps, which can then be merged. Thus, the time to perform a deleteMin is O(log N)."
            ],
            "guid": "i4CG+s7{^Y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a <b>skew heap</b>?",
                "It is a self-adjusting version of a leftist heap that is simple to implement. The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees and AVL trees. Skew heaps are binary trees with heap order, but there is no structural constraint on these trees. Unlike leftist heaps, no information is maintained about the null path length of any node. The right path of a skew heap can be arbitrarily long at any time."
            ],
            "guid": "LX/9yRYMy:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of operations in a <b>skew heap</b>?",
                "The right path of a skew heap can be arbitrarily long at any time, so the worst-case running time of all operations is O(N). However, as with splay trees, it can be shown that for any M consecutive operations, the total worst-case running time is O(M log N). Thus, skew heaps have O(log N) amortized cost per operation."
            ],
            "guid": "g-c&,xR$w&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge <b>skew heaps</b>?",
                "<img src=\"paste-12f3a7e6cc65fa617ab9239a27b31c0a0bb167e8.jpg\"><br>We can perform all operations nonrecursively, as with leftist heaps, by merging the right paths and swapping left and right children for every node on the right path, with&nbsp;the exception of the last.<br><img src=\"paste-2b624bc74bcabf69354245109a1ee8118c3d4cdb.jpg\"><br>After a few examples, it becomes clear that since all but the last node on the right path have their children swapped, the net effect is that this becomes the new left path.<br><img src=\"paste-482a887dd52a089d02bbac5cee63a8ef89aaa8b5.jpg\">"
            ],
            "guid": "iz>&[WUiA[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>binomial queues</b>?",
                "They support insert, deleteMin, and merge in O(log N) worst-case, while insertions take constant time on average.&nbsp;"
            ],
            "guid": "PMS%gfAMaY",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How does a <b>binomial queue</b>&nbsp;differ from all other priority queue implementations?",
                "Binomial queues differ from all the priority queue implementations that we have seen in that a binomial queue is not a heap-ordered tree but rather a collection of heap-ordered trees, known as a <b>forest</b>. Each of the heap-ordered trees is of a constrained form known as a <b>binomial tree</b>."
            ],
            "guid": "hI*&@vk1EG",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the structure of a <b>binomial tree.</b>",
                "There is at most one binomial tree of every height. A binomial tree of height 0 is a one-node tree; a binomial tree, B<sub>k</sub>, of height k is formed by attaching a binomial tree, B<sub>k-1</sub>, to the root of another binomial tree, B<sub>k-1</sub>. From the diagram we see that a binomial tree, B<sub>k</sub>, consists of a root with children B<sub>0</sub>, B<sub>1</sub>, . . . , B<sub>k-1</sub>. Binomial trees of height k have exactly 2k nodes, and the number of nodes at depth d is the binomial coefficient&nbsp;<img src=\"paste-0689df9309716c497c8d2c47888d56626a9f111e.jpg\"><br><img src=\"paste-1773f101a1f0ea9c1655db9709db9f5b68d0fc4e.jpg\">"
            ],
            "guid": "l0<}AFOH#{",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use <b>binomial trees</b>&nbsp;to represent a priority queue?",
                "If we impose heap order on the binomial trees and allow at most one binomial tree of any height, we can represent a priority queue of any size by a collection of binomial trees. For instance, a priority queue of size 13 could be represented by the forest B<sub>3</sub>, B<sub>2</sub>, B<sub>0</sub>. We might write this representation as 1101, which not only represents 13 in binary but also represents the fact that B<sub>3</sub>, B<sub>2</sub>, and B<sub>0</sub> are present in the representation and B<sub>1</sub> is not.<br>As an example, a priority queue of six elements could be represented as in this figure:<br><img src=\"paste-f4629e6ecef9fd0f4a560eda513f742a7e898788.jpg\">"
            ],
            "guid": "lMRRx5SS?L",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you find the minimum element in a <b>binomial queue</b>?",
                "By scanning the roots of all the trees. Since there are at most log N different trees, the minimum can be found in O(log N) time. Alternatively, we can maintain knowledge of the minimum and perform the operation in O(1) time if we remember to update the minimum when it changes during other operations."
            ],
            "guid": "ferGjEqFmB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you merge two <b>binomial queues</b>?",
                "Consider the two binomial queues, H<sub>1</sub> and H<sub>2</sub>, with six and seven elements, respectively:<br><img src=\"paste-0c7f90d442aa367deca741aeca767f438140f62b.jpg\"><br>The merge is performed by essentially adding the two queues together. Let H<sub>3</sub> be the new binomial queue. Since H<sub>1</sub> has no binomial tree of height 0 and H<sub>2</sub> does, we can just use the binomial tree of height 0 in H<sub>2</sub> as part of H<sub>3</sub>. Next, we add binomial trees of height 1. Since both H<sub>1</sub> and H<sub>2</sub> have binomial trees of height 1, we merge them by making the larger root a subtree of the smaller, creating a binomial tree of height 2:<br><img src=\"paste-138eeba44a90e7cb5959060753c99daa3f8df68a.jpg\"><br>Thus, H3 will not have a binomial tree of height 1. There are now three&nbsp;binomial trees of height 2, namely, the original trees of H<sub>1</sub> and H<sub>2</sub> plus the tree formed by the previous step. We keep one binomial tree of height 2 in H<sub>3</sub> and merge the other two, creating a binomial tree of height 3. Since H<sub>1</sub> and H<sub>2</sub> have no trees of height 3, this tree becomes part of H<sub>3</sub> and we are finished.<br><img src=\"paste-a5d102b03a54da8e83d66ea65e5d696f73c78bb9.jpg\">"
            ],
            "guid": "NqbL8>=+u4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for merging two <b>binomial queues</b>?",
                "Since merging two binomial trees takes constant time with almost any reasonable implementation, and there are O(log N) binomial trees, the merge takes O(log N) time in the worst case. To make this operation efficient, we need to keep the trees in the binomial queue sorted by height, which is certainly a simple thing to do."
            ],
            "guid": "A%U8DM>op]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you insert into a <b>binomial queue</b>?",
                "Insertion is just a special case of merging, since we merely create a one-node tree and perform a merge. The worst-case time of this operation is likewise O(log N). More precisely, if the priority queue into which the element is being inserted has the property that the smallest nonexistent binomial tree is Bi, the running time is proportional to i + 1. An analysis will show that performing N inserts on an initially empty binomial queue will take O(N) worst-case time. Indeed, it is possible to do this operation using only N - 1 comparisons.<br><img src=\"paste-6bd4b2dd58a6ddbb0e3340d175173adaf0a4ebb5.jpg\">"
            ],
            "guid": "iSt57A2SfL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform <b>deleteMin</b>&nbsp;in a <b>binomial queue</b>?",
                "A deleteMin can be performed by first finding the binomial tree with the smallest root. Let this tree be B<sub>k</sub>, and let the original priority queue be H. We remove the binomial tree B<sub>k</sub> from the forest of trees in H, forming the new binomial queue H'. We also remove the root of B<sub>k</sub>, creating binomial trees B<sub>0</sub>, B<sub>1</sub>, . . . , B<sub>k-1</sub>, which collectively form priority queue H''. We finish the operation by merging H' and H''.<br><img src=\"paste-5e624e0405e2d43b055ff6b7555d216fd1473d81.jpg\"><br><img src=\"paste-cef2cc7a7f38f1000eb7ad0e9dc539c79d4d4f5b.jpg\"><br><img src=\"paste-3d570dea05c02022deb071443e38b9efd805201a.jpg\"><br>&nbsp;<img src=\"paste-dd5b8762ffb3881b64c53fb766ecc4464a913f70.jpg\"><br>For the analysis, note first that the deleteMin operation breaks the original binomial queue into two. It takes O(log N) time to find the tree containing the minimum element and to create the queues H' and H''. Merging these two queues takes O(log N) time, so the entire deleteMin operation takes O(log N) time."
            ],
            "guid": "s=eDI.&_*9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain how a <b>binomial queue </b>is implemented.",
                "The deleteMin operation requires the ability to find all the subtrees of the root quickly, so the standard representation of general trees is required: The children of each node are kept in a linked list, and each node has a pointer to its first child (if any). This operation also requires that the children be ordered by the size of their subtrees. We also need to make sure that it is easy to merge two trees. When two trees are merged, one of the trees is added as a child to the other. Since this new tree will be the largest subtree, it makes sense to maintain the subtrees in decreasing sizes. Only then will we be able to merge two binomial trees, and thus two binomial queues, efficiently. The binomial queue will be an array of binomial trees. To summarize, then, each node in a binomial tree will contain the data, first child, and right sibling. The children in a binomial tree are arranged in decreasing rank.\n<br><img src=\"paste-6869b08b3c8739dd047ae93cdb095a962e991382.jpg\"><br><img src=\"paste-217ebe08300a935500b838ef8a1437350902853b.jpg\">"
            ],
            "guid": "n+`f3}@foO",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>comparison-based sorting</b>.",
                "It assumes the existence of the “&lt;” and “&gt;” operators, which can be used to place a consistent ordering on the input. Besides the assignment operator, these are the only operations allowed on the input data."
            ],
            "guid": "mm3D{0`6wo",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>insertion sort</b>&nbsp;done?",
                "Insertion sort consists of N-1 passes. For pass p=1 through N-1, insertion sort ensures that the elements in positions 0 through p are in sorted order. Insertion sort makes use of the fact that elements in positions 0 through p-1 are already known to be in sorted order.<br><img src=\"paste-c24a64ed279d59ca0371c0b707e98ea0fe010904.jpg\">"
            ],
            "guid": "u,6**fhPC8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for <b>insertion sort</b>?",
                "Because of the nested loops, each of which can take N iterations, insertion sort is O(N<sup>2</sup>).&nbsp;On the other hand, if the input is presorted, the running time is O(N), because the test in the inner for loop always fails immediately. Indeed, if the input is almost sorted insertion sort will run quickly.&nbsp;the average case is Θ(N<sup>2</sup>)."
            ],
            "guid": "fV`zJ-iapj",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an <b>inversion</b> in arrays and sorting algorithms?",
                "An inversion in an array of numbers is any ordered pair (i, j) having the property that i &lt; j but a[i] &gt; a[j]. For example: in the input list 34, 8, 64, 51, 32, 21 had nine inversions, namely (34, 8), (34, 32), (34, 21), (64, 51), (64, 32), (64, 21), (51, 32), (51, 21), and (32, 21).&nbsp;\n"
            ],
            "guid": "H_Kh4ebO{=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average number of inversions in an array of N distinct elements for sorting algorithms that perform only adjacent exchanges?",
                "N(N - 1)/4&nbsp;"
            ],
            "guid": "s)Mb`dh2Vb",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the required time for any algorithm that sorts by exchanging adjacent elements&nbsp;on average?",
                "Ω(N<sup>2</sup>)&nbsp;"
            ],
            "guid": "t4:9/A6Z}f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can a sorting algorithm run in a subquadratic time?",
                "For a sorting algorithm to run in subquadratic, or o(N<sup>2</sup>), time, it must do comparisons and, in particular, exchanges between elements that are far apart. A sorting algorithm makes progress by eliminating inversions, and to run efficiently, it must eliminate more than just one inversion per exchange."
            ],
            "guid": "eay@*nBUaU",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the other name of <b>shellsort</b>?",
                "Shellsort works by comparing elements that are distant; the distance between comparisons decreases as the algorithm runs until the last phase, in which adjacent elements are compared. For this reason, Shellsort is sometimes referred to as <b>diminishing increment sort</b>."
            ],
            "guid": "cDPJy:aILg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>shellsorting </b>done?",
                "Shellsort uses a sequence, h<sub>1</sub>, h<sub>2</sub>, . . . , h<sub>t</sub>, called the <b>increment sequence</b>. Any increment sequence will do as long as h<sub>1</sub> = 1, but some choices are better than others. After a <b>phase</b>, using some increment h<sub>k</sub>, for every i, we have a[i] ≤ a[i + h<sub>k</sub>] (where this makes sense); all elements spaced h<sub>k</sub> apart are sorted. The file is then said to be h<sub>k</sub>-sorted. An important property of Shellsort is that an h<sub>k</sub>-sorted file that is then h<sub>k</sub>-1-sorted remains h<sub>k</sub>-sorted. If this were not the case, the algorithm would likely be of little value, since work done by early phases would be undone by later phases.<br><img src=\"paste-8ffb06ee34372d426d7cf273de7fdd1e58f5b9d5.jpg\">"
            ],
            "guid": "A-1b`dQ{@R",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a common but poor example of an <b>increment sequence </b>in<b> shellsort</b>?",
                "h<sub>t</sub> = [N/2], and h<sub>k </sub>= [h<sub>k+1</sub>/2].&nbsp;"
            ],
            "guid": "j!kNlUxS{m",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst-case running time of <b>Shellsort</b> using Shell’s increments?",
                "Θ(N<sup>2</sup>)"
            ],
            "guid": "In#~!`_q,U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst-case running time of Shellsort using Hibbard’s increments?",
                "Θ(N<sup>3/2</sup>). His increments are of the form 1, 3, 7, . . . , 2<sup>k</sup> - 1."
            ],
            "guid": "Nq#dG$=e9.",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times of Sedgewick's increment sequences for <b>shellshort</b>?",
                "O(N<sup>4/3</sup>) worstcase running time and O(N<sup>7/6</sup>)&nbsp;average running time.&nbsp;The best of these is the sequence {1, 5, 19, 41, 109, . . .}, in which the terms are either of the form 9 · 4<sup>i</sup> - 9 · 2<sup>i</sup> + 1 or 4<sup>i</sup> - 3 · 2<sup>i</sup> + 1."
            ],
            "guid": "c./YzT7[!M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you perform <b>heapsort</b>?",
                "The basic strategy is to build a binary heap of N elements. This stage takes O(N) time. We then perform N deleteMin operations. The elements leave&nbsp;the heap smallest first, in sorted order. By recording these elements in a second array and then copying the array back, we sort N elements. Since each deleteMin takes O(log N) time, the total running time is O(N log N)."
            ],
            "guid": "h-N&!?js_-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of <b>heapsort</b>?",
                "The main problem with this algorithm is that it uses an extra array. Thus, the memory requirement is doubled. This could be a problem in some instances."
            ],
            "guid": "lv/WGqk$*t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we avoid having a second array in <b>heapsort</b>?",
                "A clever way to avoid using a second array makes use of the fact that after each deleteMin, the heap shrinks by 1. Thus the cell that was last in the heap can be used to store the element that was just deleted. As an example, suppose we have a heap with six elements. The first deleteMin produces a<sub>1</sub>. Now the heap has only five elements, so we can place a<sub>1</sub> in position 6. The next deleteMin produces a<sub>2</sub>. Since the heap will now only have four elements, we can place a<sub>2</sub> in position 5. Using this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order. If we want the elements in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger element than the child. Thus, we have a (max)heap.<br><img src=\"paste-cd90fe01f6286f6090f9c822ae1bffc81b3217f5.jpg\">"
            ],
            "guid": "c~)ccD>)JL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average number of comparisons used to heapsort a random permutation of N distinct items?",
                "2N log N - O(N log log N)."
            ],
            "guid": "C1gXA9_;&E",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst running time for&nbsp;<b>mergesort</b>?",
                "O(N log N)"
            ],
            "guid": "zuY~J5ke(k",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>mergesort</b>&nbsp;done?",
                "The fundamental operation in this algorithm is merging two sorted lists. Because the lists are sorted, this can be done in one pass through the input, if the output is put in a third list. The basic merging algorithm takes two input arrays A and B, an output array C, and three counters, Actr, Bctr, and Cctr, which are initially set to the beginning of their respective arrays. The smaller of A[Actr] and B[Bctr] is copied to the next entry in C, and the appropriate counters are advanced. When either input list is exhausted, the remainder of the other list is copied to C.<br><img src=\"paste-0f8d9ec3dbd035273ebc6c4d7d998110aacaff01.jpg\"><br><img src=\"paste-fdbe8b903808c2cd9cb1ecf5f65c08658ac92fc4.jpg\"><br><img src=\"paste-4417a05617c84f5ec6d53b092b3ee788add37f83.jpg\"><br><img src=\"paste-b02ddc574bcd3b6c9c3729e1bc5d8dddd418af91.jpg\">"
            ],
            "guid": "PRE5Sl:8~|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which strategy describes <b>mergesort</b>?",
                "Divide-and-conquer strategy"
            ],
            "guid": "qmGp=S}s4|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the difference in performance between <b>mergesort</b> and other sorting algorithms with same time complexity depend on?",
                "The running time of mergesort&nbsp;depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array).&nbsp;For a language where copying objects is expensive a better alternative would be <b>quicksort</b>."
            ],
            "guid": "swU4:sfMLV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the fastest generic sorting algorithm?",
                "Quicksort has an average running time of O(N log N), and a worst running time of O(N<sup>2</sup>) which is highly unlikely in the optimized version."
            ],
            "guid": "k?Ab?twr?F",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we increase the probability of having a running time of O(N log N) in <b>quicksort</b>?",
                "By combining it with <b>heapsort</b>."
            ],
            "guid": "u`;t?5jJ@,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What kind of algorithm is <b>quicksort</b>?",
                "A divide-and conquer recursive algorithm."
            ],
            "guid": "Bg`vQGI@df",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>classic quicksort</b>.",
                "The classic quicksort algorithm to sort an array S consists of the following four easy steps:<br>1. If the number of elements in S is 0 or 1, then return.<br>2. Pick any element v in S. This is called the pivot.<br>3. Partition S - {v} (the remaining elements in S) into two disjoint groups: S1 = {x ∈ S - {v}|x ≤ v}, and S2 = {x ∈ S - {v}|x ≥ v}.<br>4. Return {quicksort(S1) followed by v followed by quicksort(S2)}.<br><img src=\"paste-a11a6986fac3d7e1674504516a6c8f7ae1a03d9f.jpg\">"
            ],
            "guid": "DP|`2-24V-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is <b>quicksort</b>&nbsp;considered to be faster than mergesort?",
                "Like mergesort, it recursively solves two subproblems and requires linear additional work, but, unlike mergesort, the subproblems are not guaranteed to be of equal size, which is potentially bad. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efficiently. This efficiency more than makes up for the lack of equal-sized recursive calls."
            ],
            "guid": "L(AJPaG3UQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the disadvantages of choosing the first element as a pivot in <b>quicksort</b>?",
                "This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition, because either all the elements go into S<sub>1</sub> or they go into S<sub>2</sub>. Worse, this happens consistently throughout the recursive calls. The practical effect is that if the first element is used as the pivot and the input is presorted, then quicksort will take quadratic time to do essentially nothing at all. Moreover, presorted input (or input with a large presorted section) is quite frequent, so using the first element as pivot is an absolutely horrible idea and should be discarded immediately."
            ],
            "guid": "QB]{(@>>},",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of selecting a random pivot in <b>quicksort</b>?",
                "A safe course is merely to choose the pivot randomly. This strategy is generally perfectly safe, unless the random number generator has a flaw (which is not as uncommon as you might think), since it is very unlikely that a random pivot would consistently provide a poor partition. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all."
            ],
            "guid": "HiwzxOYPV@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the advantages and disadvantages of using <b>median-of-three</b> partitioning in <b>quicksort</b>?",
                "The median of a group of N numbers is the [N/2]th largest number. The best choice of pivot would be the median of the array. Unfortunately, this is hard to calculate and would slow down quicksort considerably. A good estimate can be obtained by picking three elements randomly and using the median of these three as pivot. The randomness turns out not to help much, so the common course is to use as pivot the median of the left, right, and center elements. For instance, with input 8, 1, 4, 9, 6, 3, 5, 2, 7, 0, the left element is 8, the right element is 0, and the center (in position (left + right)/2 ) element is 6. Thus, the pivot would be v = 6. Using median-of-three partitioning clearly eliminates the bad case for sorted input (the partitions become equal in this case) and actually reduces the number of comparisons by 14%."
            ],
            "guid": "f/TL-NG[im",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an optimum <b>partitioning strategy</b> for <b>quicksort</b>.",
                "The first step is to get the pivot element out of the way by swapping it with the last element. i starts at the first element and j starts at the next-to-last element.<br><img src=\"paste-738c435a7a53bcb9399576d5c709440d3c2a26bc.jpg\"><br>What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are, of course, relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If&nbsp;i is to the left of j, those elements are swapped. The effect is to push a large element to the right and a small element to the left. In the example above, i would not move and j would slide over one place. The situation is as follows:<br><img src=\"paste-f5dc75ba051a99e6cda769613ce5dcbd62bd33f6.jpg\"><br>We then swap the elements pointed to by i and j and repeat the process until i and j cross:<br><img src=\"paste-87fcc822dc9ddd16c1dd90cabf81c7b2cd9bbf90.jpg\"><br>At this stage, i and j have crossed, so no swap is performed. The final part of the partitioning is to swap the pivot element with the element pointed to by i:<br><img src=\"paste-67067edfbf3988d40e6df9067564e6b098cb2ef6.jpg\"><br>When the pivot is swapped with i in the last step, we know that every element in a position p &lt; i must be small. This is because either position p contained a small element to start with, or the large element originally in position p was replaced during a swap. A similar argument shows that elements in positions p &gt; i must be large."
            ],
            "guid": "F%dn1($N;B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How to handle elements that are equal to the pivot&nbsp;in <b>quicksort</b>?",
                "i and j ought to do the same thing, since otherwise the partitioning step is biased. For instance, if i stops and j does not, then all elements that are equal to the pivot will wind up in S2.&nbsp;<br>It is better to do the unnecessary swaps (and end with a worst case of O(N log N))&nbsp;and create even subarrays than to risk wildly uneven subarrays (and a running time of O(N<sup>2</sup>)). Therefore, we should have both i and j stop if they encounter an element equal to the pivot."
            ],
            "guid": "D&o[8H4wc?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you deal with small (sub)arrays in <b>quicksort</b>?",
                "For very small arrays (N ≤ 20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efficient for small arrays, such as insertion sort. Using this strategy can actually save about 15 percent in the running time (over doing no cutoff at all). A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two."
            ],
            "guid": "P.n3`0S(Hh",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst running time of <b>quicksort</b>&nbsp;and when does it happen?",
                "A running time of O(N<sup>2</sup>) happens when the pivot is always the smallest number."
            ],
            "guid": "u<$DD-N_^r",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best case running time in <b>quicksort</b>&nbsp;and when does it happen?",
                "O(N log N) when the pivot is always in the middle."
            ],
            "guid": "g3B`]};B+2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>quickselect</b>&nbsp;done?",
                "Let |Si| denote the number of elements in Si.<br>1. If |S| = 1, then k = 1 and return the element in S as the answer. If a cutoff for small arrays is being used and |S| ≤ CUTOFF, then sort S and return the kth smallest element.<br>2. Pick a pivot element, v ∈ S.<br>3. Partition S - {v} into S1 and S2, as was done with quicksort.<br>4. If k ≤ |S1|, then the kth smallest element must be in S1. In this case, return quickselect(S1, k). If k = 1 + |S1|, then the pivot is the kth smallest element and we can return it as the answer. Otherwise, the kth smallest element lies in S2, and it is the (k - |S1| - 1)st smallest element in S2. We make a recursive call and return quickselect(S2, k - |S1| - 1)."
            ],
            "guid": "sE8(5dYXm#",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst and average running times of <b>quickselect</b>?",
                "O(N<sup>2</sup>) worst running time.<br>O(N) average running time."
            ],
            "guid": "j4m56m{o-a",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the average and lower bound of time for sorting algorithms that use only comparisons?",
                "Ω(N log N)"
            ],
            "guid": "m=WZA>yH39",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum number of leaves in a binary tree?",
                "Let T be a binary tree of depth d. Then T has at most 2<sup>d</sup> leaves."
            ],
            "guid": "fN;EuILI}G",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a the minimum depth for a binary tree?",
                "A binary tree with L leaves must have depth at least [log L]."
            ],
            "guid": "sMyx*}>fiw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons in the worst case of a sorting algorithms that uses only comparisons?",
                "[log(N!)]"
            ],
            "guid": "FchTaPbG@w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the definition of the&nbsp;<b>information-theoretic</b> lower bound?",
                "The general theorem says that if there are P different possible cases to distinguish, and the questions are of the form YES/NO, then [log P]&nbsp;questions are always required in some case by any algorithm to solve the problem."
            ],
            "guid": "wx<]DnIG&,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the smallest item&nbsp;in a collection?",
                "N-1"
            ],
            "guid": "yhD|mzl%Tk",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the two smallest items in a collection?",
                "N + [log N]&nbsp;- 2"
            ],
            "guid": "d^^)FRw?ci",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How many comparisons are necessary to find the median of a collection?",
                "[3N/2] - O(log N)"
            ],
            "guid": "pvusr~,W<=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves in a decision tree that has all the leaves at depth <i>d</i>&nbsp;or higher?",
                "2<sup>d</sup>"
            ],
            "guid": "tO>wqJ}RP;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons to find the smallest element in a comparisons based algorithm?",
                "N - 1"
            ],
            "guid": "OSUFv0`2A7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves in a decision tree for finding the smallest of N elements?",
                "2<sup>N-1</sup>"
            ],
            "guid": ">Dj-TD9O+",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of leaves of a decision tree for finding the kth smallest of N elements?",
                "<img src=\"paste-58c77b2aea46543e4080a8b4b0a83959d25a4ad7.jpg\">"
            ],
            "guid": "JBL7>L)TXP",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons in any comparison-based algorithm to find the kth smallest element?",
                "<img src=\"paste-3c7b264556f08af3bdb44584095b2fd85c2ca0db.jpg\">"
            ],
            "guid": "zv>E%O:*tC",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the second smallest element?",
                "N + [log N] - 2&nbsp;"
            ],
            "guid": "J>I3W-FvR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the median?",
                "[3N/2] - O(log N)&nbsp;"
            ],
            "guid": "roNMVtvA&?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the minimum number of comparisons for any comparison-based algorithm to find the minimum and maximum?",
                "[3N/2] - 2"
            ],
            "guid": "le|qI*_.|K",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition of <b>bucketsort</b>?",
                "For bucket sort to work, extra information must be available. The input A<sub>1</sub>, A<sub>2</sub>, . . . , A<sub>N</sub> must consist of only positive integers smaller than M.&nbsp;"
            ],
            "guid": "om1.jCH{dB",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>bucketsorting</b>&nbsp;done?",
                "Keep an array called count, of size <i>M</i>, which is initialized to all 0s. Thus, count has <i>M</i> cells, or buckets, which are initially empty. When A<sub>i</sub> is read, increment count[A<sub>i</sub>] by 1. After all the input is read, scan the count array, printing out a representation of the sorted list."
            ],
            "guid": "ddz](5q102",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the runtime of <b>bucketsort</b>?",
                "This algorithm takes O(M + N); where M is the the maximum number in the range, and N is number of sorted values. If M is O(N), then the total is O(N)."
            ],
            "guid": "k[E~}U&1u&",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Does <b>bucketsort</b>&nbsp;violate the comparisons lower bound?",
                "It turns out that it does not because it uses a more powerful operation than simple comparisons. By incrementing the appropriate bucket, the algorithm essentially performs an M-way comparison in unit time."
            ],
            "guid": "Bb*j0o?qnR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>radix sort</b>&nbsp;done?",
                "Suppose we have 10 numbers in the range 0 to 999 that we would like to sort. In general this is N numbers in the range 0 to b<sup>p-1</sup> for some constant p. Obviously we cannot use bucket sort; there would be too many buckets. The trick is to use several passes of bucket sort. The natural algorithm would be to bucket sort by the most significant “digit” (digit is taken to base b), then next most significant, and so on. But a simpler idea is to perform bucket sorts in the reverse order, starting with the least significant “digit” first. Of course, more than one number could fall into the same bucket, and unlike the original bucket sort, these numbers could be different, so we keep them in a list. Each pass is stable: Items that agree in the current digit retain the ordering determined in prior passes.<br>After the first pass, the items are sorted on the least significant digit, and in general, after the kth pass, the items are sorted on the k least significant digits. So at the end, the items are completely sorted.\n<br><img src=\"paste-903a902b305b6352c36549a4f9e3c471e8bc54d8.jpg\">"
            ],
            "guid": "C?!Xo+#e#7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>bucketsort</b>?",
                "O (p(N + b)) where p is the number of passes, N is the number of elements to sort, and b is the number of buckets."
            ],
            "guid": "viH~rp5}=)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use <b>radix sort </b>to sort strings?",
                "If all the strings have the same length L, then by using buckets for each character, we can implement a radix sort in O (NL)&nbsp;time. We assume that all characters are ASCII, residing in the first 256 positions of the Unicode character set. In each pass, we add an item to the appropriate bucket, and then after all the buckets are populated, we step through the buckets dumping everything back to the array. Notice that when a bucket is populated and emptied in the next pass, the order from the current pass is preserved."
            ],
            "guid": "nmVGRkfJ])",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an alternative implementation of <b>radix sort </b>that&nbsp;avoids using vectors to represent buckets?",
                "In <b>counting radix sort</b>, we maintain a count of how many items would go in each bucket; this information can go into an array count, so that count[k] is the number of items that are in bucket k. Then we can use another array offset, so that offset[k]&nbsp;represents the number of items whose value is strictly smaller than k. Then when we see a value k for the first time in the final scan, offset[k] tells us a valid array spot where it can be written to (but we have to use a temporary array for the write), and after that is done, offset[k] can be incremented. Counting radix sort thus avoids the need to keep<br>lists. As a further optimization, we can avoid using offset by reusing the count array. The modification is that we initially have count[k+1] represent the number of items that are in bucket k. Then after that information is computed, we can scan the count array from the smallest to largest index and increment count[k] by count[k-1]. It is easy to verify that after this scan, the count array stores exactly the same information that would have been stored in offset."
            ],
            "guid": "k8FwuBMl<~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which cases can <b>counting radix sort</b>&nbsp;be slower than using buckets?",
                "It can suffer from poor locality (out is filled in non-sequentially) and thus, surprisingly, it is not always faster than using a vector of vectors."
            ],
            "guid": "O~`?]av-:o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we extend <b>radix sort</b>&nbsp;to work with variable length strings?",
                "We can extend either version of radix sort (buckets or counting versions) to work with variable-length strings. The basic algorithm is to first sort the strings by their length. Instead of looking at all the strings,&nbsp;we can then look only at strings that we know are long enough. Since the string lengths are small numbers, the initial sort by length can be done by—bucket sort!"
            ],
            "guid": "P<k[JR@-yJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When does <b>radix sort </b>function especially well for strings?",
                "When the characters in the string are drawn from a reasonably small alphabet and when the strings either are relatively short or are very similar, because the O(N log N) comparison-based sorting algorithms will generally look only at a small number of characters in each string comparison, once the average string length starts getting large, radix sort’s advantage is minimized or evaporates completely."
            ],
            "guid": "zS+bg`;.+y",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What category of sorting algorithms deal with very large amount of input?",
                "External sorting algorithms."
            ],
            "guid": "JF0wd!tI`g",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why do we need <b>external sorting algorithms</b>?",
                "Most of the internal sorting algorithms take advantage of the fact that memory is directly addressable. Shellsort compares elements a[i] and a[i-h<sub>k</sub>] in one time unit. Heapsort compares elements a[i] and a[i*2+1] in one time unit. Quicksort, with median-of-three partitioning, requires comparing a[left], a[center], and a[right] in a constant number of time units. If the input is on a tape, then all these operations lose their efficiency, since elements on a tape can only be accessed sequentially. Even if the data are on a disk, there is still a practical loss of efficiency because of the delay required to spin the disk and move the disk head."
            ],
            "guid": "o)4Ue-ZFG*",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What properties of tapes affect the choice of sorting algorithms?",
                "The wide variety of mass storage devices makes external sorting much more device dependent than internal sorting. The algorithms that we will consider work on tapes, which are probably the most restrictive storage medium. Since access to an element on tape is done by winding the tape to the correct location, tapes can be efficiently accessed only in sequential order (in either direction)."
            ],
            "guid": "hhC9HK4SK(",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the impact of the number of available tape-drives on sorting?",
                "We need two drives to do an efficient sort; the third drive simplifies matters. If only one tape drive is present, then we are in trouble: Any algorithm will require Ω(N<sup>2</sup>) tape accesses."
            ],
            "guid": "j8c|Q[qulN",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>basic/simple external sorting</b> done?",
                "The basic external sorting algorithm uses the merging algorithm from mergesort. Suppose we have four tapes, T<sub>a1</sub>, T<sub>a2</sub>, T<sub>b1</sub>, T<sub>b2</sub>, which are two input and two output tapes. Depending on the point in the algorithm, the a and b tapes are either input tapes or output tapes. Suppose the data are initially on T<sub>a1</sub>. Suppose further that the internal memory can hold (and sort) M records at a time. A natural first step is to read M records at a time from the input tape, sort the records internally, and then write the sorted records alternately to T<sub>b1</sub> and T<sub>b2</sub>. We will call each set of sorted records a run. When this is done, we rewind all the tapes.<br><img src=\"paste-caf8b352f892a58b68a9e7b4ad46bd79b4601262.jpg\"><br>If M = 3, then after the runs are constructed, the tapes will contain the data indicated in the following figure:<br><img src=\"paste-7b42642087ca77b8ce7a1a0796b5e802ba1fd873.jpg\"><br>Now T<sub>b1</sub> and T<sub>b2</sub> contain a group of runs. We take the first run from each tape and merge them, writing the result, which is a run twice as long, onto T<sub>a1</sub>. Recall that merging two sorted lists is simple; we need almost no memory, since the merge is performed as T<sub>b1</sub> and T<sub>b2</sub> advance. Then we take the next run from each tape, merge these, and write the result to T<sub>a2</sub>. We continue this process, alternating between T<sub>a1</sub> and T<sub>a2</sub>, until either T<sub>b1</sub> or T<sub>b2</sub> is empty. At this point either both are empty or there is one run left. In the latter case, we copy this run to the appropriate tape. We rewind all four tapes and repeat the same steps, this time using the a tapes as input and the b tapes as output. This will give runs of 4M. We continue the process until we get one run of length N.<br><img src=\"paste-5baf2e4075678e9bcee1aac407046b85f241d7cb.jpg\"><br><img src=\"paste-ba5aa4733fa77deb9fc04a339f78c672d5ada772.jpg\">"
            ],
            "guid": "DYq3^T,S>t",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>simple/basic external sorting</b>?",
                "The algorithm will require [log(N/M)]&nbsp;passes, plus the initial run-constructing pass. For instance, if we have 10 million records of 128 bytes each, and four megabytes of internal memory, then the first pass will create 320 runs. We would then need nine more passes to complete the sort."
            ],
            "guid": "lk-L-%7tsw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you do <b>multiway sort</b>?",
                "If we have extra tapes, then we can expect to reduce the number of passes required to sort our input. We do this by extending the basic (two-way) merge to a k-way merge.&nbsp;Merging two runs is done by winding each input tape to the beginning of each run. Then the smaller element is found, placed on an output tape, and the appropriate input tape is advanced. If there are k input tapes, this strategy works the same way, the only difference being that it is slightly more complicated to find the smallest of the k elements. We can find the smallest of these elements by using a priority queue. To obtain the next element to write on the output tape, we perform a deleteMin operation. The appropriate input tape is advanced, and if the run on the input tape is not yet completed, we insert the new element into the priority queue.<br><img src=\"paste-47afc056d248665879e4b7ebd51d16ba73463b97.jpg\"><br>We then need two more passes of three-way merging to complete the sort.<br><img src=\"paste-f365a5f98e8d9e4dffd0def3b7cdc75697fd27fc.jpg\"><br><img src=\"paste-63996316413844fd2e89e376fb4f25f9ce1cf015.jpg\">"
            ],
            "guid": "nEFLj]kw5-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>multiway sort</b>?",
                "After the initial run construction phase, the number of passes required using k-way merging is [log<sub>k</sub>(N/M)], because the runs get k times as large in each pass."
            ],
            "guid": "uvD]{[XdTf",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the benefit of <b>polyphase merge</b>?",
                "The <i>k</i>-way merging strategy requires the use of 2<i>k</i> tapes. This could be prohibitive for some applications. It is possible to get by with only <i>k</i> + 1&nbsp;tapes."
            ],
            "guid": "QkSXoxg1ka",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>polyphase sort</b>&nbsp;done?",
                "Suppose we have three tapes, T<sub>1</sub>, T<sub>2</sub>, and T<sub>3</sub>, and an input file on T<sub>1</sub> that will produce 34 runs. Suppose we put 21 runs on T<sub>2</sub> and 13 runs on T<sub>3</sub>. We would then merge 13 runs onto T<sub>1</sub> before T<sub>3</sub> was empty. At this point, we could rewind T<sub>1</sub> and T<sub>3</sub>, and merge T<sub>1</sub>, with 13 runs, and T<sub>2</sub>, which has 8 runs, onto T<sub>3</sub>. We could then merge 8 runs until T<sub>2</sub> was empty, which would leave 5 runs left on T<sub>1</sub> and 8 runs on T<sub>3</sub>. We could then merge T<sub>1</sub> and T<sub>3</sub>, and so on.<br><img src=\"paste-fcd4ea04e9fb20e6fec8937d1ce6ab1e6fcab292.jpg\">"
            ],
            "guid": "lOI_1_HOC;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best way to distribute runs in <b>polyphase sort</b>?",
                "If the number of runs is a Fibonacci number F<sub>N</sub>, then the best way to distribute them is to split them into two Fibonacci numbers F<sub>N-1</sub> and F<sub>N-2</sub>. Otherwise, it is necessary to pad the tape with dummy runs in order to get the number of runs up to a Fibonacci number."
            ],
            "guid": "t1P-43m,1$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>replacement selection</b>&nbsp;done?",
                "Initially, M records are read into memory and placed in a priority queue. We perform a deleteMin, writing the smallest (valued) record to the output tape. We read the next record from the input tape. If it is larger than the record we have just written, we can add it to the priority queue. Otherwise, it cannot go into the current run. Since the priority queue is smaller by one element, we can store this new element in the dead space of the priority queue until the run is completed and use the element for the next run. Storing an element in the dead space is similar to what is done in heapsort. We continue doing this until the size of the priority queue is zero, at which point the run is over. We start a new run by building a new priority queue, using all the elements in the dead space.<br><img src=\"paste-91103479777465e4fdeb255e7b0eebaeec1a30de.jpg\">"
            ],
            "guid": "Jm)`MKtqkw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which cases <b>replacement selection</b>&nbsp;is useful?",
                "It is possible for replacement selection to do no better than the standard algorithm. However, the input is frequently sorted or nearly sorted to start with, in which case replacement selection produces only a few very long runs. This kind of input is common for external sorts and makes replacement selection extremely valuable."
            ],
            "guid": "cxhzi/!}g<",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>relation</b>.",
                "A relation R is defined on a set S if for every pair of elements (a, b), a, b ∈ S, a R b is either true or false. If a R b is true, then we say that a is related to b."
            ],
            "guid": "lxi~zw]@Qu",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define an <b>equivalence relation</b>.",
                "An equivalence relation is a relation R that satisfies three properties:<br>1. (Reflexive) a R a, for all a ∈ S.<br>2. (Symmetric) a R b if and only if b R a.<br>3. (Transitive) a R b and b R c implies that a R c."
            ],
            "guid": "P>G7G2nGL^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why ≤ relationship is not an <b>equivalence relationship</b>?",
                "Although it is reflexive, since a ≤ a, and transitive, since a ≤ b and b ≤ c implies a ≤ c, it is not symmetric, since a ≤ b does not imply b ≤ a."
            ],
            "guid": "zkYTeb8;+A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Examples of <b>equivalence relation</b>.",
                "Electrical connectivity, where all connections are by metal wires, is an equivalence relation. The relation is clearly reflexive, as any component is connected to itself. If a is electrically connected to b, then b must be electrically connected to a, so the relation is symmetric. Finally, if a is connected to b and b is connected to c, then a is connected to c. Thus electrical connectivity is an equivalence relation.<br>Two cities are related if they are in the same country. It is easily verified that this is an equivalence relation. Suppose town a is related to b if it is possible to travel from a to b by taking roads. This relation is an equivalence relation if all the roads are two-way."
            ],
            "guid": "I3,WsD;XH@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the <b>equivalence class</b>?",
                "The equivalence class of an element a ∈ S is the subset of S that contains all the elements that are related to a. Notice that the equivalence classes form a partition of S: Every member of S appears in exactly one equivalence class. To decide if a ∼ b, we need only to check whether a and b are in the same equivalence class. This provides our strategy to solve the equivalence problem."
            ],
            "guid": "m*r2_07#mv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>disjoint</b>&nbsp;sets.",
                "When we have a collection of N sets, each with one element. This initial representation is that all relations (except reflexive relations) are false. Each set has a different element, so that Si ∩ Sj = ∅; this makes the sets disjoint."
            ],
            "guid": "y-.BVE`HNs",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the disjoint set <b>union/find algorithm</b>.",
                "There are two permissible operations. The first is find, which returns the name of the set (that is, the equivalence class) containing a given element. The second operation adds relations. If we want to add the relation a ∼ b, then we first see if a and b are already related. This is done by performing finds on both a and b and checking whether they are in the same equivalence class. If they are not, then we apply union.1 This operation merges&nbsp;the two equivalence classes containing a and b into a new equivalence class. From a set point of view, the result of ∪ is to create a new set Sk = S<sub>i</sub> ∪S<sub>j</sub>, destroying the originals and preserving the disjointness of all the sets."
            ],
            "guid": "v~!B7c=<&4",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of <b>union/find algorithm</b>?",
                "This algorithm is dynamic because, during the course of the algorithm, the sets can change via the union operation. The algorithm must also operate online: When a find is performed, it must give an answer before continuing. Another possibility would be an offline algorithm. Such an algorithm would be allowed to see the entire sequence of unions and finds. The answer it provides for each find must still be consistent with all the unions that were performed up until the find, but the algorithm can give all its answers after it has seen all the questions."
            ],
            "guid": "CHzmdfjs/D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Do we perform any operations comparing the relative values of elements&nbsp;in <b>union/find algorithm</b>?",
                "We do not perform any operations comparing the relative values of elements but merely require knowledge of their location. For this reason, we can assume that all the elements have been numbered sequentially from 0 to N - 1 and that the numbering can&nbsp;be determined easily by some hashing scheme. Thus, initially we have S<sub>i</sub> = {i} for i = 0 through N - 1."
            ],
            "guid": "GwV>EypCW0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the name of the set returned by find in <b>union/find algorithm</b>?",
                "The name of the set returned by find is actually fairly arbitrary. All that really matters is that find(a)==find(b) is true if and only if a and b are in&nbsp;the same set."
            ],
            "guid": "f;*Ad3To6o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the strategies of using union/find algorithm to solve graph theory problems?",
                "There are two strategies to solve this problem. One ensures that the find instruction can be executed in constant worst-case time, and the other ensures that the union instruction can be executed in constant worst-case time. It has recently been shown that both cannot be done simultaneously in constant worst-case time."
            ],
            "guid": "xOPEo_[U8u",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we implement a fast <b>find</b>&nbsp;operation in a <b>union/find algorithm</b>?",
                "We maintain, in an array, the name of the equivalence class for each element. Then find is just a simple O(1) lookup. Suppose we want to perform union(a,b). Suppose that a is in equivalence class <i>i</i> and <i>b</i> is in equivalence class <i>j</i>. Then we scan down the array, changing all <i>i</i>s to <i>j</i>. Unfortunately, this scan takes Θ(N). Thus, a sequence of N - 1 unions (the maximum, since then everything is in one set) would take Θ(N<sup>2</sup>) time. If there are Θ(N<sup>2</sup>) find operations, this performance is fine, since the total running time would then amount to O(1) for each union or find operation over the course of the algorithm. If there are fewer finds, this bound is not acceptable.<br>One idea is to keep all the elements that are in the same equivalence class in a linked list. This saves time when updating, because we do not have to search through the entire array. This by itself does not reduce the asymptotic running time, because it is still possible to perform Θ(N<sup>2</sup>) equivalence class updates over the course of the algorithm. If we also keep track of the size of each equivalence class, and when performing unions&nbsp;we change the name of the smaller equivalence class to the larger, then the total time spent for N - 1 merges is O(N log N). The reason for this is that each element can have its equivalence class changed at most log N times, since every time its class is changed, its new equivalence class is at least twice as large as its old. Using this strategy, any sequence of M finds and up to N - 1 unions takes at most O(M + N log N) time."
            ],
            "guid": "n;[)&BE*/M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we use to represent <b>disjoint sets</b>&nbsp;and why?",
                "Since the problem does not require that a find operation return any specific name, just that finds on two elements return the same answer if and only if they are in the same set. One idea might be to use a tree to represent each set, since each element in a tree has the same root. Thus, the root can be used to name the set. We will represent each set by a tree."
            ],
            "guid": "qnU#|/v@h;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we use to store a <b>disjoint set</b> and why?",
                "The name of a set is given by the node at the root. Since only the name of the parent is required, we can assume that this tree is stored implicitly in an array: Each entry s[i] in the array represents the parent of element i. If <i>i</i> is a root, then s[i] = -1."
            ],
            "guid": "zC5!&f[M%|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we perform a <b>union</b>&nbsp;in disjoint sets?",
                "<img src=\"paste-4b526a7803daa2d53b8d65b68011d4e7a4e1868d.jpg\"><br>To perform a union of two sets, we merge the two trees by making the parent link of one tree’s root link to the root node of the other tree. It should be clear that this operation takes constant time. The figures below represent the forest after each of union(4,5), union(6,7), union(4,6), where we have adopted the convention that the new root after the union(x,y) is x.<br><img src=\"paste-fba65d001b8e1a9d997ba993bd4509122620b436.jpg\"><br><img src=\"paste-a6916e3524fa8b221406150c1bfe4ba991b05208.jpg\"><br><img src=\"paste-87776b5f92b11e71cc42e802fe70751b1fa7886d.jpg\"><br>The implicit representation of the last forest is shown below:<br><img src=\"paste-07f33a00b349b29d8793850932732e3b650a8f31.jpg\">"
            ],
            "guid": "puvLU__~%?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is a <b>find(x) </b>operation implemented in disjoint sets and what's its running time?",
                "A find(x) on element x is performed by returning the root of the tree containing x. The time to perform this operation is proportional to the depth of the node representing x, assuming, of course, that we can find the node representing x in constant time. Using the strategy above, it is possible to create a tree of depth N - 1, so the worst-case running&nbsp;time of a find is Θ(N). Typically, the running time is computed for a sequence of M intermixed instructions. In this case, M consecutive operations could take Θ(MN) time in the worst case."
            ],
            "guid": "su:/{E$w4q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>union-by-size</b>.",
                "<img src=\"paste-ab4f56d3d662b0d8adaf1725fc24d46e908fbda4.jpg\"><br>The unions above were performed rather arbitrarily, by making the second tree a subtree of the first. A simple improvement is always to make the smaller tree a subtree of the larger, breaking ties by any method.<br><img src=\"paste-8e804226c87e81ef0ae579d897618d88159d2fd8.jpg\">"
            ],
            "guid": "e~=~F|XKw7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the maximum depth and running time when unions are done by size?",
                "The depth of any node is never more than log N. The running time for a find operation is O(log N), and a sequence of M operations takes O(M log N).<br><img src=\"paste-fc20d98b336d1c6124d0f423d7f44ec2defc2908.jpg\">"
            ],
            "guid": "n=Qj&o86%3",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>union-by-size</b>&nbsp;implemented?",
                "To implement this strategy, we need to keep track of the size of each tree. Since we are really just using an array, we can have the array entry of each root contain the <i>negative</i> of&nbsp;the size of its tree. Thus, initially the array representation of the tree is all -1s. When a union is performed, check the sizes; the new size is the sum of the old. Thus, union-by-size is not at all difficult to implement and requires no extra space. It is also fast, on average. For virtually all reasonable models, it has been shown that a sequence of M operations requires O(M) average time if union-by-size is used. This is because when random unions are performed, generally very small (usually one-element) sets are merged with large sets throughout the algorithm."
            ],
            "guid": "B~Zc30psCL",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What alternative to union-by-size guarantees a maximum depth of O(log N)?",
                "In <b>union-by-height,</b>&nbsp;we keep track of the height, instead of the size, of each tree and perform unions by making the shallow tree a subtree of the deeper tree. This is an easy algorithm, since the height of a tree increases only when two equally deep trees are joined (and then the height goes up by one). Thus, union-by-height is a trivial modification of union-by-size. Since heights of zero would not be negative, we actually store the negative of height, minus an additional 1. Initially, all entries are -1.<br><img src=\"paste-457b62ee8e12e5c8aed91c0f1ee159930cc655c3.jpg\">"
            ],
            "guid": "jjS{o0WJ`$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the shortcomings of the <b>union/find algorithm</b>?",
                "the worst case of O(M log N) can occur fairly easily and naturally. For instance, if we put all the sets on a queue and repeatedly dequeue the first two sets and enqueue the union, the worst case occurs. If there are many more finds than unions, this running time is worse than that of the quick-find algorithm. Moreover, it should be clear that there are probably no more improvements possible for the union algorithm. This is based on the observation that any method to perform the unions will yield the same worst-case trees, since it must break ties arbitrarily."
            ],
            "guid": "Q$Tay|tjnN",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is <b>path compression</b>&nbsp;done?",
                "Path compression is performed during a find operation and is independent of the strategy used to perform unions. Suppose the operation is find(x). Then the effect of path compression is that every node on the path from x to the root has its parent changed to the root.<br>After find(14), the following worst case<br><img src=\"paste-c3ece164e96901bb96c7095fb58bfad0ad9721d9.jpg\"><br>changes to<br><img src=\"paste-bd40421692b51039e97b050744017ef77be4b7c4.jpg\">"
            ],
            "guid": "f14/hIonm,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "When does<b> </b>the <b>path compression </b>work best and what's its impact?",
                "When unions are done arbitrarily, path compression is a good idea, because there is an abundance of deep nodes and these are brought near the root by path compression. It has been proven that when path compression is done in this case, a sequence of <i>M</i>&nbsp;operations requires at most O(M log N) time. It is still an open problem to determine what the average-case behavior is in this situation."
            ],
            "guid": "Kh`yl]W)Z,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What algorithm is <b>path compression</b>&nbsp;compatible with?",
                "Path compression is perfectly compatible with union-by-size, and thus both routines can be implemented at the same time. Since doing union-by-size by itself is expected to execute a sequence of <i>M</i> operations in linear time, it is not clear that the extra pass involved in path compression is worthwhile on average."
            ],
            "guid": "m!QYydYN1$",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What algorithm is&nbsp;<b>path compression</b>&nbsp;not compatible with and why?",
                "Path compression is not entirely compatible with union-by-height, because path compression can change the heights of the trees. It is not at all clear how to recompute them efficiently. The answer is do not! Then the heights stored for each tree become estimated heights (sometimes known as ranks), but it turns out that union-by-rank (which is what this has now become) is just as efficient in theory as union-by-size. Furthermore, heights<br>are updated less often than sizes. As with union-by-size, it is not clear whether path compression is worthwhile on average."
            ],
            "guid": "Kj<X1F<Pn:",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the worst case for union-by-rank and path compression?",
                "When both heuristics are used, the algorithm is almost linear in the worst case. Specifically, the time required in the worst case is Θ(Mα(M, N)) (provided M ≥ N), where α(M, N) is an incredibly slowly growing function that for all intents and purposes is at most 5 for any problem instance. However, α(M, N) is not a constant, so the running time is not linear."
            ],
            "guid": "luY{0sAg`/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How are <b>disjoint sets</b>&nbsp;used to create mazes?",
                "A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit). We then continually choose a wall randomly, and knock it down if the cells that the wall separates are not already connected to each other. If we repeat this process until the starting and ending cells are connected, then we have a maze. It is actually better to continue knocking down walls until every cell is reachable from every other cell (this generates more false leads in the maze).<br>We illustrate the algorithm with a 5-by-5 maze. The figure below shows the initial configuration. We use the union/find data structure to represent sets of cells that are connected to each other. Initially, walls are everywhere, and each cell is in its own equivalence class.<br><img src=\"paste-4a5cd15f5efd5d6e9747aed1bfdfa65c1ba9fc97.jpg\"><br>This figure shows a later stage of the algorithm, after a few walls have been knocked down:<br><img src=\"paste-8c3366c4c273a57f0546432ae3c0ce7b47fe1b32.jpg\"><br>Suppose, at this stage, the wall that connects cells 8 and 13 is randomly targeted. Because 8 and 13 are already connected (they are in the same set), we would not remove the wall, as it would simply trivialize the maze. Suppose that cells 18 and 13 are randomly targeted next. By performing two find operations, we see that these are in different sets; thus 18 and 13 are not already connected. Therefore, we knock down the wall that separates them.<br><img src=\"paste-3328f29f8fbae67b793800c5aaf275c9f6b419f8.jpg\"><br>Notice that as a result of this operation, the sets containing 18 and 13 are combined via a union operation. This is because everything that was connected to 18 is now connected to everything that was connected to 13. At the end of the algorithm everything is connected, and we are done.<br><img src=\"paste-8ef2fbc44c81120e0c6247cc1ea09721ab957da7.jpg\">"
            ],
            "guid": "Ids/4BHkUS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the maze generation algorithm?",
                "The running time of the algorithm is dominated by the union/find costs. The size of the union/find universe is equal to the number of cells. The number of find operations is proportional to the number of cells, since the number of removed walls is one less than the number of cells, while with care, we see that there are only about twice the number of walls as cells in the first place. Thus, if N is the number of cells, since there are two finds&nbsp;per randomly targeted wall, this gives an estimate of between (roughly) 2N and 4N find operations throughout the algorithm. Therefore, the algorithm's running time can be taken as O(N log∗ N), and this algorithm quickly generates a maze."
            ],
            "guid": "wR(R4,lRc}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>graph</b>.",
                "A graph <i>G </i>= (<i>V</i>,<i> E</i>) consists of a set of <b>vertices</b>, <i>V</i>, and a set of <b>edges</b>, <i>E</i>. Each edge is a pair (v, w), where v, w ∈ V. Edges are sometimes referred to as arcs. If the pair is ordered, then the graph is <b>directed</b>. Directed graphs are sometimes referred to as <b>digraphs</b>. Vertex <i>w</i> is adjacent to <i>v</i> if and only if (<i>v</i>, <i>w</i>) ∈ <i>E</i>. In an undirected graph with edge (<i>v</i>, <i>w</i>), and hence (<i>w</i>, <i>v</i>), <i>w</i> is adjacent to <i>v</i> and <i>v</i> is adjacent to <i>w</i>. Sometimes an edge has a third component, known as either a <b>weight</b> or a <b>cost</b>."
            ],
            "guid": "P-^L>CE3O;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>path</b> in a graph.",
                "A path in a graph is a sequence of vertices <i>w1</i>, <i>w2</i>, <i>w3</i>, . . . , <i>wN</i> such that (<i>wi</i>, <i>w</i><sub>i+1</sub>) ∈ <i>E</i> for 1 ≤ i &lt; <i>N</i>. The length of such a path is the number of edges on the path, which is equal to <i>N</i> - 1. We allow a path from a vertex to itself; if this path contains no edges, then the path length is 0. This is a convenient way to define an otherwise special case. If the graph contains an edge (<i>v</i>, <i>v</i>) from a vertex to itself, then the path <i>v</i>, <i>v</i> is sometimes referred to as a <b>loop</b>. The graphs we will consider will generally be loopless. A <b>simple path</b> is a path such that all vertices are distinct, except that the first and last could be the same."
            ],
            "guid": "jlaI[@$SRv",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>cycle</b>&nbsp;in a graph.",
                "A <b>cycle</b> in a directed graph is a path of length at least 1 such that w<sub>1</sub> = w<sub>N</sub>; this cycle is simple if the path is simple. For undirected graphs, we require that the edges be distinct. The logic of these requirements is that the path u, v, u in an undirected graph should not be considered a cycle, because (u, v) and (v, u) are the same edge. In a directed graph, these are different edges, so it makes sense to call this a cycle. A directed graph is <b>acyclic</b> if it has no cycles. A directed acyclic graph is sometimes referred to by its abbreviation, <b>DAG</b>."
            ],
            "guid": "Fo@/R$2W%f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>connected</b>&nbsp;graph.",
                "An undirected graph is <b>connected</b> if there is a path from every vertex to every other vertex. A directed graph with this property is called <b>strongly connected</b>. If a directed graph is not strongly connected, but the underlying graph (without direction to the arcs) is connected, then the graph is said to be <b>weakly connected</b>. A <b>complete graph</b> is a graph in which there is an edge between every pair of vertices."
            ],
            "guid": "KZ;jDJiA%A",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you model an airport system using a graph.",
                "Each airport is a vertex, and two vertices are connected by an edge if there is a nonstop flight from the airports that are represented by the vertices. The edge could have a weight, representing the time, distance, or cost of the flight. It is reasonable to assume that such a graph is directed, since it might take longer or cost more (depending on local taxes, for example) to fly in different directions. We would probably like to make sure that the airport system is strongly connected, so that it is always possible to fly from any airport to any other airport. We might also like to quickly determine the best flight between any two airports. “Best” could mean the path with the fewest number of edges or could be taken with respect to one, or all, of the weight measures."
            ],
            "guid": "g$>rPS8V:^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you model traffic flow using a graph.",
                "Traffic flow can be modeled by a graph. Each street intersection represents a vertex, and each street is an edge. The edge costs could represent, among other things, a speed limit or a capacity (number of lanes). We could then ask for the shortest route or use this information to find the most likely location for bottlenecks."
            ],
            "guid": "Iusiw?E_GF",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use a two-dimensional array to represent a <b>graph</b>?",
                "This is known as an adjacency matrix representation. For each edge (u, v), we set A[u][v] to true; otherwise the entry in the array is false. If the edge has a weight associated with it, then we can set A[u][v] equal to the weight and use either a very large or a very small weight as a sentinel to indicate nonexistent edges. For instance, if we were looking for the cheapest airplane route, we could represent nonexistent flights with a cost of ∞. If we were looking, for some strange reason, for the most expensive airplane route, we could use -∞ (or perhaps 0) to represent nonexistent edges."
            ],
            "guid": "C[PW2>is?0",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the pros and cons of using an <b>adjacency matrix</b> to represent a <b>graph</b>?",
                "Although this has the merit of extreme simplicity, the space requirement is Θ(|V|<sup>2</sup>), which can be prohibitive if the graph does not have very many edges. An adjacency matrix is an appropriate representation if the graph is dense: |E| = Θ(|V|<sup>2</sup>). In most of the applications that we shall see, this is not true. For instance, suppose the graph represents a street map. Assume a Manhattan-like orientation, where almost all the streets run either<br>north-south or east-west. Therefore, any intersection is attached to roughly four streets, so if the graph is directed and all streets are two-way, then |E| ≈ 4|V|. If there are 3,000 intersections, then we have a 3,000-vertex graph with 12,000 edge entries, which would require an array of size 9,000,000. Most of these entries would contain zero. This is intuitively bad, because we want our data structures to represent the data that are actually there and not the data that are not present."
            ],
            "guid": "uVMZ?Hh1K=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the best way to represent <b>sparse graphs</b>?",
                "If the graph is not dense, in other words, if the graph is sparse, a better solution is an adjacency list representation. For each vertex, we keep a list of all adjacent vertices. The space requirement is then O(|E| + |V|), which is linear in the size of the graph.&nbsp;If the edges have weights, then this additional information is also stored in the adjacency lists.<br><img src=\"paste-cb4c91bbb05e86bff147aad9a00eba584049bbeb.jpg\">"
            ],
            "guid": "vmAPh%8f/@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What options do we have for representing an&nbsp;<b>adjacency list</b>?",
                "There are several alternatives for maintaining the adjacency lists. First, observe that the lists themselves can be maintained in either vectors or lists. However, for sparse graphs, when using vectors, the programmer may need to initialize each vector with a smaller capacity than the default; otherwise, there could be significant wasted space.<br>Because it is important to be able to quickly obtain the list of adjacent vertices for any vertex, the two basic options are to use a map in which the keys are vertices and the values are adjacency lists, or to maintain each adjacency list as a data member of a Vertex class. The first option is arguably simpler, but the second option can be faster, because it avoids repeated lookups in the map.<br>In the second scenario, if the vertex is a string (for instance, an airport name, or the name of a street intersection), then a map can be used in which the key is the vertex name and the value is a Vertex (typically a pointer to a Vertex), and each Vertex object keeps a list of (pointers to the) adjacent vertices and perhaps also the original string name."
            ],
            "guid": "I7.n6o@Hvj",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>topological sort</b>.",
                "A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from v<sub>i</sub> to v<sub>j</sub>, then v<sub>j</sub> appears after v<sub>i</sub> in the ordering. The graph represents the course prerequisite structure at a state university in Miami. A directed edge (v, w) indicates that course v must be completed before course w may be attempted. A topological ordering of these courses is any course sequence that does not violate the prerequisite requirement.<br><img src=\"paste-17866d9c05d567f5b8aab12cef9e74a9604c2d4e.jpg\">"
            ],
            "guid": "iz4).2~AbQ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "In which case <b>topological order</b> is not possible?",
                "It is clear that a topological ordering is not possible if the graph has a cycle, since for two vertices v and w on the cycle, v precedes w and w precedes v. Furthermore, the ordering is not necessarily unique; any legal ordering will do. In the graph below, v<sub>1</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, v<sub>3</sub>, v<sub>7</sub>, v<sub>6</sub> and v<sub>1</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, v<sub>7</sub>, v<sub>3</sub>, v<sub>6</sub> are both topological orderings.<br><img src=\"paste-e5e3b9da0df9c8bf4a2cc600b2530c17aa65f2ae.jpg\">"
            ],
            "guid": "srk/(A4ApV",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a strategy to find a <b>topological order.</b>",
                "A simple algorithm to find a <b>topological ordering</b> is first to find any vertex with no incoming edges. We can then print this vertex, and remove it, along with its edges, from the graph. Then we apply this same strategy to the rest of the graph."
            ],
            "guid": "zb4XEltT{^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>indegree</b>&nbsp;and how is it calculated?",
                "We define the <b>indegree</b> of a vertex v as the number of edges (u, v). We compute the indegrees of all vertices in the graph."
            ],
            "guid": "vJas#rB[i^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we use <b>indegree</b>s&nbsp;to perform <b>toplogy sorting</b>?",
                "Assuming that the indegree for each&nbsp;vertex is stored, and that the graph is read into an adjacency list, we can then apply the following algorithm to generate a topological ordering:<br><img src=\"paste-48c72e2bd7fcaa1e3ca1123c2f664a2f98e163be.jpg\"><br>The function <b>findNewVertexOfIndegreeZero</b> scans the array of vertices looking for a vertex with indegree 0 that has not already been assigned a topological number. It returns NOT_A_VERTEX if no such vertex exists; this indicates that the graph has a cycle. Because <b>findNewVertexOfIndegreeZero</b> is a simple sequential scan of the array of vertices, each call to it takes O(|V|) time. Since there are |V| such calls, the running time of the algorithm is O(|V|<sup>2</sup>)."
            ],
            "guid": "HO,?xlK~@Z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we optimize scanning the vertices array in <b>topological sort</b>?",
                "By paying more careful attention to the data structures, it is possible to do better. The cause of the poor running time is the sequential scan through the array of vertices. If the&nbsp;graph is sparse, we would expect that only a few vertices have their indegrees updated during each iteration. However, in the search for a vertex of indegree 0, we look at (potentially) all the vertices, even though only a few have changed.<br>We can remove this inefficiency by keeping all the (unassigned) vertices of indegree 0 in a special box. The <b>findNewVertexOfIndegreeZero</b> function then returns (and removes) any vertex in the box. When we decrement the indegrees of the adjacent vertices, we check each vertex and place it in the box if its indegree falls to 0.<br>To implement the box, we can use either a stack or a queue; we will use a queue. First, the indegree is computed for every vertex. Then all vertices of indegree 0 are placed on an initially empty queue. While the queue is not empty, a vertex v is removed, and all vertices adjacent to v have their indegrees decremented. A vertex is put on the queue as soon as its indegree falls to 0. The topological ordering then is the order in which the vertices dequeue.<br><img src=\"paste-1a3a3e924c2529e0d3ae6665efac1ed9e27c9040.jpg\"><br><img src=\"paste-4ff51f5cbde13fe4ada98cb70b204f67f7312e7d.jpg\">"
            ],
            "guid": "EGje*=I9(O",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the queue/stack based optimized version of the <b>topological sort</b>?",
                "The time to perform this algorithm is O(|E| + |V|) if adjacency lists are used. This is apparent when one realizes that the body of the for loop is executed at most once per edge. Computing the indegrees can be done with the following code; this same logic shows that the cost of this computation is O(|E| + |V|), even though there are nested loops.<br><img src=\"paste-403714400f63227de204855151ee2448cb59f6b7.jpg\"><br>The queue operations are done at most once per vertex, and the other initialization steps, including the computation of indegrees, also take time proportional to the size of the graph."
            ],
            "guid": "EBL9Yvg`Q)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>weighted</b>&nbsp;and <b>unweighted path length</b>.",
                "The input is a weighted graph: Associated with each edge (v<sub>i</sub>, v<sub>j</sub>) is a cost c<sub>i</sub>,j to traverse the edge. The cost of a path v<sub>1&nbsp;</sub>v<sub>2</sub> . . . v<sub>N</sub> is:<br><img src=\"paste-297dd7624892ca12ddf629143366e2f8d1c4d8ac.jpg\"><br>This is referred to as the <b>weighted path length</b>. The <b>unweighted path length</b> is merely the number of edges on the path, namely, N - 1."
            ],
            "guid": "C$>gjGCE^@",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define the <b>Single-Source Shortest-Path Problem</b>.",
                "Given as input a weighted graph, G = (V, E), and a distinguished vertex, s, find the shortest weighted path from s to every other vertex in G."
            ],
            "guid": "k?qYM6{2@T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the problems related to edges of negative cost.",
                "The graph below shows the problems that negative edges can cause. The path from v<sub>5</sub> to v<sub>4</sub> has cost 1, but a shorter path exists by following the loop v<sub>5</sub>, v<sub>4</sub>, v<sub>2</sub>, v<sub>5</sub>, v<sub>4</sub>, which has cost -5. This path is still not the shortest, because we could stay in the loop arbitrarily long. Thus, the shortest path between these two points is undefined. Similarly, the shortest path from v<sub>1</sub> to v<sub>6</sub> is undefined, because we can get into the same loop. This loop is known as a <b>negative-cost cycle</b>; when one is present in the graph, the shortest paths are not defined. Negative-cost edges are not necessarily bad, as the cycles are, but their presence seems to make the problem harder. For convenience, in the absence of a negative-cost cycle, the shortest path from s to s is zero.<br><img src=\"paste-237767969ba2af0d632b596cf4807653c7968c4a.jpg\">"
            ],
            "guid": "Ol=.gD982^",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Examples of problems that require solving the <b>shortest path</b>.",
                "If the vertices represent computers; the edges represent a link between computers; and the costs represent communication costs (phone bill per a megabyte of data), delay costs (number of seconds required to transmit a megabyte), or a combination of these and other factors, then we can use the shortest-path algorithm to find the cheapest way to send electronic news from one computer to a set of other computers.<br>We can model airplane or other mass transit routes by graphs and use a shortestpath algorithm to compute the best route between two points. In this and many practical applications, we might want to find the shortest path from one vertex, s, to only one other vertex, t. Currently there are no algorithms in which finding the path from s to one vertex is any faster (by more than a constant factor) than finding the path from s to all vertices."
            ],
            "guid": "tYkYD/GJ|[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the various <b>shortest-path problems</b>?",
                "First, we will consider the unweighted shortest-path problem which solves it in O(|E|+|V|). Next, to solve the weighted shortest-path problem if we assume that there are no negative edges, the running time for this algorithm is O(|E| log |V|) when implemented&nbsp;with reasonable data structures.<br>If the graph had negative edges, a simple solution unfortunately, has a poor time bound of O(|E| · |V|). Finally, we will solve the weighted problem for the special case of acyclic graphs in linear time."
            ],
            "guid": "CP=D[Y@HuH",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>unweighted shortest-path</b>&nbsp;algorithm.",
                "Figure below shows an unweighted graph, G. Using some vertex, s, which is an input parameter, we would like to find the shortest path from s to all other vertices. We are only interested in the number of edges contained on the path, so there are no weights on the edges. This is clearly a special case of the weighted shortest-path problem, since we could assign all edges a weight of 1.<br><img src=\"paste-9bb862de47a6f79008059b54abc38c38cbfcb6ba.jpg\"><br>For now, suppose we are interested only in the length of the shortest paths, not in the actual paths themselves. Keeping track of the actual paths will turn out to be a matter of simple bookkeeping.&nbsp;<br>Suppose we choose s to be v<sub>3</sub>. Immediately, we can tell that the shortest path from s to v<sub>3</sub> is then a path of length 0. We can mark this information, obtaining the graph in this figure:<br><img src=\"paste-fab6fadd9d2ea5b29bb71abf162638fd40021857.jpg\"><br>Now we can start looking for all vertices that are a distance 1 away from s. These can be found by looking at the vertices that are adjacent to s. If we do this, we see that v<sub>1</sub> and v<sub>6</sub> are one edge from s.<br><img src=\"paste-e47f2bd70eed027f29a4e1a7b9380fabd4c0008f.jpg\"><br>We can now find vertices whose shortest path from s is exactly 2, by finding all the vertices adjacent to v<sub>1</sub> and v<sub>6</sub> (the vertices at distance 1), whose shortest paths are not&nbsp;already known. This search tells us that the shortest path to v<sub>2</sub> and v<sub>4</sub> is 2.<br><img src=\"paste-61808f2da8a62723216eb4bb649e3c6a7eec98ca.jpg\"><br>Finally we can find, by examining vertices adjacent to the recently evaluated v2 and v4, that v5 and v7 have a shortest path of three edges.<br><img src=\"paste-2b6d43694ad888ce4151e2888746d9f2f9952e72.jpg\">"
            ],
            "guid": "A7{0-<r=}%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which strategy is used for finding the <b>unweighted shortest-path</b>?",
                "The strategy for searching a graph is known as <b>breadth-first search</b>. It operates by processing vertices in layers: The vertices closest to the start are evaluated first, and the most distant vertices are evaluated last. This is much the same as a level-order traversal for trees.&nbsp;"
            ],
            "guid": "kFv*/YI8_X",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe an implementation of solving the <b>unweightest shortest-path</b>&nbsp;problem.",
                "For each vertex, we will keep track of three pieces of information. First, we will keep its distance from s in the entry d<sub>v</sub>. Initially all vertices are unreachable except for s, whose path length is 0. The entry in p<sub>v</sub> is the bookkeeping variable, which will allow us to print the actual paths. The entry known is set to true after a vertex is processed. Initially, all entries are not known, including the start vertex. When a vertex is marked known, we have&nbsp;a guarantee that no cheaper path will ever be found, and so processing for that vertex is essentially complete.<br>The basic algorithm can be described in figure below. It mimics the diagrams by declaring as known the vertices at distance d = 0, then d = 1,<br>then d = 2, and so on, and setting all the adjacent vertices w that still have dw = ∞ to a distance dw = d + 1. By tracing back through the pv variable, the actual path can be printed.\n<br><img src=\"paste-b52bfdd9f0120194a607f158e4f12b9b7456ac10.jpg\">"
            ],
            "guid": "q6XiIDuR0B",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of a simple <b>unweighted shortest-path</b>&nbsp;solution?",
                "he running time of the algorithm is O(|V|<sup>2</sup>), because of the doubly nested for loops. An obvious inefficiency is that the outside loop continues until NUM_VERTICES-1, even if all the vertices become known much earlier. Although an extra test could be made to avoid this, it does not affect the worst-case running time, as can be seen by generalizing what happens when the input is the graph below with start vertex v<sub>9</sub>.<br><img src=\"paste-304edf77b40cd69e88519ef4305ec8031f846847.jpg\">"
            ],
            "guid": "n.&zTjVIv9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe a refined version of the <b>unweighted shortest-path</b>&nbsp;algorithm.",
                "A very simple but abstract solution is to keep two boxes. Box #1 will have the unknown vertices with d<sub>v</sub> = currDist, and box #2 will have d<sub>v</sub> = currDist + 1. The test to find an appropriate vertex v can be replaced by finding any vertex in box #1. After updating w (inside the innermost if block), we can add w to box #2. After the outermost for loop terminates, box #1 is empty, and box #2 can be transferred to box #1 for the next pass of the for loop.<br>We can refine this idea even further by using just one queue. At the start of the pass, the queue contains only vertices of distance currDist. When we add adjacent vertices of distance currDist + 1, since they enqueue at the rear, we are guaranteed that they will not be processed until after all the vertices of distance currDist have been processed. After the last vertex at distance currDist dequeues and is processed, the queue only contains vertices of distance currDist + 1, so this process perpetuates. We merely need to begin the process by placing the start node on the queue by itself.<br><img src=\"paste-f14bc5e16577f912a4939e6d87d4d1751328634b.jpg\"><br>In the pseudocode, we have assumed that the start vertex, s, is passed as a parameter. Also, it is possible that the queue might empty prematurely, if some vertices are unreachable from the start node. In this case, a distance of INFINITY will be reported for these nodes, which is perfectly reasonable. Finally, the known data member is not used; once a vertex is processed it can never enter the queue again, so the fact that it need not be reprocessed is implicitly marked. Thus, the known data member can be discarded."
            ],
            "guid": "nOaYe,q-;9",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for the refined implementation of <b>unweighted shortest-path</b>&nbsp;algorithm?",
                "Using the same analysis as was performed for topological sort, we see that the running time is O(|E| + |V|), as long as adjacency lists are used."
            ],
            "guid": "i-l4:]M3.q",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the general method to solve the single-source shortest-path problem known as?",
                "It is known as <b>Dijkstra's algorithm</b>. This thirty-year-old solution is a prime example of a greedy algorithm."
            ],
            "guid": "s/!`,+T:Qb",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is <b>Dijkstra's algorithm</b> considered to be a <b>greedy algorithm</b>?",
                "Greedy algorithms generally solve a problem in stages by doing what appears to be the best thing at each stage. For example, to make change in U.S. currency, most people count out the quarters first, then the dimes, nickels, and pennies. This greedy algorithm gives change using the minimum number of coins."
            ],
            "guid": "N@<=Bz>VKM",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the main problem of <b>greedy algorithms</b>?",
                "The main problem with greedy algorithms is that they do not always work. The addition of a 12-cent piece breaks the coin-changing algorithm for returning 15 cents, because the answer it gives (one 12-cent piece and three pennies) is not optimal (one dime and one nickel)."
            ],
            "guid": "Dg`h;,?76l",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Dijkstra's algorithm</b>.",
                "Dijkstra's algorithm proceeds in stages, just like the unweighted shortest-path algorithm. At each stage, Dijkstra's algorithm selects a vertex, v, which has the smallest d<sub>v&nbsp;</sub>among all the unknown vertices and declares that the shortest path from s to v is known. The remainder of a stage consists of updating the values of d<sub>w</sub>. In the unweighted case, we set d<sub>w</sub> = d<sub>v</sub> + 1 if d<sub>w</sub> = ∞. Thus, we essentially lowered the value of dw if vertex v offered a shorter path. If we apply the same logic to the weighted case, then we should set d<sub>w</sub> = d<sub>v</sub> + c<sub>v</sub>,<sub>w</sub> if this new value for dw would be an improvement.<br>Put simply, the algorithm decides whether or not it is a good idea to use v on the path to w. The original cost, d<sub>w</sub>, is the cost without using v; the cost calculated above is the cheapest path using v (and only known vertices).&nbsp;<br><img src=\"paste-21ea0184d8b75a195a2743d8e6aada2dc0271859.jpg\">"
            ],
            "guid": "ze5vEnS?9T",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the condition for <b>Dijkstra's algorithm</b>&nbsp;to work?",
                "A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If any edge has negative cost, the algorithm could produce the wrong answer."
            ],
            "guid": "L8pK%[4!q>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Dijkstra's</b>&nbsp;<b>algorithm</b>&nbsp;when sequential scanning is used?",
                "If we use the obvious algorithm of sequentially scanning the vertices to find the minimum d<sub>v</sub>, each phase will take O(|V|) time to find the minimum, and thus O(|V|<sup>2</sup>) time will be spent finding the minimum over the course of the algorithm. The time for updating d<sub>w</sub> is constant per update, and there is at most one update per edge for a total of O(|E|). Thus, the total running time is O(|E|+|V|<sup>2</sup>) = O(|V|<sup>2</sup>). If the graph is dense, with |E| = Θ(|V|<sup>2</sup>), this algorithm is not only simple but also essentially optimal, since it runs in time linear in the number of edges. If the graph is sparse, with |E| = Θ(|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue."
            ],
            "guid": "Bt2#;;8gQ%",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What does the selection of vertex&nbsp;<i>v </i>correspond to in <b>Dijkstra's algorithm</b>?",
                "It is a deleteMin operation, since once the unknown minimum vertex is found, it is no longer unknown and must be removed from future consideration."
            ],
            "guid": "h:6LAnL{3o",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What methods can be used to update <i>w </i>in <b>Dijkstra's algorithm</b>?",
                "One way treats the update as a decreaseKey operation. The time to find the minimum is then O(log |V|), as is the time to perform updates, which amount to decreaseKey operations. This gives a running time of O(|E| log |V| + |V| log |V|) = O(|E| log |V|), an improvement&nbsp;over the previous bound for sparse graphs. Since priority queues do not efficiently support the find operation, the location in the priority queue of each value of d<sub>i</sub> will need to be maintained and updated whenever di changes in the priority queue. If the priority queue is implemented by a binary heap, this will be messy. If a pairing heap is used, the code is not too bad.<br>An alternate method is to insert w and the new value d<sub>w</sub> into the priority queue every time w's distance changes. Thus, there may be more than one representative for each vertex in the priority queue. When the deleteMin operation removes the smallest vertex from the priority queue, it must be checked to make sure that it is not already known and, if&nbsp;it is, it is simply ignored and another deleteMin is performed. Although this method is superior from a software point of view, and is certainly much easier to code, the size of the priority queue could get to be as large as |E|. This does not affect the asymptotic time bounds, since |E| ≤ |V|<sup>2</sup> implies that log |E| ≤ 2 log |V|. Thus, we still get an O(|E| log |V|) algorithm. However, the space requirement does increase, and this could be important in some applications. Moreover, because this method requires |E| deleteMins instead of only |V|, it is likely to be slower in practice&nbsp;"
            ],
            "guid": "Hu|fsmM`Se",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why is important to use a priority queue based implementation of <b>Dijkstra's algorithm</b>&nbsp;with computer mail and mass transit commutes?",
                "For the typical problems, the graphs are typically very sparse because most vertices have only a couple of edges."
            ],
            "guid": "uesJ_-8*R3",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Dijkstra's </b>algorithm when Fibonacci heaps are used?",
                "When this is used, the running time is O(|E|+|V| log |V|). Fibonacci heaps have good theoretical time bounds but a fair amount of overhead, so it is not clear whether using Fibonacci heaps is actually better in practice than Dijkstra’s algorithm with binary heaps. To date, there are no meaningful average-case results for this problem."
            ],
            "guid": "d`.WUs:<(2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why doesn't <b>Dijkstra's algorithm </b>work with negative edge cost?",
                "The problem is that once a vertex, u, is declared known, it is possible that from some other unknown vertex, v, there is a path back to u that is very negative. In such a case, taking a path from s to v back to u is better than going from s to u without using v."
            ],
            "guid": "CaO8[7.&Lt",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why adding a constant to each neagtive edge does not solve the case of using <b>Dijkstra's algorithm </b>with negative edges?",
                "A tempting solution is to add a constant to each edge cost, thus removing negative edges, calculate a shortest path on the new graph, and then use that result on the original. The naive implementation of this strategy does not work because paths with many edges become more weighty than paths with few edges."
            ],
            "guid": "K|L?4:13h/",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the negative edges problem when calculating shortest-path?",
                "A combination of the weighted and unweighted algorithms will solve the problem, but at the cost of a drastic increase in running time. We forget about the concept of known vertices, since our algorithm needs to be able to change its mind. We begin by placing <i>s</i> on a queue. Then, at each stage, we dequeue a vertex <i>v</i>. We find all vertices w adjacent to v such that d<sub>w</sub> &gt; d<sub>v</sub> + c<sub>v,w</sub>. We update d<sub>w</sub> and p<sub>w</sub>, and place <i>w</i> on a queue if it is not&nbsp;already there. A bit can be set for each vertex to indicate presence in the queue. We repeat the process until the queue is empty.<br><img src=\"paste-9b37081e9fd399acab9728744b6c7f4689c5efb4.jpg\">"
            ],
            "guid": "MJax~8bH1N",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of the shortest-path calculation when there are negative edges?",
                "Although the algorithm works if there are no negative-cost cycles, it is no longer true that the code in the inner for loop is executed once per edge. Each vertex can dequeue at most |V| times, so the running time is O(|E| · |V|) if adjacency lists are used. This is quite an increase from Dijkstra’s algorithm, so it is fortunate that, in practice, edge costs are nonnegative. If negative-cost cycles are present, then the algorithm as written will loop indefinitely. By stopping the algorithm after any vertex has dequeued |V| + 1 times, we can guarantee termination."
            ],
            "guid": "RiNYjgR51e",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we improve Dijkstra's algorithm if the graph is known to be <b>acyclic</b>?",
                "We can improve Dijkstra's algorithm by changing the order in which vertices are declared known, otherwise known as the vertex selection&nbsp;rule. The new rule is to select vertices in topological order. The algorithm can be done in one pass, since the selections and updates can take place as the topological sort is being performed.<br>This selection rule works because when a vertex <i>v</i> is selected, its distance, d<sub>v</sub>, can no longer be lowered, since by the topological ordering rule it has no incoming edges emanating from unknown nodes. There is no need for a priority queue with this selection rule; the running time is&nbsp;O(|E| + |V|), since the selection takes constant time.&nbsp;"
            ],
            "guid": "Oo>9s?-{ux",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define&nbsp;<b>critical path analysis.</b>",
                "It is an important use of acyclic graphs. Each node represents an activity that must be performed, along with the time it takes to complete the activity. This graph is thus known as an <b>activity-node graph</b>. The edges represent precedence relationships: An edge (v, w) means that activity v must be completed before activity w may begin. Of course, this implies that the graph must be acyclic. We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers.<br><img src=\"paste-33c4a58f14918968a81a7d1a077a91c3746a3c23.jpg\"><br>This type of a graph could be (and frequently is) used to model construction projects. In this case, there are several important questions which would be of interest to answer. First, what is the earliest completion time for the project? We can see from the graph that 10 time units are required along the path A, C, F, H. Another important question is to determine which activities can be delayed, and by how long, without affecting the minimum completion time. For instance, delaying any of A, C, F, or H would push the completion&nbsp;time past 10 units. On the other hand, activity B is less critical and can be delayed up to two time units without affecting the final completion time."
            ],
            "guid": "Df82WMO*Ob",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What do we need to perform the calculations of&nbsp;<b>critical path analysis</b>?",
                "To perform these calculations, we convert the activity-node graph to an <b>event-node graph</b>. Each event corresponds to the completion of an activity and all its dependent activities. Events reachable from a node v in the event-node graph may not commence until after the event v is completed. This graph can be constructed automatically or by hand. Dummy edges and nodes may need to be inserted in the case where an activity depends on several others. This is necessary in order to avoid introducing false dependencies (or false lack of dependencies).<br><img src=\"paste-3769af4173894b3f6570ee5d6055100c042181d1.jpg\"><br><img src=\"paste-3b34a29d8f42307be988c1ae4f9d00b888ac8aec.jpg\">"
            ],
            "guid": "q^8.xINu+Z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we calculate the earliest completion time from an <b>event-node graph</b>?",
                "<img src=\"paste-931e71a5385ce9c9266c2573c9037f48e51822f7.jpg\"><br>To find the earliest completion time of the project, we merely need to find the length of the longest path from the first event to the last event. For general graphs, the longest-path problem generally does not make sense, because of the possibility of <b>positive-cost cycles</b>. These are the equivalent of negative-cost cycles in shortest-path problems. If positive-cost cycles are present, we could ask for the longest simple path, but no satisfactory solution is known for this problem. Since the event-node graph is acyclic, we need not worry about cycles. In this case, it is easy to adapt the shortest-path algorithm to compute the earliest&nbsp;completion time for all nodes in the graph. If EC<sub>i</sub> is the earliest completion time for node i, then the applicable rules are:<br><img src=\"paste-152e70520fb2a034923e9d517afcdb5983babe79.jpg\"><br><img src=\"paste-dad1f31867490f93a1aed8be5274a5a74e92b0ee.jpg\">"
            ],
            "guid": "p{)zvS`pt~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we calculate the latest completion time from an&nbsp;<b>event-node graph</b>?",
                "<img src=\"paste-25950947b1193801a715541288690e4ea087d281.jpg\"><br>We can compute the latest time, LC<sub>i</sub>, that each event can finish without affecting the final completion time. The formulas to do this are:<br><img src=\"paste-9f97deafddb34a4d4a7b617ff14735b8dc724dfb.jpg\"><br><img src=\"paste-599e081c9312f6717ea86e336f828e1e0a8b0658.jpg\">"
            ],
            "guid": "uk}&$7$fNJ",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time for calculating earliest- and latest-completion times?",
                "These values can be computed in linear time by maintaining, for each vertex, a list of all adjacent and preceding vertices. The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order.&nbsp;"
            ],
            "guid": "kBw#7JVzwz",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>slack time.</b>",
                "The slack time for each edge in the event-node graph represents the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion. It is easy to see that:<br>Slack<sub>(v,w)</sub> = LC<sub>w</sub> - EC<sub>v</sub> - c<sub>v,w<br></sub><img src=\"paste-4d93a61a959ebebddcacddf9c400e114160c7004.jpg\"><br>The figure shows the slack (as the third entry) for each activity in the event-node graph. For each node, the top number is the earliest completion time and the bottom entry is the latest completion time."
            ],
            "guid": "U4_Jg(QqI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>critical path.</b>",
                "Some activities have zero slack. These are critical activities, which must finish on schedule. There is at least one path consisting entirely of zero-slack edges; such a path is a critical path."
            ],
            "guid": "C/$,I3r5`w",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <b>world ladders</b>&nbsp;problem.",
                "In a word ladder each word is formed by changing one character in the ladder’s previous word. For instance, we can convert zero to five by a sequence of one-character substitutions as follows: zero hero here hire fire five.<br>This is an unweighted shortest problem in which each word is a vertex, and two vertices have edges (in both directions) between them if they can be converted to each other with a one-character substitution."
            ],
            "guid": "tBH[G(I{7U",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you solve the <b>word ladders</b> problem?",
                "We start with a routine that creates a map in which the keys are words, and the values are vectors containing the words that can result from a one-character transformation. As such, this map represents the graph, in adjacency list format, and we only need to write one routine to run the single-source unweighted shortest-path algorithm and a second routine to output the sequence of words, after the&nbsp;single-source shortest-path algorithm has completed.<br>The first routine is findChain, which takes the map representing the adjacency lists and the two words to be connected and returns a map in which the keys are words, and the corresponding value is the word prior to the key on the shortest ladder starting at first. In other words, if the starting word is zero, the value for key five is fire, the value for key fire is hire, the value for key hire is here, and so on. Clearly this provides enough information for the second routine, getChainFromPreviousMap, which can work its way backward.<br>findChain is a direct implementation of unweighted shortest-path algorithm, and for simplicity, it assumes that first is a key in adjacentWords.<br>getChainFromPrevMap uses the prev map and second, which presumably is a key in the map and returns the words used to form the word ladder by working its way backward through prev. This generates the words backward, so the reverse algorithm is used to fix the problem."
            ],
            "guid": "d&+}J;-$GI",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we generalize a <b>world ladders</b>&nbsp;solution to include deletion or addition of a character?",
                "To compute the adjacency&nbsp;list requires only a little more effort: every time a representative for word <i>w</i> in group <i>g</i> is computed, we check if the representative is a word in group <i>g</i> - 1. If it is, then the representative is adjacent to <i>w</i> (it is a single-character deletion), and <i>w</i> is adjacent to the representative (it is a single-character addition). It is also possible to assign a cost to a character deletion or insertion (that is higher than a simple substitution), and this yields a weighted shortest-path problem that can be solved with Dijkstra’s algorithm."
            ],
            "guid": "tgiVen(,o2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>network flow problems</b>?",
                "Suppose we are given a directed graph G = (V, E) with edge capacities c<sub>v,w</sub>. These capacities could represent the amount of water that could flow through a pipe or the amount of traffic that could flow on a street between two intersections. We have two vertices: s, which we call the source, and t, which is the sink. Through any edge, (v, w), at most c<sub>v,w</sub> units of “flow” may pass. At any vertex, v, that is not either s or t, the total flow coming in must equal the total flow going out. The maximum-flow problem is to determine the maximum amount of flow that can pass from s to t. As an example, for the graph in figure on the left the maximum flow is 5, as indicated by the graph on the&nbsp;right. Although this example graph is acyclic, this is not a requirement; our (eventual) algorithm will work even if the graph has a cycle.<br><img src=\"paste-b787a56dae16ba09df0eb0a52952d2fbc1df94d3.jpg\">"
            ],
            "guid": "iUGhwU}P*S",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we prove the maximum flow in a <b>network flow problem</b>?",
                "Looking at the graph, we see that s has edges of capacities 4 and 2 leaving it, and t has edges of capacities 3 and 3 entering it. So perhaps the maximum flow could be 6 instead of 5. However, the figure shows how we can prove that the maximum flow is 5. We cut the graph into two parts; one part contains s and some other vertices; the other part contains t. Since flow must cross through the cut, the total capacity of all edges (u, v) where u is in s’s partition and v is in t’s partition is a bound on the maximum flow. These edges are (a, c) and (d, t), with total capacity 5, so the maximum flow cannot exceed 5. Any graph has a large number of cuts; the cut with minimum total capacity provides a bound on the maximum flow, and as it turns out (but it is not immediately obvious), the minimum cut capacity is exactly equal to the maximum flow.<br><img src=\"paste-15c1861be7d9e204ebda7c33baf4c216925e9126.jpg\">"
            ],
            "guid": "Eugi!4`L$=",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we generate the initial stages of the graphs in a&nbsp;<b>simple maximum-flow algorithm</b>?",
                "A first attempt to solve the problem proceeds in stages. We start with our graph, G, and construct a <b>flow graph</b> G<sub>f</sub>. G<sub>f</sub> tells the flow that has been attained at any stage in the algorithm. Initially all edges in G<sub>f</sub> have no flow, and we hope that when the algorithm terminates, G<sub>f</sub> contains a maximum flow. We also construct a graph, G<sub>r</sub>, called the <b>residual graph</b>. G<sub>r</sub> tells, for each edge, how much more flow can be added. We can calculate this by subtracting the current flow from the capacity for each edge. An edge in G<sub>r</sub> is known as a residual edge.<br>At each stage, we find a path in G<sub>r</sub> from s to t. This path is known as an <b>augmenting path</b>. The minimum edge on this path is the amount of flow that can be added to every edge on the path. We do this by adjusting G<sub>f</sub> and recomputing G<sub>r</sub>. When we find no path from s to t in G<sub>r</sub>, we terminate.&nbsp;<br><img src=\"paste-bec8bb8f3d76d5e94512ff2af9266de35b9bf0f1.jpg\">"
            ],
            "guid": "O(`ho0&[L-",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Demonstrate why a greedy algorithm for solving <b>network-flow problems</b>&nbsp;does not work.",
                "There are many paths from s to t in the residual graph. Suppose we select s, b, d, t. Then we can send two units of flow through every edge on this path. We will adopt the convention that once we have filled (saturated) an edge, it is removed from the residual graph.<br><img src=\"paste-ecb98386498b5511cdf857a7bab1e676112fbd5c.jpg\"><br>Next, we might select the path s, a, c, t, which also allows two units of flow. Making the required adjustments gives the graphs in figure.<br><img src=\"paste-d020223182cf852dfaf0421f11d742d1a6ace915.jpg\"><br>The only path left to select is s, a, d, t, which allows one unit of flow. The resulting graphs are shown in figure. The algorithm terminates at this point, because t is unreachable from s. The resulting flow of 5 happens to be the maximum. To see what the problem is, suppose that with our initial graph, we chose the path s, a, d, t. This path allows three units of flow and thus seems to be a good choice. The result of this choice, however, leaves only one path from s to t in the residual graph; it allows one more unit of flow, and thus, our algorithm has&nbsp;failed to find an optimal solution. This is an example of a greedy algorithm that does not work.<br><img src=\"paste-73c77fb390f3082d2b4a28bd834ef1cc47b924db.jpg\">"
            ],
            "guid": "sY$1f3||Cx",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What addition do we need to make to the greedy algorithm to make it work with <b>network flow problems</b>?",
                "In order to make this algorithm work, we need to allow the algorithm to change its mind. To do this, for every edge (v, w) with flow f<sub>v,w</sub> in the flow graph, we will add an edge in the residual graph (w, v) of capacity f<sub>v,w</sub>. In effect, we are allowing the algorithm to undo its decisions by sending flow back in the opposite direction. This is best seen by example. Starting from our original graph and selecting the augmenting path s, a, d, t, we obtain the graphs in figure.<br><img src=\"paste-eddb3c7a16cb90abc3bca2d3c86eb42a17a95330.jpg\"><br>Notice that in the residual graph, there are edges in both directions between a and d. Either one more unit of flow can be pushed from a to d, or up to three units can be pushed back—we can undo flow. Now the algorithm finds the augmenting path s, b, d, a, c, t, of flow 2. By pushing two units of flow from d to a, the algorithm takes two units of flow away from the edge (a, d) and is essentially changing its mind.<br><img src=\"paste-7780261789fd30bc60d1aa75bd7935bbe972ac36.jpg\"><br>There is no augmenting path in this graph, so the algorithm terminates. Note that the same result would occur if the augmenting path s, a, c, t was chosen which allows one unit of flow, because then a subsequent augmenting path could be found."
            ],
            "guid": "E&1q:#a!}[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How can we prove that the correct implementation of the <b>network flow problem</b>'s algorithm terminates?",
                "It is easy to see that if the algorithm terminates, then it must terminate with a maximum flow. Termination implies that there is no path from s to t in the residual graph. So cut the residual graph, putting the vertices reachable from s on one side and the unreachables (which include t) on the other side. Clearly any edges in the original graph G that cross the cut must be saturated; otherwise, there would be residual flow remaining on one of the edges, which would then imply an edge that crosses the cut (in the wrong disallowed direction) in Gr. But that means that the flow in G is exactly equal to the capacity of a cut in G; hence, we have a maximum flow.<br><img src=\"paste-bf409e64da466adc427ef0c6bbc48625f0fea0bd.jpg\"><br>If the edge costs in the graph are integers, then the algorithm must terminate; each augmentation adds a unit of flow, so we eventually reach the maximum flow, though there&nbsp;is no guarantee that this will be efficient."
            ],
            "guid": "H?T5V0QZLg",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why the solution of a <b>network flow problem</b> can be inefficient? And what solutions do we have for that?",
                "If the edge costs in the graph are integers, then the algorithm&nbsp;must&nbsp;terminate; each&nbsp;augmentation adds a unit of flow, so we eventually reach the maximum flow, though there&nbsp;is no guarantee that this will be efficient. In particular, if the capacities are all integers and&nbsp;the maximum flow is&nbsp;f, then, since each augmenting path increases the flow value by at&nbsp;least 1,&nbsp;f&nbsp;stages suffice, and the total running time is&nbsp;O(f&nbsp;·|E|), since an augmenting path can&nbsp;be found in&nbsp;O(|E|) time by an unweighted shortest-path algorithm.<br><img src=\"paste-d8032a3f82abcab07b4450da7cc0aa5bb34bb74a.jpg\"><br>The maximum flow is seen by inspection to be 2,000,000 by sending 1,000,000 down each side. Random augmentations could continually augment along a path that includes the edge connected by a and b. If this were to occur repeatedly, 2,000,000 augmentations would be required, when we could get by with only 2.<br>A simple method to get around this problem is always to choose the augmenting path that allows the largest increase in flow. Finding such a path is similar to solving a weighted shortest-path problem, and a single-line modification to Dijkstra's algorithm will do the trick. If capmax is the maximum edge capacity, then one can show that O(|E| log capmax) augmentations will suffice to find the maximum flow. In this case, since O(|E| log |V|) time is used for each calculation of an augmenting path, a total bound of O(|E|<sup>2</sup> log |V| log capmax) is obtained. If the capacities are all small integers, this reduces to O(|E|<sup>2</sup> log |V|).<br>Another way to choose augmenting paths is always to take the path with the least number of edges, with the plausible expectation that by choosing a path in this manner, it is less likely that a small, flow-restricting edge will turn up on the path. With this rule, each augmenting step computes the shortest unweighted path from s to t in the residual graph, so assume that each vertex in the graph maintains d<sub>v</sub>, representing the shortest-path<br>distance from s to v in the residual graph. Each augmenting step can add new edges into the residual graph, but it is clear that no d<sub>v</sub> can decrease, because an edge is added in the opposite direction of an existing shortest path.<br>Each augmenting step saturates at least one edge. Suppose edge (u, v) is saturated; at that point, u had distance du and v had distance d<sub>v</sub> = d<sub>u</sub> + 1; then (u, v) was removed from&nbsp;the residual graph, and edge (v, u) was added. (u, v) cannot reappear in the residual graph again, unless and until (v, u) appears in a future augmenting path. But if it does, then the distance to u at that point must be d<sub>v</sub> + 1, which would be 2 higher than at the time (u, v) was previously removed.<br>This means that each time (u, v) reappears, u's distance goes up by 2. This means that any edge can reappear at most |V|/2 times. Each augmentation causes some edge to reappear so the number of augmentations is O(|E||V|). Each step takes O(|E|), due to the unweighted shortest-path calculation, yielding an O(|E|2|V|) bound on the running time."
            ],
            "guid": "y3X1~PZ@m;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are some special solutions related to the <b>network flow problems</b>?",
                "Further data structure improvements are possible to this algorithm, and there are several, more complicated, algorithms. A long history of improved bounds has lowered the current best-known bound for this problem to O(|E||V|). There are also a host of very good bounds for special cases. For instance, O(|E||V|<sup>1/2</sup>) time finds a maximum flow in a graph, having the property that all vertices except the source and sink have either a single incoming edge of capacity 1 or a single outgoing edge of capacity 1. These graphs occur in many applications.<br>The analyses required to produce these bounds are rather intricate, and it is not clear how the worst-case results relate to the running times encountered in practice. A related, even more difficult problem is the <b>min-cost flow</b> problem. Each edge has not only a capacity but also a cost per unit of flow. The problem is to find, among all maximum flows, the one flow of minimum cost. Both of these problems are being actively researched.&nbsp;"
            ],
            "guid": "Q8rnqx]d0b",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define a <b>minimum spanning tree</b> of an undirected graph.",
                "Informally, a minimum spanning tree of an undirected graph G is a tree formed from graph edges that connects all the vertices of G at lowest total cost. A minimum spanning tree exists if and only if G is connected.<br><img src=\"paste-fcfc8f72cc2daf428023c2718a96fa9e8101440a.jpg\">"
            ],
            "guid": "yp({S`Hd{D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the properties of a <b>minimum spanning tree</b>?",
                "The number of edges in the minimum spanning tree is |V| - 1. The minimum spanning tree is a tree because it is acyclic, it is spanning because it covers every vertex, and it is minimum for the obvious reason. If we need to wire a house with a minimum of cable (assuming no other electrical constraints), then a minimum spanning tree problem needs to be solved.<br>For any spanning tree, T, if an edge, e, that is not in T is added, a cycle is created. The removal of any edge on the cycle reinstates the spanning tree property. The cost of the spanning tree is lowered if e has lower cost than the edge that was removed. If, as a spanning tree is created, the edge that is added is the one of minimum cost that avoids creation of a cycle, then the cost of the resulting spanning tree cannot be improved, because any replacement edge would have cost at least as much as an edge already in the spanning tree. This shows that greed works for the minimum spanning tree problem.&nbsp;"
            ],
            "guid": "q.-X!sG+yn",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Prim's algorithm</b>.",
                "One way to compute a minimum spanning tree is to grow the tree in successive stages. In each stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the tree.<br>At any point in the algorithm, we can see that we have a set of vertices that have already been included in the tree; the rest of the vertices have not. The algorithm then finds, at each stage, a new vertex to add to the tree by choosing the edge (u, v) such that the cost of (u, v) is the smallest among all edges where u is in the tree and v is not.<br><img src=\"paste-e5b9d5be1e4aa03baaaab156b48896cb6d36aaa3.jpg\"><br><img src=\"paste-239d9916b6aba59b35f1f5bb4dcdd8afdd6ca6df.jpg\">"
            ],
            "guid": "om2SD^mQ#f",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between <b>Prim's algorithm </b>and Daijkstra's?",
                "We can see that Prim's algorithm is essentially identical to Dijkstra's algorithm for shortest paths. As before, for each vertex we keep values d<sub>v</sub> and pv and an indication of whether it is known or unknown. d<sub>v</sub> is the weight of the shortest edge connecting v to a known vertex,&nbsp;and p<sub>v</sub>, as before, is the last vertex to cause a change in d<sub>v</sub>. The rest of the algorithm is exactly the same, with the exception that since the definition of d<sub>v</sub> is different, so is the update rule. For this problem, the update rule is even simpler than before: After a vertex, v, is selected, for each unknown w adjacent to v, d<sub>w</sub> = min(d<sub>w</sub>, c<sub>w,v</sub>)."
            ],
            "guid": "m[)14Xtmi|",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What's the running time of&nbsp;<b>Prim's algorithm?</b>",
                "The running time is O(|V|<sup>2</sup>) without heaps, which is optimal for dense graphs, and O(|E| log |V|) using binary heaps, which is good for<br>sparse graphs.&nbsp;"
            ],
            "guid": "K8?O%;3KuR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>Kruskal's algorithm</b>.",
                "It's a minimum spanning tree algorithm that uses greedy strategy is to continually select the edges in order of smallest weight and accept an edge if it does not cause a cycle.<br><img src=\"paste-16fe74b80b60d6ad24d282cdccfcc6f34223e0cf.jpg\"><br>Formally, Kruskal’s algorithm maintains a forest—a collection of trees. Initially, there are |V| single-node trees. Adding an edge merges two trees into one. When the algorithm terminates, there is only one tree, and this is the minimum spanning tree.<br>The algorithm terminates when enough edges are accepted. It turns out to be simple to decide whether edge (u, v) should be accepted or rejected. The appropriate data structure is the union/find algorithm. The invariant we will use is that at any point in the process, two vertices belong to the same set if and only if they are connected in the current spanning forest. Thus, each vertex is initially in its own set. If u and v are in the same set, the edge is rejected, because since they are already connected, adding (u, v) would form a cycle. Otherwise, the edge is accepted, and a union is performed on the two sets containing u and v. It is easy to see that this maintains the set invariant, because once the edge (u, v) is added to the spanning forest, if w was connected to u and x was connected to v, then x and w must now be connected, and thus belong in the same set.<br>The edges could be sorted to facilitate the selection, but building a heap in linear time is a much better idea. Then deleteMins give the edges to be tested in order. Typically, only a small fraction of the edges need to be tested before the algorithm can terminate, although it is always possible that all the edges must be tried. For instance, if there was an extra vertex v<sub>8</sub> and edge (v<sub>5</sub>, v<sub>8</sub>) of cost 100, all the edges would have to be examined.<br><img src=\"paste-a2a51799c5613b8b570d6ecbad2c8cb1eab3fe29.jpg\">"
            ],
            "guid": "M03}#|=J#2",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the running time of <b>Kruskal's algorithm</b>?",
                "The worst-case running time of this algorithm is O(|E| log |E|), which is dominated by the heap operations. Notice that since |E| = O(|V|<sup>2</sup>), this running time is&nbsp;actually O(|E| log |V|). In practice, the algorithm is much faster than this time bound would indicate."
            ],
            "guid": "I%^GFD??pD",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the relationship between <b>depth-first search </b>and <b>preorder traversal</b>?",
                "Depth-first search is a generalization of preorder traversal. Starting at some vertex, v, we process v and then recursively traverse all vertices adjacent to v. If this process is performed on a tree, then all tree vertices are systematically visited in a total of O(|E|) time, since |E| = Θ(|V|). If we perform this process on an arbitrary graph, we need to be careful to avoid cycles. To do this, when we visit a vertex, v, we mark it visited, since now we have been there, and recursively call depth-first search on all adjacent vertices that are not already marked. We implicitly assume that for undirected graphs every edge (v, w) appears twice in the adjacency lists: once as (v, w) and once as (w, v).<br><img src=\"paste-7d2051b8ac2b10122dc64d304e395c3a17ed7dd9.jpg\"><br>For each vertex, the data member visited is initialized to false. By recursively calling the procedures only on nodes that have not been visited, we guarantee that we do not loop indefinitely. If the graph is undirected and not connected, or directed and not strongly connected, this strategy might fail to visit some nodes. We then search for an unmarked node,&nbsp;apply a depth-first traversal there, and continue this process until there are no unmarked nodes.&nbsp;Because this strategy guarantees that each edge is encountered only once, the total time to perform the traversal is O(|E| + |V|), as long as adjacency lists are used.<br>An efficient way of implementing this is to begin the depth-first search at v<sub>1</sub>. If we need to restart the depth-first search, we examine the sequence v<sub>k</sub>, v<sub>k+1</sub>, . . . for an unmarked vertex, where v<sub>k-1</sub> is the vertex where the last depth-first search was started. This guarantees that throughout the algorithm, only O(|V|) is spent looking for vertices where new depth-first search trees can be started."
            ],
            "guid": "BD}|U5?GSS",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>depth-first spanning forest</b>.",
                "We graphically illustrate these steps with a depth-first spanning tree. The root of the tree is A, the first vertex visited. Each edge (v, w) in the graph is present in the tree. If, when we process (v, w), we find that w is unmarked, or if, when we process (w, v), we find that v is unmarked, we indicate this with a tree edge. If, when we process (v, w), we find that w is already marked, and when processing (w, v), we find that v is already marked, we draw a dashed line, which we will call a back edge, to indicate that this “edge” is not really part of the tree. The depth-first search of the graph:<br><img src=\"paste-2f7652a98b89ce1f1e8b4f09806ef411108d5590.jpg\">is shown in figure:<br><img src=\"paste-bb7204bb9d0e4afb17de88c08de0e0d33468ee01.jpg\"><br>The tree will simulate the traversal we performed. A preorder numbering of the tree, using only tree edges, tells us the order in which the vertices were marked. If the graph is not connected, then processing all nodes (and edges) requires several calls to dfs, and each generates a tree. This entire collection is a <b>depth-first spanning forest</b>."
            ],
            "guid": "po>cM<;S=D",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>biconnected</b>&nbsp;graphs.",
                "A connected undirected graph is biconnected if there are no vertices whose removal disconnects the rest of the graph.&nbsp;If the nodes&nbsp;are computers and the edges are links, then if any computer goes down, network mail is&nbsp;unaffected, except, of course, at the down computer. Similarly, if a mass transit system is biconnected, users always have an alternate route should some terminal be disrupted."
            ],
            "guid": "qZIxcu6]t>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are <b>articulation points</b>?",
                "If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as articulation points. These nodes are critical in many applications. The graph in figure is not biconnected: C and D are articulation points. The removal of C would disconnect G, and the removal of D would disconnect E and F, from the rest of the graph.<br><img src=\"paste-4df8a6fac3ee8ff605b2587946a6a1fc2eacc76d.jpg\">"
            ],
            "guid": "KvR/fP.4[[",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we find <b>articulation points</b>?",
                "We will assume that Vertex contains the data members visited (initialized to false), num, low, and parent. We will also keep a (Graph) class variable called counter, which is initialized to 1, to assign the preorder traversal numbers, num. We also leave out the easily implemented test for the root.<br>As we have already stated, this algorithm can be implemented by performing a preorder traversal to compute Num and then a postorder traversal to compute Low. A third traversal can be used to check which vertices satisfy the articulation point criteria. Performing three traversals, however, would be a waste. The first pass is shown here:<br><img src=\"paste-1a59eee151ca60f3537025c1eec3f001ce7b1b16.jpg\"><br>The second and third passes, which are postorder traversals, can be implemented by the code:<br><img src=\"paste-ad2b64415d5ed9a3da90cacf77ed1e7d8baf2c42.jpg\"><br>The last if statement handles a special case. If w is adjacent to&nbsp;v, then the recursive call to w will find v adjacent to w. This is not a back edge, only an edge that has already been considered and needs to be ignored. Otherwise, the procedure computes the minimum of the various low and num entries, as specified by the algorithm. There is no rule that a traversal must be either preorder or postorder. It is possible to do processing both before and after the recursive calls. The procedure below combines the two routines assignNum and assignLow in a straightforward manner to produce the procedure findArt.&nbsp;<br><img src=\"paste-d806069343b9645943b424d12f6de09a5780e059.jpg\">"
            ],
            "guid": "sN?qeu!_lR",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you use depth-first search to find <b>articulation points</b>?",
                "Depth-first search provides a linear-time algorithm to find all articulation points in a connected graph. First, starting at any vertex, we perform a depth-first search and number the nodes as they are visited. For each vertex, v, we call this preorder number Num(v). Then, for every vertex, v, in the depth-first search spanning tree, we compute the lowestnumbered vertex, which we call Low(v), that is reachable from v by taking zero or more&nbsp;tree edges and then possibly one back edge (in that order). The depth-first search tree in the figure shows the preorder number first, and then the lowest-numbered vertex reachable under the rule described above.<br><img src=\"paste-d1fed0acf4019f8220b18c977e226cb8bc0da754.jpg\"><br>The lowest-numbered vertex reachable by A, B, and C is vertex 1 (A), because they can all take tree edges to D and then one back edge back to A. We can efficiently compute Low by performing a postorder traversal of the depth-first spanning tree. By the definition of Low, Low(v) is the minimum of<br>1. Num(v)<br>2. the lowest Num(w) among all back edges (v, w)<br>3. the lowest Low(w) among all tree edges (v, w)<br><br>The first condition is the option of taking no edges, the second way is to choose no tree edges and a back edge, and the third way is to choose some tree edges and possibly a&nbsp;back edge. This third method is succinctly described with a recursive call. Since we need to evaluate Low for all the children of v before we can evaluate Low(v), this is a postorder traversal. For any edge (v, w), we can tell whether it is a tree edge or a back edge merely by checking Num(v) and Num(w). Thus, it is easy to compute Low(v): We merely scan down v's adjacency list, apply the proper rule, and keep track of the minimum. Doing all the computation takes O(|E| + |V|) time.<br>All that is left to do is to use this information to find articulation points. The root is an articulation point if and only if it has more than one child, because if it has two children, removing the root disconnects nodes in different subtrees, and if it has only one child, removing the root merely disconnects the root. Any other vertex v is an articulation point if and only if v has some child w such that Low(w) ≥ Num(v). Notice that this condition is always satisfied at the root, hence the need for a special test. The if part of the proof is clear when we examine the articulation points that the algorithm determines, namely, C and D. D has a child E, and Low(E) ≥ Num(D), since both are 4. Thus, there is only one way for E to get to any node above D, and that is by going through D. Similarly, C is an articulation point, because Low(G) ≥ Num(C). To prove that this algorithm is correct, one must show that the only if part of the assertion is true (that is, this finds all articulation points). We leave this as an exercise. As a second example, the figure below shows the result of applying this algorithm on the same graph, starting the depth-first search at C.<br><img src=\"paste-317cc996cd21a24e3569047e5b49a1b0e711b2fa.jpg\">"
            ],
            "guid": "B`JB3RXlb8",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give an example of <b>Euler circuit problem</b>.",
                "<img src=\"paste-181f50b65b3affa7adc52bb6aff32958c3dd24a2.jpg\"><br>A popular puzzle is to reconstruct these figures using a pen, drawing each line exactly once. The pen may not be lifted from the paper while the drawing is being performed. As an extra challenge, make the pen finish at the same point at which it started.<br>The first figure can be drawn only if the starting point is the lower left- or right-hand corner, and it is not possible to finish at the starting point. The second figure is easily drawn with the finishing point the same as the starting point, but the third figure cannot be drawn at all within the parameters of the puzzle."
            ],
            "guid": "D@daN$:auA",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the generic solution to <b>Euler circuit problem</b>.",
                "<img src=\"paste-9e186cf04192205626e56fee41b87802d390944e.jpg\"><br>We can convert this problem to a graph theory problem by assigning a vertex to each intersection. Then the edges can be assigned in the natural manner, as in the figure:<br><img src=\"paste-72fcfd247d80544e3dfa7a5f5eaecd86313786cd.jpg\"><br>After this conversion is performed, we must find a path in the graph that visits every edge exactly once. If we are to solve the “extra challenge,” then we must find a cycle that visits every edge exactly once. This graph problem was solved in 1736 by Euler and marked the beginning of graph theory. The problem is thus commonly referred to as an Euler path (sometimes Euler tour) or Euler circuit problem, depending on the specific problem statement. The Euler tour and Euler circuit problems, though slightly different, have the same basic solution."
            ],
            "guid": "QoXT@SE-R;",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the conditions of an Euler circuit that must end on its starting vertex?",
                "The first observation that can be made is that an Euler circuit, which must end on its starting vertex, is possible only if the graph is connected and each vertex has an even degree (number of edges). This is because, on the Euler circuit, a vertex is entered and then left. If any vertex v has odd degree, then eventually we will reach the point where only one edge into v is unvisited, and taking it will strand us at v. If exactly two vertices have odd degree, an Euler tour, which must visit every edge but need not return to its starting vertex, is still possible if we start at one of the odd-degree vertices and finish at the other. If more than two vertices have odd degree, then an Euler tour is not possible.<br>The observations of the preceding paragraph provide us with a necessary condition for the existence of an Euler circuit. It does not, however, tell us that all connected graphs that satisfy this property must have an Euler circuit, nor does it give us guidance on how to find one. It turns out that the necessary condition is also sufficient. That is, any connected graph, all of whose vertices have even degree, must have an Euler circuit. Furthermore, a circuit can be found in linear time."
            ],
            "guid": "reRT7~*O_m",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What solutions work and what doesn't for <b>Euler's circuit</b>?",
                "We can assume that we know that an Euler circuit exists, since we can test the necessary and sufficient condition in linear time. Then the basic algorithm is to perform a depth-first search. There are a surprisingly large number of “obvious” solutions that do not work. Some of these are presented in the exercises.<br>The main problem is that we might visit a portion of the graph and return to the starting point prematurely. If all the edges coming out of the start vertex have been used up, then part of the graph is untraversed. The easiest way to fix this is to find the first vertex on this path that has an untraversed edge and perform another depth-first search. This will give another circuit, which can be spliced into the original. This is continued until all edges have been traversed."
            ],
            "guid": "=lzZs5LA)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What data-structures are used in <b>Euler's circuits</b>?",
                "To make this algorithm efficient, we must use appropriate data structures. To make splicing simple, the path should be maintained as a linked list. To avoid repetitious scanning of adjacency lists, we must maintain, for each adjacency list, a pointer to the last edge scanned. When a path is spliced in, the search for a new vertex from which to perform the next depth-first search must begin at the start of the splice point. This guarantees that&nbsp;he total work performed on the vertex search phase is O(|E|) during the entire life of the algorithm. With the appropriate data structures, the running time of the algorithm is O(|E| + |V|)."
            ],
            "guid": "d5(Ox@64gw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Which algorithm similar to <b>Euler circuit problem</b>&nbsp;using undirected graphs?",
                "A very similar problem is to find a simple cycle, in an undirected graph, that visits every vertex. This is known as the <b>Hamiltonian cycle problem</b>. Although it seems almost identical to the Euler circuit problem, no efficient algorithm for it is known."
            ],
            "guid": "GYx`.ggah]",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What strategy can be used to traverse <b>directed graphs</b>?<br><img src=\"paste-451428472900b303aadce2d023f2e69744d73d1a.jpg\">",
                "Using the same strategy as with undirected graphs, directed graphs can be traversed in linear time, using depth-first search. If the graph is not strongly connected, a depth-first search starting at some node might not visit all nodes. In this case, we repeatedly perform depth-first searches, starting at some unmarked node, until all vertices have been visited.<br>We arbitrarily start the depth-first search at vertex B. This visits vertices B, C, A, D, E, and F. We then restart at some unvisited vertex. Arbitrarily, we start at H, which visits J and I. Finally, we start at G, which is the last vertex that needs to be visited.<br><img src=\"paste-be4e723174991ade35931612a711a714d14ca119.jpg\">"
            ],
            "guid": "bd+3&Zj%zr",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>depth-first spanning tree</b>.",
                "<img src=\"paste-82a2d8672aa3c143217af2b898eeacbcfe7905ee.jpg\"><br>The dashed arrows in the depth-first spanning forest are edges (v, w) for which w was already marked at the time of consideration. In undirected graphs, these are always back edges, but, as we can see, there are three types of edges that do not lead to new vertices. First, there are <b>back edges</b>, such as (A, B) and (I, H). There are also <b>forward edges</b>, such as (C, D) and (C, E), that lead from a tree node to a descendant. Finally, there are <b>cross edges</b>, such as (F, C) and (G, F), which connect two tree nodes that are not directly related. Depthfirst search forests are generally drawn with children and new trees added to the forest from left to right. In a depth-first search of a directed graph drawn in this manner, cross edges always go from right to left. Some algorithms that use depth-first search need to distinguish between the three types of nontree edges. This is easy to check as the depth-first search is being performed, and it is left as an exercise."
            ],
            "guid": "yxw*i(LB?z",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What are the uses for <b>depth-first spanning trees</b>?",
                "One use of depth-first search is to test whether or not a directed graph is acyclic. The rule is that a directed graph is acyclic if and only if it has no back edges. (The graph above has back edges, and thus is not acyclic.) A topological sort can also be used to determine whether a graph is acyclic. Another way to perform topological sorting is to assign the vertices topological numbers N, N - 1, . . . , 1 by postorder traversal of the depth-first spanning forest. As long as the graph is acyclic, this ordering will be consistent."
            ],
            "guid": "zK@Wn&a:#p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you test whether a directed graph is <b>strongly connected</b>?<br><img src=\"paste-3425bbeea1747147eafd63ef514039faf2ba9f19.jpg\">",
                "First, a depth-first search is performed on the input graph G. The vertices of G are numbered by a postorder traversal of the depth-first spanning forest, and then all edges in G are reversed, forming Gr.<br><img src=\"paste-83e52836349ced9e73bcc3a2886148228c1ee7cd.jpg\"><br>The algorithm is completed by performing a depth-first search on Gr, always starting a new depth-first search at the highest-numbered vertex. Thus, we begin the depth-first search of Gr at vertex G, which is numbered 10. This leads nowhere, so the next search is started at H. This call visits I and J. The next call starts at B and visits A, C, and F. The next calls after this are dfs(D) and finally dfs(E). The resulting depth-first spanning forest is shown in figure:<br>Each of the trees (this is easier to see if you completely ignore all nontree edges) in this depth-first spanning forest forms a strongly connected component. Thus, for our example, the strongly connected components are {G}, {H, I, J}, {B, A, C, F}, {D}, and {E}.<br><img src=\"paste-6618833ff5f21027e6a65c66038288137dad1e7e.jpg\">"
            ],
            "guid": "n#,5g*l%&N",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the difference between the running times of graph theory problems?",
                "All these problems have polynomial running times, and with the exception of the network flow problem, the running time is either linear or only slightly more than linear (O(|E| log |E|)). The Euler circuit problem, which finds a path that touches every edge exactly once, is solvable in linear time. The Hamiltonian cycle problem asks for a simple cycle that contains every vertex. No linear algorithm is known for this problem. The single-source unweighted shortest-path problem for directed graphs is also solvable in linear time. No linear-time algorithm is known for the corresponding longestsimple-path problem. The situation for these problem variations is actually much worse than we have described. Not only are no linear algorithms known for these variations, but there are no known algorithms that are guaranteed to run in polynomial time. The best known&nbsp;algorithms for these problems could take exponential time on some inputs.&nbsp;"
            ],
            "guid": "j%G+rW@>]M",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the first step in classifying problems?",
                "When classifying problems, the first step is to examine the boundaries. We have alreadyseen that many problems can be solved in linear time. We have also seen some O(log N) running times, but these either assume some preprocessing (such as input already being read or a data structure already being built) or occur on arithmetic examples. For instance, the gcd algorithm, when applied on two numbers M and N, takes O(log N) time. Since the numbers consist of log M and log N bits, respectively, the gcd algorithm is really taking time that is linear in the amount or size of input. Thus, when we measure running time, we will be concerned with the running time as a function of the amount of input. Generally, we cannot expect better than linear running time.<br>At the other end of the spectrum lie some truly hard problems. These problems are so hard that they are impossible. This does not mean the typical exasperated moan, which means that it would take a genius to solve the problem. Just as real numbers are not sufficient to express a solution to x<sup>2</sup> &lt; 0, one can prove that computers cannot solve every problem that happens to come along. These “impossible” problems are called undecidable problems.&nbsp;"
            ],
            "guid": "ysyQ_2~mp,",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>halting problem</b>.",
                "A particular undecidable problem is the halting problem. Is it possible to have your C++ compiler have an extra feature that not only detects syntax errors but also all infinite loops? This seems like a hard problem, but one might expect that if some very clever programmers spent enough time on it, they could produce this enhancement. The intuitive reason that this problem is undecidable is that such a program might have a hard time checking itself. For this reason, these problems are sometimes called <b>recursively undecidable</b>."
            ],
            "guid": "ruE#c09&J~",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define NP class of problems.",
                "NP stands for <b>nondeterministic polynomial-time</b>. A deterministic machine, at each point in time, is executing an instruction. Depending on the instruction, it then goes to some next instruction, which is unique. A nondeterministic machine has a choice of next steps.<br>A simple way to check if a problem is in NP is to phrase the problem as a yes/no question. The problem is in NP if, in polynomial time, we can prove that any “yes” instance is correct. We do not have to worry about “no” instances, since the program always makes the right choice. Thus, for the Hamiltonian cycle problem, a “yes” instance would be any simple circuit in the graph that includes all the vertices. This is in NP, since, given the path, it is a simple matter to check that it is really a Hamiltonian cycle. Appropriately phrased questions, such as “Is there a simple path of length &gt; K?” can also easily be checked and are in NP. Any path that satisfies this property can be checked trivially."
            ],
            "guid": "L&0*lh)3Fw",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Are all decidable problems in NP?",
                "Not all decidable problems are in NP. Consider the problem of determining whether a graph does not have a Hamiltonian cycle. To prove that a graph has a Hamiltonian cycle is a relatively simple matter—we just need to exhibit one. Nobody knows how to show, in polynomial time, that a graph does not have a Hamiltonian cycle. It seems that one must enumerate all the cycles and check them one by one. Thus the non–Hamiltonian cycle problem is not known to be in NP."
            ],
            "guid": "yF0/OK%S!1",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Define <b>NP complete problems</b>.",
                "Among all the problems known to be in NP, there is a subset, known as the <b>NP-complete problems</b>, which contains the hardest. An NP-complete problem has the property that any problem in NP can be <b>polynomially reduced</b> to it. A problem, P1, can be reduced to P2 as follows: Provide a mapping so that any instance of P1 can be transformed to an instance of P2. Solve P2, and then map the answer back to the original. As an example, numbers are entered into a pocket calculator in decimal. The decimal numbers are converted to binary, and all calculations are performed in binary. Then the final answer is converted back to decimal for display. For P1 to be polynomially reducible to P2, all the work associated with the transformations must be performed in polynomial time."
            ],
            "guid": "LB{P[H(nhd",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why <b>NP-complete</b> <b>problems</b> are the hardest NP problems?",
                "Because a problem that is NP-complete can essentially be used as a subroutine for any problem in NP, with only a polynomial amount of overhead. Thus, if any NP-complete problem has a polynomial-time solution, then every problem in NP must have a polynomial-time solution. This makes the NP-complete problems the hardest of all NP problems. Suppose we have an NP-complete problem, P1. Suppose P2 is known to be in NP.<br>Suppose further that P1 polynomially reduces to P2, so that we can solve P1 by using P2 with only a polynomial time penalty. Since P1 is NP-complete, every problem in NP polynomially reduces to P1. By applying the closure property of polynomials, we see that every problem in NP is polynomially reducible to P2: We reduce the problem to P1 and then reduce P1 to P2. Thus, P2 is NP-complete."
            ],
            "guid": "s-u@Pm[N?)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you describe <b>traveling salesman problem&nbsp;</b>in terms of Hamiltonian cycle problem?",
                "Suppose that we already know that the Hamiltonian cycle problem is NP-complete. The traveling salesman problem is as follows:<br>Given a complete graph, G = (V, E), with edge costs, and an integer K, is there a simple cycle that visits all vertices and has total cost ≤ K?<br>The problem is different from the Hamiltonian cycle problem, because all |V|(|V|-1)/2 edges are present and the graph is weighted. This problem has many important applications. For instance, printed circuit boards need to have holes punched so that chips, resistors, and other electronic components can be placed. This is done mechanically. Punching the hole is a quick operation; the time-consuming step is positioning the hole puncher. The time required for positioning depends on the distance traveled from hole to hole. Since we would like to punch every hole (and then return to the start for the next board), and minimize the total amount of time spent traveling, what we have is a traveling salesman problem.<br>The traveling salesman problem is NP-complete. It is easy to see that a solution can be checked in polynomial time, so it is certainly in NP. To show that it is NP-complete, we polynomially reduce the Hamiltonian cycle problem to it. To do this we construct a new graph, G'. G' has the same vertices as G. For G', each edge (v, w) has a weight of 1 if (v, w) ∈ G, and 2 otherwise. We choose K = |V|.<br>It is easy to verify that G has a Hamiltonian cycle if and only if G' has a traveling salesman tour of total weight |V|.<br><img src=\"paste-8944a416c2aff3101e8de7ba91df5eb68e35932a.jpg\">"
            ],
            "guid": "I`660C`]Kq",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do you prove that a problem is&nbsp;<b>NP complete</b>?",
                "To prove that some new problem is NP-complete, it must be shown to be in NP, and then an appropriate NP-complete problem must be transformed into it. Although the transformation to a traveling salesman problem was rather straightforward, most transformations are actually quite involved and require some tricky constructions. Generally, several different NP-complete&nbsp;problems are considered before the problem that actually provides the reduction. As we are only interested in the general ideas, we will not show any more transformations; the interested reader can consult the references."
            ],
            "guid": "BGp)+J%!9j",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe the <b>satisfiability problem</b>.",
                "The satisfiability problem takes as input a Boolean expression and asks whether the expression has an assignment to the variables that gives a value of true.<br>Satisfiability is certainly in NP, since it is easy to evaluate a Boolean expression and check whether the result is true. In 1971, Cook showed that satisfiability was NP-complete by directly proving that all problems that are in NP could be transformed to satisfiability. To do this, he used the one known fact about every problem in NP: Every problem in NP can be solved in polynomial time by a nondeterministic computer. The formal model for a computer is known as a <b>Turing machine</b>. Cook showed how the actions of this machine could be simulated by an extremely complicated and long, but still polynomial, Boolean formula. This Boolean formula would be true if and only if the program which was being run by the Turing machine produced a “yes” answer for its input.<br>Once satisfiability was shown to be NP-complete, a host of new NP-complete problems, including some of the most classic problems, were also shown to be NP-complete."
            ],
            "guid": "I0)y&UdV9W",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give some examples of <b>NP complete problems</b>.",
                "In addition to the satisfiability, Hamiltonian circuit, traveling salesman, and longestpath problems, some of the more well-known NPcomplete problems which we have not discussed are bin packing, knapsack, graph coloring, and clique. The list is quite extensive and includes problems from operating systems (scheduling and security), database systems, operations research, logic, and especially graph theory."
            ],
            "guid": "iL&pFV[6v>",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>greedy algorithms</b>.",
                "Greedy algorithms work in phases. In each phase, a decision is made that appears to be good, without regard for future consequences. Generally, this means that some <i>local optimum</i> is chosen. This “take what you can get now” strategy is the source of the name for this class of algorithms. When the algorithm terminates, we hope that the local optimum is equal to the <i>global optimum</i>. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. If the absolute best answer is not required, then simple greedy algorithms are sometimes used to generate approximate answers, rather than using the more complicated algorithms generally required to generate an exact answer. Examples include: Dijkstra’s, Prim’s, and Kruskal’s algorithms."
            ],
            "guid": "l(B.He!$%}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Give examples where <b>greedy algorithms</b>&nbsp;are not optimum.",
                "There are several real-life examples of greedy algorithms. The most obvious is the coinchanging problem. To make change in U.S. currency, we repeatedly dispense the largest denomination. Thus, to give out seventeen dollars and sixty-one cents in change, we give out a ten-dollar bill, a five-dollar bill, two one-dollar bills, two quarters, one dime, and one penny. By doing this, we are guaranteed to minimize the number of bills and coins. This algorithm does not work in all monetary systems, but fortunately, we can prove that it does work in the American monetary system. Indeed, it works even if two-dollar bills and fifty-cent pieces are allowed.<br>Traffic problems provide an example where making locally optimal choices does not always work. For example, during certain rush hour times in Miami, it is best to stay off the prime streets even if they look empty, because traffic will come to a standstill a mile down the road, and you will be stuck. Even more shocking, it is better in some cases to make a temporary detour in the direction opposite your destination in order to avoid all traffic bottlenecks."
            ],
            "guid": "iKnOnB6t3k",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Describe <b>simple scheduling problem.</b>",
                "We are given jobs j<sub>1</sub>, j<sub>2</sub>, . . . , j<sub>N</sub>, all with known running times t<sub>1</sub>, t<sub>2</sub>, . . . , t<sub>N</sub>, respectively. We have a single processor. What is the best way to schedule these jobs in order to minimize the average completion time?"
            ],
            "guid": "AyIE%5(*W_",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is <b>nonpreemptive scheduling</b>?",
                "Once a job is started, it must run to completion."
            ],
            "guid": "kq(.&!L|WO",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Suppose we have the four jobs and associated running times shown in figure, what is the best schedule to reduce average completion time?<br><img src=\"paste-f3b0d5def21168abf7f0ddefa23018cd8e4bcba7.jpg\">",
                "One possible schedule is shown in:<br><img src=\"paste-294e9e4c42d34e7e10d1e612f669fd6a65f7ddfe.jpg\"><br>Because j<sub>1</sub> finishes in 15 (time units), j<sub>2</sub> in 23, j<sub>3</sub> in 26, and j<sub>4</sub> in 36, the average completion time is 25. A better schedule, which yields a mean completion time of 17.75, is shown in:<br><img src=\"paste-85df9bb180cfc08df6e8d8ab3bcedb63a460dc6b.jpg\"><br>The schedule given in the figure above is arranged by shortest job first."
            ],
            "guid": "E=_ILBC>:?",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the optimum way to schedule jobs for optimum average completion time?",
                "The schedule should be arranged by shortest job first. We can show that this will always yield an optimal schedule. Let the jobs in the schedule be j<sub>i1</sub>, j<sub>i2</sub>, . . . , j<sub>iN</sub>. The first job finishes in time t<sub>i1</sub>. The second job finishes after t<sub>i1</sub> + t<sub>i2</sub>, and the third job finishes after t<sub>i1</sub> + t<sub>i2</sub> + t<sub>i3</sub>. From this, we see that the total cost, C, of the schedule is:<br><img src=\"paste-02088ec5250f2c0c58a5a71ea2db2de66c4a419f.jpg\"><br>Notice that in in the second equation, the first sum is independent of the job ordering, so only the second sum affects the total cost. Suppose that in an ordering there exists some x &gt; y such that t<sub>ix</sub> &lt; t<sub>iy</sub>. Then a calculation shows that by swapping j<sub>ix</sub> and j<sub>iy</sub>, the second sum increases, decreasing the total cost. Thus, any schedule of jobs in which the times are not monotonically nondecreasing must be suboptimal. The only schedules left are those in which the jobs are arranged by smallest running time first, breaking ties arbitrarily.<br>This result indicates the reason the operating system scheduler generally gives precedence to shorter jobs."
            ],
            "guid": "MgYd13{B-}",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Explain the solution to the job scheduling problem for multi-processors with <i>P </i>= 3.<br><img src=\"paste-e1d7f5dd1983461c2c5b91de785fe0d78dc7bba9.jpg\">",
                "The figure shows an optimal arrangement to minimize mean completion time. Jobs j<sub>1</sub>, j<sub>4</sub>, and j<sub>7</sub> are run on Processor 1. Processor 2 handles j<sub>2</sub>, j<sub>5</sub>, and j<sub>8</sub>, and Processor 3 runs the remaining jobs. The total time to completion is 165, for an average of 165/9 = 18.33.<br><img src=\"paste-3acaddd2a21ffbf8d1e25b6029c2a82d765b1590.jpg\"><br>The algorithm to solve the multiprocessor case is to start jobs in order, cycling through processors. It is not hard to show that no other ordering can do better, although if the number of processors, P, evenly divides the number of jobs, N, there are many optimal orderings. This is obtained by, for each 0 ≤ i &lt; N/P, placing each of the jobs j<sub>iP+1</sub> through j<sub>(i+1)P</sub> on a different processor. The figure below shows a second optimal solution.<br><img src=\"paste-25b4262438225c1a4904f9c40f8e610451902b88.jpg\">"
            ],
            "guid": "wjNpXgS}>)",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How is the number of processors related to the number of optimal solutions in the<b> job scheduling problem</b>?",
                "Even if&nbsp;P&nbsp;does not divide&nbsp;N&nbsp;exactly, there can still be many optimal solutions, even if&nbsp;all the job times are distinct."
            ],
            "guid": "DGPdHGABt7",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How do we minimize the final completion time for the following jobs?<br><img src=\"paste-b8cc3746e416849bba85b54c74221dfa8a28038d.jpg\">",
                "In our example above, these completion times when optimized for average job processing time is be 40 and 38, respectively. The figure below shows that the minimum final completion time is 34, and this clearly cannot be improved, because every processor is always busy.<br><img src=\"paste-14f64e311620cf3515281c2b84a6b9fca61eb254.jpg\"><br>Although this schedule does not have minimum mean completion time, it has merit in that the completion time of the entire sequence is earlier. If the same user owns all these jobs, then this is the preferable method of scheduling. Although these problems are very similar, this new problem turns out to be NP-complete; it is just another way of phrasing the knapsack or bin packing problems. Thus, minimizing the final completion time is apparently much harder than minimizing the mean completion time."
            ],
            "guid": "rmQq&{@X-p",
            "note_model_uuid": "bf77a69a-77d5-11ec-851a-18c04df1fa07",
            "tags": []
        }
    ]
}